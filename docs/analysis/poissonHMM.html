<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>blechpy.analysis.poissonHMM API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>blechpy.analysis.poissonHMM</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import os
import math
import numpy as np
import itertools as it
import pandas as pd
import tables
import time as sys_time
from numba import njit
from scipy.ndimage.filters import gaussian_filter1d
from blechpy.utils.particles import HMMInfoParticle
from blechpy import load_dataset
from blechpy.dio import h5io, hmmIO
from blechpy.plotting import hmm_plot as hmmplt
from blechpy.utils import math_tools as mt
from joblib import Parallel, delayed, Memory, cpu_count
from appdirs import user_cache_dir
cachedir = user_cache_dir(&#39;blechpy&#39;)
memory = Memory(cachedir, verbose=0)



TEST_PARAMS = {&#39;n_cells&#39;: 10, &#39;n_states&#39;: 4, &#39;state_seq_length&#39;: 5,
               &#39;trial_time&#39;: 3.5, &#39;dt&#39;: 0.001, &#39;max_rate&#39;: 50, &#39;n_trials&#39;: 15,
               &#39;min_state_dur&#39;: 0.05, &#39;noise&#39;: 0.01, &#39;baseline_dur&#39;: 1}


HMM_PARAMS = {&#39;hmm_id&#39;: None, &#39;taste&#39;: None, &#39;channel&#39;: None,
              &#39;unit_type&#39;: &#39;single&#39;, &#39;dt&#39;: 0.001, &#39;threshold&#39;: 1e-7,
              &#39;max_iter&#39;: 200, &#39;n_cells&#39;: None, &#39;n_trials&#39;: None,
              &#39;time_start&#39;: -250, &#39;time_end&#39;: 2000, &#39;n_repeats&#39;: 25,
              &#39;n_states&#39;: 3, &#39;fitted&#39;: False, &#39;area&#39;: &#39;GC&#39;,
              &#39;hmm_class&#39;: &#39;PoissonHMM&#39;, &#39;notes&#39;: &#39;&#39;}


FACTORIAL_LOOKUP = np.array([math.factorial(x) for x in range(20)])
MIN_PROB = 1e-100


@njit
def fast_factorial(x):
    if x &lt; len(FACTORIAL_LOOKUP):
        return FACTORIAL_LOOKUP[x]
    else:
        y = 1
        for i in range(1,x+1):
            y = y*i

        return y


@njit
def poisson(rate, n, dt):
    &#39;&#39;&#39;Gives probability of each neurons spike count assuming poisson spiking
    &#39;&#39;&#39;
    #tmp = np.power(rate*dt, n) / np.array([fast_factorial(x) for x in n])
    #tmp = tmp * np.exp(-rate*dt)
    tmp = n*np.log(rate*dt) - np.array([np.log(fast_factorial(x)) for x in n])
    tmp = tmp - rate*dt
    return np.exp(tmp)

@njit
def log_emission(rate, n , dt):
    return np.sum(np.log(poisson(rate, n, dt)))


@njit
def fix_arrays(PI,A,B):
    &#39;&#39;&#39;copy and remove zero values so that log probabilities can be computed
    &#39;&#39;&#39;
    PI = PI.copy()
    A = A.copy()
    B = B.copy()
    nx, ny = A.shape
    for i in range(nx):
        for j in range(ny):
            if A[i,j] == 0.:
                A[i,j] = MIN_PROB

    nx, ny = B.shape
    for i in range(nx):
        for j in range(ny):
            if B[i,j] == 0.:
                B[i,j] = MIN_PROB

    for i in range(len(PI)):
        if PI[i] == 0.:
            PI[i] = MIN_PROB

    return PI, A, B


@njit
def forward(spikes, dt, PI, A, B):
    &#39;&#39;&#39;Run forward algorithm to compute alpha = P(Xt = i| o1...ot, pi)
    Gives the probabilities of being in a specific state at each time point
    given the past observations and initial probabilities

    Parameters
    ----------
    spikes : np.array
        N x T matrix of spike counts with each entry ((i,j)) holding the # of
        spikes from neuron i in timebine j
    nStates : int, # of hidden states predicted to have generate the spikes
    dt : float, timebin in seconds (i.e. 0.001)
    PI : np.array
        nStates x 1 vector of initial state probabilities
    A : np.array
        nStates x nStates state transmission matrix with each entry ((i,j))
        giving the probability of transitioning from state i to state j
    B : np.array
        N x nSates rate matrix. Each entry ((i,j)) gives this predicited rate
        of neuron i in state j

    Returns
    -------
    alpha : np.array
        nStates x T matrix of forward probabilites. Each entry (i,j) gives
        P(Xt = i | o1,...,oj, pi)
    norms : np.array
        1 x T vector of norm used to normalize alpha to be a probability
        distribution and also to scale the outputs of the backward algorithm.
        norms(t) = sum(alpha(:,t))
    &#39;&#39;&#39;
    nTimeSteps = spikes.shape[1]
    nStates = A.shape[0]
    PI, A, B = fix_arrays(PI, A, B)

    # For each state, use the the initial state distribution and spike counts
    # to initialize alpha(:,1)
    #row = np.array([PI[i] * np.prod(poisson(B[:,i], spikes[:,0], dt))
    #row = np.array([np.log(PI[i]) + np.sum(np.log(poisson(B[:,i], spikes[:,0], dt)))
    #                for i in range(nStates)])
    a0 = [np.exp(np.log(PI[i]) + log_emission(B[:,i], spikes[:,0], dt))
          for i in range(nStates)]
    a0 = np.array(a0)
    alpha = np.zeros((nStates, nTimeSteps))
    norms = [np.sum(a0)]
    alpha[:, 0] = a0/norms[0]
    for t in range(1, nTimeSteps):
        for s in range(nStates):
            tmp_em = log_emission(B[:,s], spikes[:, t], dt)
            tmp_a = np.sum(np.exp(np.log(alpha[:, t-1]) + np.log(A[:,s])))
            tmp = np.exp(tmp_em + np.log(tmp_a))
            alpha[s,t] = tmp

        tmp_norm = np.sum(alpha[:,t])
        norms.append(tmp_norm)
        alpha[:, t] = alpha[:,t] / tmp_norm

    return alpha, norms


@njit
def backward(spikes, dt, A, B, norms):
    &#39;&#39;&#39; Runs the backward algorithm to compute beta = P(ot+1...oT | Xt=s)
    Computes the probability of observing all future observations given the
    current state at each time point

    Paramters
    ---------
    spike : np.array, N x T matrix of spike counts
    nStates : int, # of hidden states predicted
    dt : float, timebin size in seconds
    A : np.array, nStates x nStates matrix of transition probabilities
    B : np.array, N x nStates matrix of estimated spike rates for each neuron

    Returns
    -------
    beta : np.array, nStates x T matrix of backward probabilities
    &#39;&#39;&#39;
    _, A, B = fix_arrays(np.array([0]), A, B)
    nTimeSteps = spikes.shape[1]
    nStates = A.shape[0]
    beta = np.zeros((nStates, nTimeSteps))
    beta[:, -1] = 1  # Initialize final beta to 1 for all states
    tStep = list(range(nTimeSteps-1))
    tStep.reverse()
    for t in tStep:
        for s in range(nStates):
            tmp_em = log_emission(B[:,s], spikes[:,t+1], dt)
            tmp_b = np.log(beta[:,t+1]) + np.log(A[s,:]) + tmp_em
            beta[s,t] = np.sum(np.exp(tmp_b))

        beta[:, t] = beta[:, t] / norms[t+1]

    return beta


@njit
def compute_baum_welch(spikes, dt, A, B, alpha, beta):
    _, A, B = fix_arrays(np.array([0]), A, B)
    nTimeSteps = spikes.shape[1]
    nStates = A.shape[0]
    gamma = np.zeros((nStates, nTimeSteps))
    epsilons = np.zeros((nStates, nStates, nTimeSteps-1))
    for t in range(nTimeSteps):
        tmp_g = np.exp(np.log(alpha[:, t]) + np.log(beta[:, t]))
        gamma[:, t] = tmp_g / np.sum(tmp_g)
        if t &lt; nTimeSteps-1:
            epsilonNumerator = np.zeros((nStates, nStates))
            for si in range(nStates):
                for sj in range(nStates):
                    probs = log_emission(B[:, sj], spikes[:, t+1], dt)
                    tmp_en = (np.log(alpha[si, t]) + np.log(A[si, sj]) +
                              np.log(beta[sj, t+1]) + probs)
                    epsilonNumerator[si, sj] = np.exp(tmp_en)

            epsilons[:, :, t] = epsilonNumerator / np.sum(epsilonNumerator)

    return gamma, epsilons


@njit
def baum_welch(trial_dat, dt, PI, A, B):
    alpha, norms = forward(trial_dat, dt, PI, A, B)
    beta = backward(trial_dat, dt, A, B, norms)
    tmp_gamma, tmp_epsilons = compute_baum_welch(trial_dat, dt, A, B, alpha, beta)
    return tmp_gamma, tmp_epsilons, norms


def compute_new_matrices(spikes, dt, gammas, epsilons):
    nTrials, nCells, nTimeSteps = spikes.shape
    n_states = gammas.shape[1]
    minFR = 1/(nTimeSteps*dt)

    PI = np.mean(gammas[:, :, 0], axis=0)
    A = np.zeros((n_states, n_states))
    B = np.zeros((nCells, n_states))
    for si in range(n_states):
        for sj in range(n_states):
            Anumer = np.sum(epsilons[:, si, sj, :])
            Adenom = np.sum(gammas[:, si, -1])
            if np.isfinite(Adenom) and Adenom != 0.:
                A[si, sj] = Anumer / Adenom
            else:
                A[si, sj] = 0 # incase of floating point errors resulting in zeros

        #A[si, A[si,:] &lt; 1e-50] = 0
        row = A[si,:]
        if np.sum(row) == 0.0:
            A[si, sj] = 1.0
        else:
            A[si, :] = A[si,:] / np.sum(row)


    for si in range(n_states):
        for tri in range(nTrials):
            for t in range(nTimeSteps-1):
                for u in range(nCells):
                    B[u,si] = B[u,si] + gammas[tri, si, t]*spikes[tri, u, t]

    # Convert and really small transition values into zeros
    #A[A &lt; 1e-50] = 0
    #sums = np.sum(A, axis=1)
    #A = A/np.sum(A, axis=1) # This divides columns not rows
    #Bnumer = np.sum(np.array([np.matmul(tmp_y, tmp_g.T)
    #                          for tmp_y, tmp_g in zip(spikes, gammas)]),
    #                axis=0)
    Bdenom =  np.sum(np.sum(gammas, axis=2), axis=0)
    B = (B / Bdenom)/dt
    B[B &lt; minFR] = minFR
    A[A &lt;= MIN_PROB] = 0.0

    return PI, A, B


def poisson_viterbi_deprecated(spikes, dt, PI, A, B):
    &#39;&#39;&#39;
    Parameters
    ----------
    spikes : np.array, Neuron X Time matrix of spike counts
    PI : np.array, nStates x 1 vector of initial state probabilities
    A : np.array, nStates X nStates matric of state transition probabilities
    B : np.array, Neuron X States matrix of estimated firing rates
    dt : float, time step size in seconds

    Returns
    -------
    bestPath : np.array
        1 x Time vector of states representing the most likely hidden state
        sequence
    maxPathLogProb : float
        Log probability of the most likely state sequence
    T1 : np.array
        State X Time matrix where each entry (i,j) gives the log probability of
        the the most likely path so far ending in state i that generates
        observations o1,..., oj
    T2: np.array
        State X Time matrix of back pointers where each entry (i,j) gives the
        state x(j-1) on the most likely path so far ending in state i
    &#39;&#39;&#39;
    if A.shape[0] != A.shape[1]:
        raise ValueError(&#39;Transition matrix is not square&#39;)

    nStates = A.shape[0]
    nCells, nTimeSteps = spikes.shape
    # get rid of zeros for computation 
    A[np.where(A==0)] = 1e-300
    T1 = np.zeros((nStates, nTimeSteps))
    T2 = np.zeros((nStates, nTimeSteps))
    T1[:,0] = np.array([np.log(PI[i]) +
                        np.log(np.prod(poisson(B[:,i], spikes[:, 1], dt)))
                        for i in range(nStates)])
    for t, s in it.product(range(1,nTimeSteps), range(nStates)):
        probs = np.log(np.prod(poisson(B[:, s], spikes[:, t], dt)))
        vec2 = T1[:, t-1] + np.log(A[:,s])
        vec1 = vec2 + probs
        T1[s, t] = np.max(vec1)
        idx = np.argmax(vec1)
        T2[s, t] = idx

    bestPathEndState = np.argmax(T1[:, -1])
    maxPathLogProb = T1[bestPathEndState, -1]
    bestPath = np.zeros((nTimeSteps,))
    bestPath[-1] = bestPathEndState
    tStep = list(range(nTimeSteps-1))
    tStep.reverse()
    for t in tStep:
        bestPath[t] = T2[int(bestPath[t+1]), t+1]

    return bestPath, maxPathLogProb, T1, T2


def poisson_viterbi(spikes, dt, PI, A, B):
    n_states = A.shape[0]
    PI, A, B = fix_arrays(PI, A, B)
    n_cells, n_steps = spikes.shape
    T1 = np.ones((n_states, n_steps))*1e-300
    T2 = np.zeros((n_states, n_steps))
    T1[:, 0] = [np.log(PI[i])+np.sum(np.log(poisson(B[:,i], spikes[:,0], dt)))
                for i in range(n_states)]
    #for t,s in it.product(range(1,n_steps), range(n_states)):
    for t in range(1,n_steps):
        for s in range(n_states):
            probs = np.sum(np.log(poisson(B[:,s], spikes[:,t], dt)))
            vec1 = T1[:,t-1]+np.log(A[:,s])+probs
            T1[s,t] = np.max(vec1)
            T2[s,t] = np.argmax(vec1)

    best_end_state = np.argmax(T1[:,-1])
    max_log_prob = T1[best_end_state, -1]
    bestPath = np.zeros((n_steps,))
    bestPath[-1] = best_end_state
    tStep = list(range(n_steps-1))
    tStep.reverse()
    for t in tStep:
        bestPath[t] = T2[int(bestPath[t+1]), t+1]

    return bestPath, max_log_prob, T1, T2


def compute_BIC(PI, A, B, spikes=None, dt=None, maxLogProb=None, n_time_steps=None):
    if (maxLogProb is None or n_time_steps is None) and (spikes is None or dt is None):
        raise ValueError(&#39;Must provide max log prob and n_time_steps or spikes and dt&#39;)

    nParams = (A.shape[0]*(A.shape[1]-1) +
               (PI.shape[0]-1) +
               B.shape[0]*(B.shape[1]-1))
    if maxLogProb and n_time_steps:
        pass
    else:
        bestPaths, path_probs = compute_best_paths(spikes, dt, PI, A, B)
        maxLogProb = np.sum(path_probs)
        n_time_steps = spikes.shape[-1]

    BIC = -2 * maxLogProb + nParams * np.log(n_time_steps)
    return BIC, bestPaths, maxLogProb


def compute_hmm_cost(spikes, dt, PI, A, B, win_size=0.25, true_rates=None):
    if true_rates is None:
        true_rates = convert_spikes_to_rates(spikes, dt, win_size,
                                             step_size=win_size)

    BIC, bestPaths, maxLogProb = compute_BIC(PI, A, B, spikes=spikes, dt=dt)
    hmm_rates = generate_rate_array_from_state_seq(bestPaths, B, dt, win_size,
                                                   step_size=win_size)
    RMSE = compute_rate_rmse(true_rates, hmm_rates)
    return RMSE, BIC, bestPaths, maxLogProb


def compute_best_paths(spikes, dt, PI, A, B):
    if len(spikes.shape) == 2:
        spikes = np.array([spikes])

    nTrials, nCells, nTimeSteps = spikes.shape
    bestPaths = np.zeros((nTrials, nTimeSteps))-1
    pathProbs = np.zeros((nTrials,))

    for i, trial in enumerate(spikes):
        bestPaths[i,:], pathProbs[i], _, _ = poisson_viterbi(trial, dt, PI,
                                                             A, B)
    return bestPaths, pathProbs


@njit
def compute_rate_rmse(rates1, rates2):
    # Compute RMSE per trial
    # Mean over trials
    n_trials, n_cells, n_steps = rates1.shape
    RMSE = np.zeros((n_trials,))
    for i in range(n_trials):
        t1 = rates1[i, :, :]
        t2 = rates2[i, :, :]
        # Compute RMSE from euclidean distances at each time point
        distances = np.zeros((n_steps,))
        for j in range(n_steps):
            distances[j] =  mt.euclidean(t1[:,j], t2[:,j])

        RMSE[i] = np.sqrt(np.mean(np.power(distances,2)))

    return np.mean(RMSE)


def convert_path_state_numbers(paths, state_map):
    newPaths = np.zeros(paths.shape)
    for k,v in state_map.items():
        idx = np.where(paths == k)
        newPaths[idx] = v

    return newPaths


def match_states(emission1, emission2):
    &#39;&#39;&#39;Takes 2 Cell X State firing rate matrices and determines which states
    are most similar. Returns dict mapping emission2 states to emission1 states
    &#39;&#39;&#39;
    distances = np.zeros((emission1.shape[1], emission2.shape[1]))
    for x, y in it.product(range(emission1.shape[1]), range(emission2.shape[1])):
        tmp = mt.euclidean(emission1[:, x], emission2[:, y])
        distances[x, y] = tmp

    states = list(range(emission2.shape[1]))
    out = {}
    for i in range(emission2.shape[1]):
        s = np.argmin(distances[:,i])
        r = np.argmin(distances[s, :])
        if r == i and s in states:
            out[i] = s
            idx = np.where(states == s)[0]
            states.pop(int(idx))

    for i in range(emission2.shape[1]):
        if i not in out:
            s = np.argmin(distances[states, i])
            out[i] = states[s]

    return out


@memory.cache
@njit
def convert_spikes_to_rates(spikes, dt, win_size, step_size=None):
    if step_size is None:
        step_size = win_size

    n_trials, n_cells, n_steps = spikes.shape
    n_pts = int(win_size/dt)
    n_step_pts = int(step_size/dt)
    win_starts = np.arange(0, n_steps, n_step_pts)
    out = np.zeros((n_trials, n_cells, len(win_starts)))
    for i, w in enumerate(win_starts):
        out[:, :, i] = np.sum(spikes[:, :, w:w+n_pts], axis=2) / win_size

    return out


@memory.cache
@njit
def generate_rate_array_from_state_seq(bestPaths, B, dt, win_size,
                                       step_size=None):
    if not step_size:
        step_size = win_size

    n_trials, n_steps = bestPaths.shape
    n_cells, n_states = B.shape
    rates = np.zeros((n_trials, n_cells, n_steps))
    for j in range(n_trials):
        seq = bestPaths[j, :].astype(np.int64)
        rates[j, :, :] = B[:, seq]

    n_pts = int(win_size / dt)
    n_step_pts = int(step_size/dt)
    win_starts = np.arange(0, n_steps, n_step_pts)
    mean_rates = np.zeros((n_trials, n_cells, len(win_starts)))
    for i, w in enumerate(win_starts):
        mean_rates[:, :, i] = np.sum(rates[:, : , w:w+n_pts], axis=2) / n_pts

    return mean_rates


@memory.cache
@njit
def rebin_spike_array(spikes, dt, time, new_dt):
    if dt == new_dt:
        return spikes, time

    n_trials, n_cells, n_steps = spikes.shape
    n_bins = int(new_dt/dt)
    new_time = np.arange(time[0], time[-1], n_bins)
    new_spikes = np.zeros((n_trials, n_cells, len(new_time)))
    for i, w in enumerate(new_time):
        idx = np.where((time &gt;= w) &amp; (time &lt; w+new_dt))[0]
        new_spikes[:,:,i] = np.sum(spikes[:,:,idx], axis=-1)

    return new_spikes.astype(np.int32), new_time


@memory.cache
def get_hmm_spike_data(rec_dir, unit_type, channel, time_start=None,
                       time_end=None, dt=None, trials=None, area=None):
    # unit type can be &#39;single&#39;, &#39;pyramidal&#39;, or &#39;interneuron&#39;, or a list of unit names
    if isinstance(unit_type, str):
        units = query_units(rec_dir, unit_type, area=area)
    elif isinstance(unit_type, list):
        units = unit_type

    time, spike_array = h5io.get_spike_data(rec_dir, units, channel, trials=trials)
    spike_array = spike_array.astype(np.int32)
    if len(units) == 1:
        spike_array = np.expand_dims(spike_array, 1)

    time = time.astype(np.float64)
    curr_dt = np.unique(np.diff(time))[0] / 1000
    if dt is not None and curr_dt &lt; dt:
        print(&#39;%s: Rebinning Spike Array&#39; % os.getpid())
        spike_array, time = rebin_spike_array(spike_array, curr_dt, time, dt)
    elif dt is not None and curr_dt &gt; dt:
        raise ValueError(&#39;Cannot upsample spike array from %f sec &#39;
                         &#39;bins to %f sec bins&#39; % (dt, curr_dt))
    else:
        dt = curr_dt

    if time_start is not None and time_end is not None:
        print(&#39;%s: Trimming spike array&#39; % os.getpid())
        idx = np.where((time &gt;= time_start) &amp; (time &lt; time_end))[0]
        time = time[idx]
        spike_array = spike_array[:, :, idx]

    return spike_array, dt, time


@memory.cache
def query_units(dat, unit_type, area=None):
    &#39;&#39;&#39;Returns the units names of all units in the dataset that match unit_type

    Parameters
    ----------
    dat : blechpy.dataset or str
        Can either be a dataset object or the str path to the recording
        directory containing that data .h5 object
    unit_type : str, {&#39;single&#39;, &#39;pyramidal&#39;, &#39;interneuron&#39;, &#39;all&#39;}
        determines whether to return &#39;single&#39; units, &#39;pyramidal&#39; (regular
        spiking single) units, &#39;interneuron&#39; (fast spiking single) units, or
        &#39;all&#39; units
    area : str
        brain area of cells to return, must match area in
        dataset.electrode_mapping

    Returns
    -------
        list of str : unit_names
    &#39;&#39;&#39;
    if isinstance(dat, str):
        units = h5io.get_unit_table(dat)
        el_map = h5io.get_electrode_mapping(dat)
    else:
        units = dat.get_unit_table()
        el_map = dat.electrode_mapping.copy()

    u_str = unit_type.lower()
    q_str = &#39;&#39;
    if u_str == &#39;single&#39;:
        q_str = &#39;single_unit == True&#39;
    elif u_str == &#39;pyramidal&#39;:
        q_str = &#39;single_unit == True and regular_spiking == True&#39;
    elif u_str == &#39;interneuron&#39;:
        q_str = &#39;single_unit == True and fast_spiking == True&#39;
    elif u_str == &#39;all&#39;:
        return units[&#39;unit_name&#39;].tolist()
    else:
        raise ValueError(&#39;Invalid unit_type %s. Must be &#39;
                         &#39;single, pyramidal, interneuron or all&#39; % u_str)

    units = units.query(q_str)
    if area is None or area == &#39;&#39; or area == &#39;None&#39;:
        return units[&#39;unit_name&#39;].to_list()

    out = []
    el_map = el_map.set_index(&#39;Electrode&#39;)
    for i, row in units.iterrows():
        if el_map.loc[row[&#39;electrode&#39;], &#39;area&#39;] == area:
            out.append(row[&#39;unit_name&#39;])

    return out


def fit_hmm_mp(rec_dir, params, h5_file=None, constraint_func=None):
    hmm_id = params[&#39;hmm_id&#39;]
    n_states = params[&#39;n_states&#39;]
    dt = params[&#39;dt&#39;]
    time_start = params[&#39;time_start&#39;]
    time_end = params[&#39;time_end&#39;]
    max_iter = params[&#39;max_iter&#39;]
    threshold = params[&#39;threshold&#39;]
    unit_type = params[&#39;unit_type&#39;]
    channels = params[&#39;channel&#39;]
    tastes = params[&#39;taste&#39;]
    n_trials = params[&#39;n_trials&#39;]
    if &#39;area&#39; in params.keys():
        area = params[&#39;area&#39;]
    else:
        area = None

    if not isinstance(channels, list):
        channels = [channels]

    if not isinstance(tastes, list):
        tastes = [tastes]

    spikes = []
    row_id = []
    time = None
    for ch, tst in zip(channels, tastes):
        tmp_s, _, time = get_hmm_spike_data(rec_dir, unit_type, ch,
                                             time_start=time_start,
                                             time_end=time_end, dt=dt,
                                             trials=n_trials, area=area)
        tmp_id = np.vstack([(hmm_id, ch, tst, x) for x in range(tmp_s.shape[0])])
        spikes.append(tmp_s)
        row_id.append(tmp_id)

    spikes = np.vstack(spikes)
    row_id = np.vstack(row_id)

    if params[&#39;hmm_class&#39;] == &#39;PoissonHMM&#39;:
        hmm = PoissonHMM(n_states, hmm_id=hmm_id)
    elif params[&#39;hmm_class&#39;] == &#39;ConstrainedHMM&#39;:
        hmm = ConstrainedHMM(len(channels), hmm_id=hmm_id)

    hmm.randomize(spikes, dt, time, row_id=row_id, constraint_func=constraint_func)
    success = hmm.fit(spikes, dt, time, max_iter=max_iter, threshold=threshold)
    if not success:
        print(&#39;%s: Fitting Aborted for hmm %s&#39; % (os.getpid(), hmm_id))
        if h5_file:
            return hmm_id, False
        else:
            return hmm_id, hmm

    # hmm = roll_back_hmm_to_best(hmm, spikes, dt, threshold)
    print(&#39;%s: Done Fitting for hmm %s&#39; % (os.getpid(), hmm_id))
    written = False
    if h5_file:
        pid = os.getpid()
        lock_file = h5_file + &#39;.lock&#39;
        while os.path.exists(lock_file):
            print(&#39;%s: Waiting for file lock&#39; % pid)
            sys_time.sleep(20)

        locked = True
        while locked:
            try:
                os.mknod(lock_file)
                locked=False
            except:
                sys_time.sleep(10)

        try:
            old_hmm, _, old_params = load_hmm_from_hdf5(h5_file, hmm_id)

            if old_hmm is None:
                print(&#39;%s: No existing HMM %s. Writing ...&#39; % (pid, hmm_id))
                hmmIO.write_hmm_to_hdf5(h5_file, hmm, params)
                written = True
            else:
                print(&#39;%s: Existing HMM %s found. Comparing log likelihood ...&#39; % (pid, hmm_id))
                print(&#39;New %.3E vs Old %.3E&#39; % (hmm.fit_LL, old_hmm.fit_LL))
                if hmm.fit_LL &gt; old_hmm.fit_LL:
                    print(&#39;%s: Replacing HMM %s due to higher log likelihood&#39; % (pid, hmm_id))
                    hmmIO.write_hmm_to_hdf5(h5_file, hmm, params)
                    written = True

        except Exception as e:
            os.remove(lock_file)
            raise Exception(e)

        os.remove(lock_file)
        del old_hmm, hmm, spikes, dt, time
        return hmm_id, written
    else:
        return hmm_id, hmm


def load_hmm_from_hdf5(h5_file, hmm_id):
    hmm_id = int(hmm_id)
    existing_hmm = hmmIO.read_hmm_from_hdf5(h5_file, hmm_id)
    if existing_hmm is None:
        return None, None, None

    PI, A, B, stat_arrays, params = existing_hmm
    hmm = PoissonHMM(params[&#39;n_states&#39;], hmm_id=hmm_id)
    hmm._init_history()
    hmm.initial_distribution = PI
    hmm.transition = A
    hmm.emission = B
    hmm.iteration = params[&#39;n_iterations&#39;]
    for k,v in stat_arrays.items():
        if k in hmm.stat_arrays.keys() and isinstance(hmm.stat_arrays[k], list):
            hmm.stat_arrays[k] = list(v)
        else:
            hmm.stat_arrays[k] = v

    hmm.BIC = params.pop(&#39;BIC&#39;)
    hmm.converged = params.pop(&#39;converged&#39;)
    hmm.fitted = params.pop(&#39;fitted&#39;)
    hmm.cost = params.pop(&#39;cost&#39;)
    hmm.fit_LL = params.pop(&#39;log_likelihood&#39;)
    hmm.max_log_prob = params.pop(&#39;max_log_prob&#39;)

    return hmm, stat_arrays[&#39;time&#39;], params


def isConverged(hmm, thresh):
    &#39;&#39;&#39;Check HMM convergence based on the log-likelihood
    NOT WORKING YET
    &#39;&#39;&#39;
    pass


def check_ll_trend(hmm, thresh, n_iter=None):
    &#39;&#39;&#39;Check the trend of the log-likelihood to see if it has plateaued, is
    decreasing or is increasing
    &#39;&#39;&#39;
    if n_iter is None:
        n_iter = hmm.iteration

    ll_hist = np.array(hmm.stat_arrays[&#39;max_log_prob&#39;])
    iterations = np.array(hmm.stat_arrays[&#39;iterations&#39;])
    if n_iter not in iterations:
        raise ValueError(&#39;Iteration %i is not in history&#39; % n_iter)

    idx = np.where(iterations &lt;= n_iter)[0]
    ll_hist = ll_hist[idx]
    filt_ll = gaussian_filter1d(ll_hist, 4)
    diff_ll = np.diff(filt_ll)

    # Linear fit, if overall trend is decreasing, it fails
    z = np.polyfit(range(len(ll_hist)), filt_ll, 1)
    if z[0] &lt;= 0:
        return &#39;decreasing&#39;

    # Check if it has plateaued
    if all(np.abs(diff_ll[-5:]) &lt;= thresh):
        return &#39;plateau&#39;

    # if its a maxima and hasn&#39;t plateaued it needs to continue fitting
    if np.max(filt_ll) == filt_ll[-1]:
        return &#39;increasing&#39;

    return &#39;flux&#39;


def roll_back_hmm_to_best(hmm, spikes, dt, thresh):
    &#39;&#39;&#39;Looks at the log likelihood over fitting and determines the best
    iteration to have stopped at by choosing a local maxima during a period
    where the smoothed LL trace has plateaued
    &#39;&#39;&#39;
    ll_hist = np.array(hmm.stat_arrays[&#39;max_log_prob&#39;])
    idx = np.where(np.isfinite(ll_hist))[0]
    if len(idx) == 0:
        return hmm

    iterations = np.array(hmm.stat_arrays[&#39;iterations&#39;])
    ll_hist = ll_hist[idx]
    iterations = iterations[idx]
    filt_ll = gaussian_filter1d(ll_hist, 4)
    diff_ll = np.diff(filt_ll)
    below = np.where(np.abs(diff_ll) &lt; thresh)[0] + 1 # since diff_ll is 1 smaller than ll_hist
    # Exclude maxima less than 50 iterations since its pretty spikey early on
    below = [x for x in below if (iterations[x] &gt; 50)]
    # If there are none that fit criteria, just pick best past 50
    if len(below) == 0:
        below = np.where(iterations &gt; 50)[0]

    if len(below) == 0:
        below = np.arange(len(iterations))

    below = below[below&gt;2]

    tmp = [x for x in below if check_ll_trend(hmm, thresh, n_iter=iterations[x]) == &#39;plateau&#39;]
    if len(tmp) != 0:
        below = tmp

    maxima = np.argmax(ll_hist[below]) # this gives the index in below
    maxima = iterations[below[maxima]] # this is the iteration at which the maxima occurred
    hmm.roll_back(maxima, spikes=spikes, dt=dt)
    return hmm


def get_new_id(ids=None):
    if ids is None or len(ids) == 0:
        return 0

    nums = np.arange(0, np.max(ids) + 2)
    diff_nums = [x for x in nums if x not in ids]
    return np.min(diff_nums)


class PoissonHMM(object):
    def __init__(self, n_states, hmm_id=None):
        self.stat_arrays = {} # dict of cumulative stats to keep while fitting
                              # iterations, max_log_likelihood, fit log
                              # likelihood, cost, best_sequences, gamma
                              # probabilities, time, row_id
        self.n_states = n_states
        self.hmm_id = hmm_id

        self.transition = None
        self.emission = None
        self.initial_distribution = None

        self.fitted = False
        self.converged = False

        self.cost = None
        self.BIC = None
        self.max_log_prob = None
        self.fit_LL = None

    def randomize(self, spikes, dt, time, row_id=None, constraint_func=None):
        &#39;&#39;&#39;Initialize and randomize HMM matrices: initial_distribution (PI),
        transition (A) and emission/rates (B)
        Parameters
        ----------
        spikes : np.ndarray, dtype=int
            matrix of spike counts with dimensions trials x cells x time with binsize dt
        dt : float
            time step of spikes matrix in seconds
        time : np.ndarray
            1-D time vector corresponding to final dimension of spikes matrix,
            in milliseconds
        row_id : np.ndarray
            array to uniquely identify each row of the spikes array. This will
            thus identify each row of the best_sequences and gamma_probability
            matrices that are computed and stored
            useful when fitting a single HMM to trials with differing stimuli
        constrain_func : function
            user can provide a function that is used after randomization to
            constrain the PI, A and B matrices. The function must take PI, A, B
            as arguments and return PI, A, B.
        &#39;&#39;&#39;
        # setup parameters 
        # make transition matrix
        # all baseline states have equal probability of staying or changing
        # into each other and the early states
        # each early state has high stay probability and low chance to transition into 
        np.random.seed(None)
        n_trials, n_cells, n_steps = spikes.shape
        n_states = self.n_states

        # Initialize transition matrix with high stay probability
        # A is prob from going from state row to state column
        print(&#39;%s: Randomizing&#39; % os.getpid())
        # Design transition matrix with large diagnonal and small everything else
        diag = np.abs(np.random.normal(.99, .01, n_states))
        A = np.abs(np.random.normal(0.01/(n_states-1), 0.01, (n_states, n_states)))
        for i in range(n_states):
            A[i, i] = diag[i]
            A[i,:] = A[i,:] / np.sum(A[i,:]) # normalize row to sum to 1

        # Initialize rate matrix (&#34;Emission&#34; matrix)
        spike_counts = np.sum(spikes, axis=2) / (len(time)*dt)
        mean_rates = np.mean(spike_counts, axis=0)
        std_rates = np.std(spike_counts, axis=0)
        B = np.vstack([np.abs(np.random.normal(x, y, n_states))
                       for x,y in zip(mean_rates, std_rates)])
        PI = np.ones((n_states,)) / n_states

        # RN10 preCTA fit better without constraining initial firing rate
        # mr = np.mean(np.sum(spikes[:, :, :int(500/dt)], axis=2), axis=0)
        # sr = np.std(np.sum(spikes[:, :, :int(500/dt)], axis=2), axis=0)
        # B[:, 0] = [np.abs(np.random.normal(x, y, 1))[0] for x,y in zip(mr, sr)]

        if constraint_func is not None:
            PI, A, B = constraint_func(PI, A, B)

        self.transition = A
        self.emission = B
        self.initial_distribution = PI
        self.fitted = False
        self.converged = False
        self.iteration = 0
        self.stat_arrays[&#39;row_id&#39;] = row_id
        self._init_history()
        self.stat_arrays[&#39;gamma_probabilities&#39;] = self.get_gamma_probabilities(spikes, dt)
        self.stat_arrays[&#39;time&#39;] = time
        self._update_cost(spikes, dt)
        self.fit_LL = self.max_log_prob
        self._update_history()

    def _init_history(self):
        self.stat_arrays[&#39;cost&#39;] = []
        self.stat_arrays[&#39;BIC&#39;] = []
        self.stat_arrays[&#39;max_log_prob&#39;] = []
        self.stat_arrays[&#39;fit_LL&#39;] = []
        self.stat_arrays[&#39;iterations&#39;] = []
        self.history = {&#39;A&#39;: [], &#39;B&#39;: [], &#39;PI&#39;: [], &#39;iterations&#39;:[]}

    def _update_history(self):
        itr = self.iteration
        self.history[&#39;A&#39;].append(self.transition)
        self.history[&#39;B&#39;].append(self.emission)
        self.history[&#39;PI&#39;].append(self.initial_distribution)
        self.history[&#39;iterations&#39;].append(itr)

        self.stat_arrays[&#39;cost&#39;].append(self.cost)
        self.stat_arrays[&#39;BIC&#39;].append(self.BIC)
        self.stat_arrays[&#39;max_log_prob&#39;].append(self.max_log_prob)
        self.stat_arrays[&#39;fit_LL&#39;].append(self.fit_LL)
        self.stat_arrays[&#39;iterations&#39;].append(itr)

    def fit(self, spikes, dt, time, max_iter = 500, threshold=1e-5, parallel=False):
        &#39;&#39;&#39;using parallels for processing trials actually seems to slow down
        processing (with 15 trials). Might still be useful if there is a very
        large nubmer of trials
        &#39;&#39;&#39;
        spikes = spikes.astype(&#39;int32&#39;)
        if (self.initial_distribution is None or
            self.transition is None or
            self.emission is None):
            raise ValueError(&#39;Must first initialize fit matrices either manually or via randomize&#39;)

        converged = False
        last_logl = None
        self.stat_arrays[&#39;time&#39;] = time
        while (not converged and (self.iteration &lt; max_iter)):
            self.fit_LL = self._step(spikes, dt, parallel=parallel)
            self._update_history()
            # if self.iteration &gt;= 100:
            #     trend = check_ll_trend(self, threshold)
            #     if trend == &#39;decreasing&#39;:
            #         return False
            #     elif trend == &#39;plateau&#39;:
            #         converged = True

            if last_logl is None:
                delta_ll = np.abs(self.fit_LL)
            else:
                delta_ll = np.abs((last_logl - self.fit_LL)/self.fit_LL)

            if (last_logl is not None and
                np.isfinite(delta_ll) and
                delta_ll &lt; threshold and
                np.isfinite(self.fit_LL) and
                self.iteration&gt;2):
                # This log likelihood measure doesn&#39;t look right, the change
                # seems to always be 0
                # 8/24/20: Fixed, this is now a good measure
                converged = True
                print(&#39;%s: %s: Change in log likelihood converged&#39; % (os.getpid(), self.hmm_id))

            last_logl = self.fit_LL

            # Convergence check is replaced by checking LL trend for plateau
            # converged = self.isConverged(convergence_thresh)
            print(&#39;%s: %s: Iter #%i complete. Log-likelihood is %.2E. Delta is %.2E&#39;
                  % (os.getpid(), self.hmm_id, self.iteration, self.fit_LL, delta_ll))

        self.fitted = True
        self.converged = converged
        return True

    def _step(self, spikes, dt, parallel=False):
        if len(spikes.shape) == 2:
            spikes = np.expand_dims(spikes, 0)

        nTrials, nCells, nTimeSteps = spikes.shape

        A = self.transition
        B = self.emission
        PI = self.initial_distribution
        nStates = self.n_states

        # For multiple trials need to cmpute gamma and epsilon for every trial
        # and then update
        if parallel:
            n_cores = cpu_count() - 1
        else:
            n_cores = 1

        results = Parallel(n_jobs=n_cores)(delayed(baum_welch)(trial, dt, PI, A, B)
                                           for trial in spikes)
        gammas, epsilons, norms = zip(*results)
        gammas = np.array(gammas)
        epsilons = np.array(epsilons)
        norms = np.array(norms)
        #logl = np.sum(norms)
        logl = np.sum(np.log(norms))

        PI, A, B = compute_new_matrices(spikes, dt, gammas, epsilons)
        # Make sure rates are non-zeros for computations
        # B[np.where(B==0)] = 1e-300
        A[A &lt; 1e-50] = 0.0
        for i in range(self.n_states):
            A[i,:] = A[i,:] / np.sum(A[i,:])

        self.transition = A
        self.emission = B
        self.initial_distribution = PI
        self.stat_arrays[&#39;gamma_probabilities&#39;] = gammas
        self.iteration = self.iteration + 1
        self._update_cost(spikes, dt)
        return logl

    def get_best_paths(self, spikes, dt):
        if &#39;best_sequences&#39; is self.stat_arrays.keys():
            return self.stat_arrays[&#39;best_sequences&#39;], self.max_log_prob

        PI = self.initial_distribution
        A = self.transition
        B = self.emission

        bestPaths, pathProbs = compute_best_paths(spikes, dt, PI, A, B)
        return bestPaths, np.sum(pathProbs)

    def get_forward_probabilities(self, spikes, dt, parallel=False):
        PI = self.initial_distribution
        A = self.transition
        B = self.emission
        if parallel:
            n_cpu = cpu_count() -1
        else:
            n_cpu = 1

        a_results = Parallel(n_jobs=n_cpu)(delayed(forward)
                                           (trial, dt, PI, A, B)
                                           for trial in spikes)
        alphas, norms = zip(*a_results)
        return np.array(alphas), np.array(norms)

    def get_backward_probabilities(self, spikes, dt, parallel=False):
        PI = self.initial_distribution
        A = self.transition
        B = self.emission
        betas = []
        if parallel:
            n_cpu = cpu_count() -1
        else:
            n_cpu = 1

        a_results = Parallel(n_jobs=n_cpu)(delayed(forward)(trial, dt, PI, A, B)
                                         for trial in spikes)
        _, norms = zip(*a_results)
        b_results = Parallel(n_jobs=n_cpu)(delayed(backward)(trial, dt, A, B, n)
                                           for trial, n in zip(spikes, norms))
        betas = np.array(b_results)

        return betas

    def get_gamma_probabilities(self, spikes, dt, parallel=False):
        PI = self.initial_distribution
        A = self.transition
        B = self.emission
        if parallel:
            n_cpu = cpu_count()-1
        else:
            n_cpu = 1

        results = Parallel(n_jobs=n_cpu)(delayed(baum_welch)(trial, dt, PI, A, B)
                                         for trial in spikes)
        gammas, _, _ = zip(*results)
        return np.array(gammas)

    def _update_cost(self, spikes, dt):
        spikes = spikes.astype(&#39;int&#39;)
        PI = self.initial_distribution
        A  = self.transition
        B  = self.emission
        cost, BIC, bestPaths, maxLogProb = compute_hmm_cost(spikes, dt, PI, A, B)
        self.cost = cost
        self.BIC = BIC
        self.max_log_prob = maxLogProb
        self.stat_arrays[&#39;best_sequences&#39;] = bestPaths

    def roll_back(self, iteration, spikes=None, dt=None):
        itrs = self.history[&#39;iterations&#39;]
        idx = np.where(itrs == iteration)[0]
        if len(idx) == 0:
            raise ValueError(&#39;Iteration %i not found in history&#39; % iteration)

        idx = idx[0]
        self.emission = self.history[&#39;B&#39;][idx]
        self.transition = self.history[&#39;A&#39;][idx]
        self.initial_distribution = self.history[&#39;PI&#39;][idx]
        self.iteration = iteration

        itrs = self.stat_arrays[&#39;iterations&#39;]
        idx = np.where(itrs == iteration)[0][0]
        self.fit_LL = self.stat_arrays[&#39;fit_LL&#39;][idx]
        self.max_log_prob = self.stat_arrays[&#39;max_log_prob&#39;][idx]
        self.BIC = self.stat_arrays[&#39;BIC&#39;][idx]
        self.cost = self.stat_arrays[&#39;cost&#39;][idx]
        if spikes is not None and dt is not None:
            self.stat_arrays[&#39;gamma_probabilities&#39;] = self.get_gamma_probabilities(spikes, dt)
            self._update_cost(spikes, dt)

        self._update_history()


class HmmHandler(object):
    def __init__(self, dat, save_dir=None):
        &#39;&#39;&#39;Takes a blechpy dataset object and fits HMMs for each tastant

        Parameters
        ----------
        dat: blechpy.dataset
        params: dict or list of dicts
            each dict must have fields:
                time_window: list of int, time window to cut around stimuli in ms
                convergence_thresh: float
                max_iter: int
                n_repeats: int
                unit_type: str, {&#39;single&#39;, &#39;pyramidal&#39;, &#39;interneuron&#39;, &#39;all&#39;}
                bin_size: time bin for spike array when fitting in seconds
                n_states: predicted number of states to fit
        &#39;&#39;&#39;
        if isinstance(dat, str):
            fd = dat
            dat = load_dataset(dat)
            if os.path.realpath(fd) != os.path.realpath(dat.root_dir):
                print(&#39;Changing dataset root_dir to match local directory&#39;)
                dat._change_root(fd)

            if dat is None:
                raise FileNotFoundError(&#39;No dataset.p file found given directory&#39;)

        if save_dir is None:
            save_dir = os.path.join(dat.root_dir,
                                    &#39;%s_analysis&#39; % dat.data_name)

        self._dataset = dat
        self.root_dir = dat.root_dir
        self.save_dir = save_dir
        self.h5_file = os.path.join(save_dir, &#39;%s_HMM_Analysis.hdf5&#39; % dat.data_name)
        self.load_params()

        if not os.path.isdir(save_dir):
            os.makedirs(save_dir)

        self.plot_dir = os.path.join(save_dir, &#39;HMM_Plots&#39;)
        if not os.path.isdir(self.plot_dir):
            os.makedirs(self.plot_dir)

        hmmIO.setup_hmm_hdf5(self.h5_file)
        # this function can be edited to account for parameters added in the
        # future
        # hmmIO.fix_hmm_overview(self.h5_file)

    def load_params(self):
        self._data_params = []
        self._fit_params = []
        h5_file = self.h5_file
        if not os.path.isfile(h5_file):
            return

        overview = self.get_data_overview()
        if overview.empty:
            return

        for i in overview.hmm_id:
            _, _, _, _, p = hmmIO.read_hmm_from_hdf5(h5_file, i)
            for k in list(p.keys()):
                if k not in HMM_PARAMS.keys():
                    _ = p.pop(k)

            self.add_params(p)

    def get_parameter_overview(self):
        df = pd.DataFrame(self._data_params)
        return df

    def get_data_overview(self):
        return hmmIO.get_hmm_overview_from_hdf5(self.h5_file)

    def run(self, parallel=True, overwrite=False, constraint_func=None):
        h5_file = self.h5_file
        rec_dir = self.root_dir
        if overwrite:
            fit_params = self._fit_params
        else:
            fit_params = [x for x in self._fit_params if not x[&#39;fitted&#39;]]

        if len(fit_params) == 0:
            return

        print(&#39;Running fittings&#39;)
        if parallel:
            n_cpu = np.min((cpu_count()-1, len(fit_params)))
        else:
            n_cpu = 1

        results = Parallel(n_jobs=n_cpu, verbose=100)(delayed(fit_hmm_mp)
                                                     (rec_dir, p, h5_file,
                                                      constraint_func)
                                                     for p in fit_params)


        memory.clear(warn=False)
        print(&#39;=&#39;*80)
        print(&#39;Fitting Complete&#39;)
        print(&#39;=&#39;*80)
        print(&#39;HMMs written to hdf5:&#39;)
        for hmm_id, written in results:
            print(&#39;%s : %s&#39; % (hmm_id, written))

        #self.plot_saved_models()
        self.load_params()

    def plot_saved_models(self):
        print(&#39;Plotting saved models&#39;)
        data = self.get_data_overview().set_index(&#39;hmm_id&#39;)
        rec_dir = self.root_dir
        for i, row in data.iterrows():
            hmm, _, params = load_hmm_from_hdf5(self.h5_file, i)
            spikes, dt, time = get_hmm_spike_data(rec_dir, params[&#39;unit_type&#39;],
                                                  params[&#39;channel&#39;],
                                                  time_start=params[&#39;time_start&#39;],
                                                  time_end=params[&#39;time_end&#39;],
                                                  dt=params[&#39;dt&#39;],
                                                  trials=params[&#39;n_trials&#39;],
                                                  area=params[&#39;area&#39;])
            plot_dir = os.path.join(self.plot_dir, &#39;hmm_%s&#39; % i)
            if not os.path.isdir(plot_dir):
                os.makedirs(plot_dir)

            print(&#39;Plotting HMM %s...&#39; % i)
            hmmplt.plot_hmm_figures(hmm, spikes, dt, time, save_dir=plot_dir)

    def add_params(self, params):
        if isinstance(params, list):
            for p in params:
                self.add_params(p)

            return
        elif not isinstance(params, dict):
            raise ValueError(&#39;Input must  be a dict or list of dicts&#39;)

        # Fill in blanks with defaults
        for k, v in HMM_PARAMS.items():
            if k not in params.keys():
                params[k] = v
                print(&#39;Parameter %s not provided. Using default value: %s&#39;
                      % (k, repr(v)))

        # Grab existing parameters
        data_params = self._data_params
        fit_params = self._fit_params

        # Get taste and trial info from dataset
        dat = self._dataset
        dim = dat.dig_in_mapping.query(&#39;exclude == False and spike_array == True&#39;)
        if params[&#39;taste&#39;] is None:
            tastes = dim[&#39;name&#39;].tolist()
            single_taste = True
        elif isinstance(params[&#39;taste&#39;], list):
            tastes = [t for t in params[&#39;taste&#39;] if any(dim[&#39;name&#39;] == t)]
            single_taste = False
        elif params[&#39;taste&#39;] == &#39;all&#39;:
            tastes = dim[&#39;name&#39;].tolist()
            single_taste = False
        else:
            tastes = [params[&#39;taste&#39;]]
            single_taste = True

        dim = dim.set_index(&#39;name&#39;)
        if not hasattr(dat, &#39;dig_in_trials&#39;):
            dat.create_trial_list()

        trials = dat.dig_in_trials
        hmm_ids = [x[&#39;hmm_id&#39;] for x in data_params]
        if single_taste:
            for t in tastes:
                p = params.copy()

                p[&#39;taste&#39;] = t
                # Skip if parameter is already in parameter set
                if any([hmmIO.compare_hmm_params(p, dp) for dp in data_params]):
                    print(&#39;Parameter set already in data_params, &#39;
                          &#39;to re-fit run with overwrite=True&#39;)
                    continue

                if t not in dim.index:
                    print(&#39;Taste %s not found in dig_in_mapping or marked to exclude. Skipping...&#39; % t)
                    continue

                if p[&#39;hmm_id&#39;] is None:
                    hid = get_new_id(hmm_ids)
                    p[&#39;hmm_id&#39;] = hid
                    hmm_ids.append(hid)

                p[&#39;channel&#39;] = dim.loc[t, &#39;channel&#39;]
                unit_names = query_units(dat, p[&#39;unit_type&#39;], area=p[&#39;area&#39;])
                p[&#39;n_cells&#39;] = len(unit_names)
                if p[&#39;n_trials&#39;] is None:
                    p[&#39;n_trials&#39;] = len(trials.query(&#39;name == @t&#39;))

                data_params.append(p)
                for i in range(p[&#39;n_repeats&#39;]):
                    fit_params.append(p.copy())

        else:
            if any([hmmIO.compare_hmm_params(p, dp) for dp in data_params]):
                print(&#39;Parameter set already in data_params, &#39;
                      &#39;to re-fit run with overwrite=True&#39;)
                return

            channels = [dim.loc[x,&#39;channel&#39;] for x in tastes]
            params[&#39;taste&#39;] = tastes
            params[&#39;channel&#39;] = channels

            # this is basically meaningless right now, since this if clause
            # should only be used with ConstrainedHMM which will fit 5
            # baseline states and 2 states per taste
            params[&#39;n_states&#39;] = params[&#39;n_states&#39;]*len(tastes)

            if params[&#39;hmm_id&#39;] is None:
                hid = get_new_id(hmm_ids)
                params[&#39;hmm_id&#39;] = hid
                hmm_ids.append(hid)

            unit_names = query_units(dat, params[&#39;unit_type&#39;],
                                     area=params[&#39;area&#39;])
            params[&#39;n_cells&#39;] = len(unit_names)
            if params[&#39;n_trials&#39;] is None:
                params[&#39;n_trials&#39;] = len(trials.query(&#39;name == @t&#39;))

            data_params.append(params)
            for i in range(params[&#39;n_repeats&#39;]):
                fit_params.append(params.copy())

        self._data_params = data_params
        self._fit_params = fit_params

    def get_hmm(self, hmm_id):
        return load_hmm_from_hdf5(self.h5_file, hmm_id)

    def delete_hmm(self, **kwargs):
        &#39;&#39;&#39;Deletes any HMMs whose parameters match the kwargs. i.e. n_states=2,
        taste=&#34;Saccharin&#34; would delete all 2-state HMMs for Saccharin trials
        also reload parameters from hdf5, so any added but un-fit params will
        be lost
        &#39;&#39;&#39;
        hmmIO.delete_hmm_from_hdf5(self.h5_file, **kwargs)
        self.load_params()


def sequential_constraint(PI, A, B):
    &#39;&#39;&#39;Forces all states to occur sequentially
    Can be passed to HmmHandler.run() or fit_hmm_mp as the constraint_func
    argument

    Parameters
    ----------
    PI: np.ndarray, initial state probability vector
    A: np.ndarray, transition matrix
    B: np.ndarray, emission or rate matrix

    Returns
    -------
    np, ndarray, np.ndarray, np.ndarray : PI, A, B
    &#39;&#39;&#39;
    n_states = len(PI)
    PI[0] = 1.0
    PI[1:] = 0.0
    for i in np.arange(n_states):
        if i &gt; 0:
            A[i, :i] = 0.0

        if i &lt; n_states-2:
            A[i, i+2:] = 0.0

        A[i, :] = A[i,:]/np.sum(A[i,:])

    A[-1, :] = 0.0
    A[-1, -1] = 1.0

    return PI, A, B


class ConstrainedHMM(PoissonHMM):
    def __init__(self, n_tastes, n_baseline=3, hmm_id=None):
        self.stat_arrays = {} # dict of cumulative stats to keep while fitting
                              # iterations, max_log_likelihood, fit log
                              # likelihood, cost, best_sequences, gamma
                              # probabilities, time, row_id
        self.n_tastes = n_tastes
        self.n_baseline = n_baseline
        n_states = n_baseline + 2*n_tastes
        super().__init__(n_states, hmm_id=hmm_id)

    def randomize(self, spikes, dt, time, row_id=None, constraint_func=None):
        # setup parameters 
        # make transition matrix
        # all baseline states have equal probability of staying or changing
        # into each other and the early states
        # each early state has high stay probability and low chance to transition into 
        n_trials, n_cells, n_steps = spikes.shape
        n_tastes = self.n_tastes
        n_baseline = self.n_baseline
        n_states = n_baseline + n_tastes*2

        # Transition Matrix: state X state, A[i,j] is prob to go from state i to state j
        unit = 1/(n_baseline + n_tastes)
        A0 = np.random.normal(unit, 0.01, (n_baseline, n_baseline)).astype(&#39;float64&#39;)
        A1 = np.vstack([[unit, 0]*n_tastes]*n_baseline).astype(&#39;float64&#39;)
        A2 = np.zeros((n_tastes*2, n_baseline)).astype(&#39;float64&#39;)
        A3 = np.zeros((n_tastes*2, n_tastes*2)).astype(&#39;float64&#39;)
        for i in range(n_tastes):
            j = 2*i
            A3[j, j] = np.min((0.999, np.random.normal(0.98, 0.01, 1)))
            A3[j, j+1] = 1-A3[j,j]
            A3[j+1, j] = 0
            A3[j+1, j+1] = 1

        A = np.hstack((np.vstack((A0, A2)), np.vstack((A1, A3))))

        # Rate Matrix: cells X states, Bij is firing rate of cell i in state j
        b_idx = np.where(time &lt; 0)[0]
        e_idx = np.where((time &gt;= 0) &amp; (time &lt; np.max(time)/2))[0]
        l_idx = np.where(time &gt;= np.max(time)/2)[0]
        if len(b_idx) == 0:
            b_idx = np.arange(n_steps)

        baseline = np.mean(np.sum(spikes[:, :, b_idx], axis=2), axis=0) / (len(b_idx)*dt)
        b_sd = np.std(np.sum(spikes[:, :, b_idx], axis=2), axis=0) / (len(b_idx)*dt)
        early = np.mean(np.sum(spikes[:, :, e_idx], axis=2), axis=0) / (len(e_idx)*dt)
        e_sd = np.std(np.sum(spikes[:, :, e_idx], axis=2), axis=0) / (len(e_idx)*dt)
        late = np.mean(np.sum(spikes[:, :, l_idx], axis=2), axis=0) / (len(l_idx)*dt)
        l_sd = np.std(np.sum(spikes[:, :, l_idx], axis=2), axis=0) / (len(l_idx)*dt)

        rates = np.zeros((n_cells, n_states))
        minFR = 1/n_steps
        for i in range(n_cells):
            row = [np.random.normal(baseline[i], b_sd[i], n_baseline)]
            for j in range(n_tastes):
                row.append(np.random.normal(early[i], e_sd[i], 1))
                row.append(np.random.normal(late[i], l_sd[i], 1))

            row = np.hstack(row)
            rates[i, :] = np.array([np.max((x, minFR)) for x in row])

        # Initial probabilities
        # Equal prob of all baseline states
        unit = 1/n_baseline
        PI = np.hstack([[np.random.normal(unit, 0.02, 1)[0]
                         for x in range(n_baseline)],
                        np.zeros((n_tastes*2,))])
        PI = PI/np.sum(PI)

        self.transition = A
        self.emission = rates
        self.initial_distribution = PI
        self.fitted = False
        self.converged = False
        self.iteration = 0
        self.stat_arrays[&#39;row_id&#39;] = row_id
        self._init_history()
        self.stat_arrays[&#39;gamma_probabilities&#39;] = self.get_gamma_probabilities(spikes, dt)
        self.stat_arrays[&#39;time&#39;] = time
        self._update_cost(spikes, dt)
        self.fit_LL = self.max_log_prob
        self._update_history()
        self._update_history()

    def fit(self, spikes, dt, time, max_iter = 500, threshold=1e-5, parallel=False):
        &#39;&#39;&#39;using parallels for processing trials actually seems to slow down
        processing (with 15 trials). Might still be useful if there is a very
        large nubmer of trials
        &#39;&#39;&#39;
        spikes = spikes.astype(&#39;int32&#39;)
        if (self.initial_distribution is None or
            self.transition is None or
            self.emission is None):
            raise ValueError(&#39;Must first initialize fit matrices either manually or via randomize&#39;)

        converged = False
        last_logl = None
        self.stat_arrays[&#39;time&#39;] = time
        while (not converged and (self.iteration &lt; max_iter)):
            self.fit_LL = self._step(spikes, dt, parallel=parallel)
            self._update_history()
            # if self.iteration &gt;= 100:
            #     trend = check_ll_trend(self, threshold)
            #     if trend == &#39;decreasing&#39;:
            #         return False
            #     elif trend == &#39;plateau&#39;:
            #         converged = True

            if last_logl is None:
                delta_ll = np.abs(self.fit_LL)
            else:
                delta_ll = np.abs((last_logl - self.fit_LL)/self.fit_LL)

            if (last_logl is not None and
                np.isfinite(delta_ll) and
                delta_ll &lt; threshold and
                np.isfinite(self.fit_LL) and
                self.iteration&gt;2):
                converged = True
                print(&#39;%s: %s: Change in log likelihood converged&#39; % (os.getpid(), self.hmm_id))

            last_logl = self.fit_LL

            # Convergence check is replaced by checking LL trend for plateau
            # converged = self.isConverged(convergence_thresh)
            print(&#39;%s: %s: Iter #%i complete. Log-likelihood is %.2E. Delta is %.2E&#39;
                  % (os.getpid(), self.hmm_id, self.iteration, self.fit_LL, delta_ll))

        self.fitted = True
        self.converged = converged
        return True

    def get_baseline_states(self):
        return np.arange(self.n_baseline)

    def get_early_states(self):
        return np.arange(self.n_baseline, self.n_states, 2)

    def get_late_states(self):
        return np.arange(self.n_baseline+1, self.n_states, 2)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="blechpy.analysis.poissonHMM.backward"><code class="name flex">
<span>def <span class="ident">backward</span></span>(<span>spikes, dt, A, B, norms)</span>
</code></dt>
<dd>
<div class="desc"><p>Runs the backward algorithm to compute beta = P(ot+1&hellip;oT | Xt=s)
Computes the probability of observing all future observations given the
current state at each time point</p>
<h2 id="paramters">Paramters</h2>
<p>spike : np.array, N x T matrix of spike counts
nStates : int, # of hidden states predicted
dt : float, timebin size in seconds
A : np.array, nStates x nStates matrix of transition probabilities
B : np.array, N x nStates matrix of estimated spike rates for each neuron</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>beta</code></strong> :&ensp;<code>np.array, nStates x T matrix</code> of <code><a title="blechpy.analysis.poissonHMM.backward" href="#blechpy.analysis.poissonHMM.backward">backward()</a> probabilities</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@njit
def backward(spikes, dt, A, B, norms):
    &#39;&#39;&#39; Runs the backward algorithm to compute beta = P(ot+1...oT | Xt=s)
    Computes the probability of observing all future observations given the
    current state at each time point

    Paramters
    ---------
    spike : np.array, N x T matrix of spike counts
    nStates : int, # of hidden states predicted
    dt : float, timebin size in seconds
    A : np.array, nStates x nStates matrix of transition probabilities
    B : np.array, N x nStates matrix of estimated spike rates for each neuron

    Returns
    -------
    beta : np.array, nStates x T matrix of backward probabilities
    &#39;&#39;&#39;
    _, A, B = fix_arrays(np.array([0]), A, B)
    nTimeSteps = spikes.shape[1]
    nStates = A.shape[0]
    beta = np.zeros((nStates, nTimeSteps))
    beta[:, -1] = 1  # Initialize final beta to 1 for all states
    tStep = list(range(nTimeSteps-1))
    tStep.reverse()
    for t in tStep:
        for s in range(nStates):
            tmp_em = log_emission(B[:,s], spikes[:,t+1], dt)
            tmp_b = np.log(beta[:,t+1]) + np.log(A[s,:]) + tmp_em
            beta[s,t] = np.sum(np.exp(tmp_b))

        beta[:, t] = beta[:, t] / norms[t+1]

    return beta</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.poissonHMM.baum_welch"><code class="name flex">
<span>def <span class="ident">baum_welch</span></span>(<span>trial_dat, dt, PI, A, B)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@njit
def baum_welch(trial_dat, dt, PI, A, B):
    alpha, norms = forward(trial_dat, dt, PI, A, B)
    beta = backward(trial_dat, dt, A, B, norms)
    tmp_gamma, tmp_epsilons = compute_baum_welch(trial_dat, dt, A, B, alpha, beta)
    return tmp_gamma, tmp_epsilons, norms</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.poissonHMM.check_ll_trend"><code class="name flex">
<span>def <span class="ident">check_ll_trend</span></span>(<span>hmm, thresh, n_iter=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Check the trend of the log-likelihood to see if it has plateaued, is
decreasing or is increasing</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_ll_trend(hmm, thresh, n_iter=None):
    &#39;&#39;&#39;Check the trend of the log-likelihood to see if it has plateaued, is
    decreasing or is increasing
    &#39;&#39;&#39;
    if n_iter is None:
        n_iter = hmm.iteration

    ll_hist = np.array(hmm.stat_arrays[&#39;max_log_prob&#39;])
    iterations = np.array(hmm.stat_arrays[&#39;iterations&#39;])
    if n_iter not in iterations:
        raise ValueError(&#39;Iteration %i is not in history&#39; % n_iter)

    idx = np.where(iterations &lt;= n_iter)[0]
    ll_hist = ll_hist[idx]
    filt_ll = gaussian_filter1d(ll_hist, 4)
    diff_ll = np.diff(filt_ll)

    # Linear fit, if overall trend is decreasing, it fails
    z = np.polyfit(range(len(ll_hist)), filt_ll, 1)
    if z[0] &lt;= 0:
        return &#39;decreasing&#39;

    # Check if it has plateaued
    if all(np.abs(diff_ll[-5:]) &lt;= thresh):
        return &#39;plateau&#39;

    # if its a maxima and hasn&#39;t plateaued it needs to continue fitting
    if np.max(filt_ll) == filt_ll[-1]:
        return &#39;increasing&#39;

    return &#39;flux&#39;</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.poissonHMM.compute_BIC"><code class="name flex">
<span>def <span class="ident">compute_BIC</span></span>(<span>PI, A, B, spikes=None, dt=None, maxLogProb=None, n_time_steps=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_BIC(PI, A, B, spikes=None, dt=None, maxLogProb=None, n_time_steps=None):
    if (maxLogProb is None or n_time_steps is None) and (spikes is None or dt is None):
        raise ValueError(&#39;Must provide max log prob and n_time_steps or spikes and dt&#39;)

    nParams = (A.shape[0]*(A.shape[1]-1) +
               (PI.shape[0]-1) +
               B.shape[0]*(B.shape[1]-1))
    if maxLogProb and n_time_steps:
        pass
    else:
        bestPaths, path_probs = compute_best_paths(spikes, dt, PI, A, B)
        maxLogProb = np.sum(path_probs)
        n_time_steps = spikes.shape[-1]

    BIC = -2 * maxLogProb + nParams * np.log(n_time_steps)
    return BIC, bestPaths, maxLogProb</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.poissonHMM.compute_baum_welch"><code class="name flex">
<span>def <span class="ident">compute_baum_welch</span></span>(<span>spikes, dt, A, B, alpha, beta)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@njit
def compute_baum_welch(spikes, dt, A, B, alpha, beta):
    _, A, B = fix_arrays(np.array([0]), A, B)
    nTimeSteps = spikes.shape[1]
    nStates = A.shape[0]
    gamma = np.zeros((nStates, nTimeSteps))
    epsilons = np.zeros((nStates, nStates, nTimeSteps-1))
    for t in range(nTimeSteps):
        tmp_g = np.exp(np.log(alpha[:, t]) + np.log(beta[:, t]))
        gamma[:, t] = tmp_g / np.sum(tmp_g)
        if t &lt; nTimeSteps-1:
            epsilonNumerator = np.zeros((nStates, nStates))
            for si in range(nStates):
                for sj in range(nStates):
                    probs = log_emission(B[:, sj], spikes[:, t+1], dt)
                    tmp_en = (np.log(alpha[si, t]) + np.log(A[si, sj]) +
                              np.log(beta[sj, t+1]) + probs)
                    epsilonNumerator[si, sj] = np.exp(tmp_en)

            epsilons[:, :, t] = epsilonNumerator / np.sum(epsilonNumerator)

    return gamma, epsilons</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.poissonHMM.compute_best_paths"><code class="name flex">
<span>def <span class="ident">compute_best_paths</span></span>(<span>spikes, dt, PI, A, B)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_best_paths(spikes, dt, PI, A, B):
    if len(spikes.shape) == 2:
        spikes = np.array([spikes])

    nTrials, nCells, nTimeSteps = spikes.shape
    bestPaths = np.zeros((nTrials, nTimeSteps))-1
    pathProbs = np.zeros((nTrials,))

    for i, trial in enumerate(spikes):
        bestPaths[i,:], pathProbs[i], _, _ = poisson_viterbi(trial, dt, PI,
                                                             A, B)
    return bestPaths, pathProbs</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.poissonHMM.compute_hmm_cost"><code class="name flex">
<span>def <span class="ident">compute_hmm_cost</span></span>(<span>spikes, dt, PI, A, B, win_size=0.25, true_rates=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_hmm_cost(spikes, dt, PI, A, B, win_size=0.25, true_rates=None):
    if true_rates is None:
        true_rates = convert_spikes_to_rates(spikes, dt, win_size,
                                             step_size=win_size)

    BIC, bestPaths, maxLogProb = compute_BIC(PI, A, B, spikes=spikes, dt=dt)
    hmm_rates = generate_rate_array_from_state_seq(bestPaths, B, dt, win_size,
                                                   step_size=win_size)
    RMSE = compute_rate_rmse(true_rates, hmm_rates)
    return RMSE, BIC, bestPaths, maxLogProb</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.poissonHMM.compute_new_matrices"><code class="name flex">
<span>def <span class="ident">compute_new_matrices</span></span>(<span>spikes, dt, gammas, epsilons)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_new_matrices(spikes, dt, gammas, epsilons):
    nTrials, nCells, nTimeSteps = spikes.shape
    n_states = gammas.shape[1]
    minFR = 1/(nTimeSteps*dt)

    PI = np.mean(gammas[:, :, 0], axis=0)
    A = np.zeros((n_states, n_states))
    B = np.zeros((nCells, n_states))
    for si in range(n_states):
        for sj in range(n_states):
            Anumer = np.sum(epsilons[:, si, sj, :])
            Adenom = np.sum(gammas[:, si, -1])
            if np.isfinite(Adenom) and Adenom != 0.:
                A[si, sj] = Anumer / Adenom
            else:
                A[si, sj] = 0 # incase of floating point errors resulting in zeros

        #A[si, A[si,:] &lt; 1e-50] = 0
        row = A[si,:]
        if np.sum(row) == 0.0:
            A[si, sj] = 1.0
        else:
            A[si, :] = A[si,:] / np.sum(row)


    for si in range(n_states):
        for tri in range(nTrials):
            for t in range(nTimeSteps-1):
                for u in range(nCells):
                    B[u,si] = B[u,si] + gammas[tri, si, t]*spikes[tri, u, t]

    # Convert and really small transition values into zeros
    #A[A &lt; 1e-50] = 0
    #sums = np.sum(A, axis=1)
    #A = A/np.sum(A, axis=1) # This divides columns not rows
    #Bnumer = np.sum(np.array([np.matmul(tmp_y, tmp_g.T)
    #                          for tmp_y, tmp_g in zip(spikes, gammas)]),
    #                axis=0)
    Bdenom =  np.sum(np.sum(gammas, axis=2), axis=0)
    B = (B / Bdenom)/dt
    B[B &lt; minFR] = minFR
    A[A &lt;= MIN_PROB] = 0.0

    return PI, A, B</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.poissonHMM.compute_rate_rmse"><code class="name flex">
<span>def <span class="ident">compute_rate_rmse</span></span>(<span>rates1, rates2)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@njit
def compute_rate_rmse(rates1, rates2):
    # Compute RMSE per trial
    # Mean over trials
    n_trials, n_cells, n_steps = rates1.shape
    RMSE = np.zeros((n_trials,))
    for i in range(n_trials):
        t1 = rates1[i, :, :]
        t2 = rates2[i, :, :]
        # Compute RMSE from euclidean distances at each time point
        distances = np.zeros((n_steps,))
        for j in range(n_steps):
            distances[j] =  mt.euclidean(t1[:,j], t2[:,j])

        RMSE[i] = np.sqrt(np.mean(np.power(distances,2)))

    return np.mean(RMSE)</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.poissonHMM.convert_path_state_numbers"><code class="name flex">
<span>def <span class="ident">convert_path_state_numbers</span></span>(<span>paths, state_map)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def convert_path_state_numbers(paths, state_map):
    newPaths = np.zeros(paths.shape)
    for k,v in state_map.items():
        idx = np.where(paths == k)
        newPaths[idx] = v

    return newPaths</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.poissonHMM.convert_spikes_to_rates"><code class="name flex">
<span>def <span class="ident">convert_spikes_to_rates</span></span>(<span>spikes, dt, win_size, step_size=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@memory.cache
@njit
def convert_spikes_to_rates(spikes, dt, win_size, step_size=None):
    if step_size is None:
        step_size = win_size

    n_trials, n_cells, n_steps = spikes.shape
    n_pts = int(win_size/dt)
    n_step_pts = int(step_size/dt)
    win_starts = np.arange(0, n_steps, n_step_pts)
    out = np.zeros((n_trials, n_cells, len(win_starts)))
    for i, w in enumerate(win_starts):
        out[:, :, i] = np.sum(spikes[:, :, w:w+n_pts], axis=2) / win_size

    return out</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.poissonHMM.fast_factorial"><code class="name flex">
<span>def <span class="ident">fast_factorial</span></span>(<span>x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@njit
def fast_factorial(x):
    if x &lt; len(FACTORIAL_LOOKUP):
        return FACTORIAL_LOOKUP[x]
    else:
        y = 1
        for i in range(1,x+1):
            y = y*i

        return y</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.poissonHMM.fit_hmm_mp"><code class="name flex">
<span>def <span class="ident">fit_hmm_mp</span></span>(<span>rec_dir, params, h5_file=None, constraint_func=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit_hmm_mp(rec_dir, params, h5_file=None, constraint_func=None):
    hmm_id = params[&#39;hmm_id&#39;]
    n_states = params[&#39;n_states&#39;]
    dt = params[&#39;dt&#39;]
    time_start = params[&#39;time_start&#39;]
    time_end = params[&#39;time_end&#39;]
    max_iter = params[&#39;max_iter&#39;]
    threshold = params[&#39;threshold&#39;]
    unit_type = params[&#39;unit_type&#39;]
    channels = params[&#39;channel&#39;]
    tastes = params[&#39;taste&#39;]
    n_trials = params[&#39;n_trials&#39;]
    if &#39;area&#39; in params.keys():
        area = params[&#39;area&#39;]
    else:
        area = None

    if not isinstance(channels, list):
        channels = [channels]

    if not isinstance(tastes, list):
        tastes = [tastes]

    spikes = []
    row_id = []
    time = None
    for ch, tst in zip(channels, tastes):
        tmp_s, _, time = get_hmm_spike_data(rec_dir, unit_type, ch,
                                             time_start=time_start,
                                             time_end=time_end, dt=dt,
                                             trials=n_trials, area=area)
        tmp_id = np.vstack([(hmm_id, ch, tst, x) for x in range(tmp_s.shape[0])])
        spikes.append(tmp_s)
        row_id.append(tmp_id)

    spikes = np.vstack(spikes)
    row_id = np.vstack(row_id)

    if params[&#39;hmm_class&#39;] == &#39;PoissonHMM&#39;:
        hmm = PoissonHMM(n_states, hmm_id=hmm_id)
    elif params[&#39;hmm_class&#39;] == &#39;ConstrainedHMM&#39;:
        hmm = ConstrainedHMM(len(channels), hmm_id=hmm_id)

    hmm.randomize(spikes, dt, time, row_id=row_id, constraint_func=constraint_func)
    success = hmm.fit(spikes, dt, time, max_iter=max_iter, threshold=threshold)
    if not success:
        print(&#39;%s: Fitting Aborted for hmm %s&#39; % (os.getpid(), hmm_id))
        if h5_file:
            return hmm_id, False
        else:
            return hmm_id, hmm

    # hmm = roll_back_hmm_to_best(hmm, spikes, dt, threshold)
    print(&#39;%s: Done Fitting for hmm %s&#39; % (os.getpid(), hmm_id))
    written = False
    if h5_file:
        pid = os.getpid()
        lock_file = h5_file + &#39;.lock&#39;
        while os.path.exists(lock_file):
            print(&#39;%s: Waiting for file lock&#39; % pid)
            sys_time.sleep(20)

        locked = True
        while locked:
            try:
                os.mknod(lock_file)
                locked=False
            except:
                sys_time.sleep(10)

        try:
            old_hmm, _, old_params = load_hmm_from_hdf5(h5_file, hmm_id)

            if old_hmm is None:
                print(&#39;%s: No existing HMM %s. Writing ...&#39; % (pid, hmm_id))
                hmmIO.write_hmm_to_hdf5(h5_file, hmm, params)
                written = True
            else:
                print(&#39;%s: Existing HMM %s found. Comparing log likelihood ...&#39; % (pid, hmm_id))
                print(&#39;New %.3E vs Old %.3E&#39; % (hmm.fit_LL, old_hmm.fit_LL))
                if hmm.fit_LL &gt; old_hmm.fit_LL:
                    print(&#39;%s: Replacing HMM %s due to higher log likelihood&#39; % (pid, hmm_id))
                    hmmIO.write_hmm_to_hdf5(h5_file, hmm, params)
                    written = True

        except Exception as e:
            os.remove(lock_file)
            raise Exception(e)

        os.remove(lock_file)
        del old_hmm, hmm, spikes, dt, time
        return hmm_id, written
    else:
        return hmm_id, hmm</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.poissonHMM.fix_arrays"><code class="name flex">
<span>def <span class="ident">fix_arrays</span></span>(<span>PI, A, B)</span>
</code></dt>
<dd>
<div class="desc"><p>copy and remove zero values so that log probabilities can be computed</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@njit
def fix_arrays(PI,A,B):
    &#39;&#39;&#39;copy and remove zero values so that log probabilities can be computed
    &#39;&#39;&#39;
    PI = PI.copy()
    A = A.copy()
    B = B.copy()
    nx, ny = A.shape
    for i in range(nx):
        for j in range(ny):
            if A[i,j] == 0.:
                A[i,j] = MIN_PROB

    nx, ny = B.shape
    for i in range(nx):
        for j in range(ny):
            if B[i,j] == 0.:
                B[i,j] = MIN_PROB

    for i in range(len(PI)):
        if PI[i] == 0.:
            PI[i] = MIN_PROB

    return PI, A, B</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.poissonHMM.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>spikes, dt, PI, A, B)</span>
</code></dt>
<dd>
<div class="desc"><p>Run forward algorithm to compute alpha = P(Xt = i| o1&hellip;ot, pi)
Gives the probabilities of being in a specific state at each time point
given the past observations and initial probabilities</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>spikes</code></strong> :&ensp;<code>np.array</code></dt>
<dd>N x T matrix of spike counts with each entry ((i,j)) holding the # of
spikes from neuron i in timebine j</dd>
<dt><strong><code>nStates</code></strong> :&ensp;<code>int, #</code> of <code>hidden states predicted to have generate the spikes</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>dt</code></strong> :&ensp;<code>float, timebin in seconds (i.e. 0.001)</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>PI</code></strong> :&ensp;<code>np.array</code></dt>
<dd>nStates x 1 vector of initial state probabilities</dd>
<dt><strong><code>A</code></strong> :&ensp;<code>np.array</code></dt>
<dd>nStates x nStates state transmission matrix with each entry ((i,j))
giving the probability of transitioning from state i to state j</dd>
<dt><strong><code>B</code></strong> :&ensp;<code>np.array</code></dt>
<dd>N x nSates rate matrix. Each entry ((i,j)) gives this predicited rate
of neuron i in state j</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>alpha</code></strong> :&ensp;<code>np.array</code></dt>
<dd>nStates x T matrix of forward probabilites. Each entry (i,j) gives
P(Xt = i | o1,&hellip;,oj, pi)</dd>
<dt><strong><code>norms</code></strong> :&ensp;<code>np.array</code></dt>
<dd>1 x T vector of norm used to normalize alpha to be a probability
distribution and also to scale the outputs of the backward algorithm.
norms(t) = sum(alpha(:,t))</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@njit
def forward(spikes, dt, PI, A, B):
    &#39;&#39;&#39;Run forward algorithm to compute alpha = P(Xt = i| o1...ot, pi)
    Gives the probabilities of being in a specific state at each time point
    given the past observations and initial probabilities

    Parameters
    ----------
    spikes : np.array
        N x T matrix of spike counts with each entry ((i,j)) holding the # of
        spikes from neuron i in timebine j
    nStates : int, # of hidden states predicted to have generate the spikes
    dt : float, timebin in seconds (i.e. 0.001)
    PI : np.array
        nStates x 1 vector of initial state probabilities
    A : np.array
        nStates x nStates state transmission matrix with each entry ((i,j))
        giving the probability of transitioning from state i to state j
    B : np.array
        N x nSates rate matrix. Each entry ((i,j)) gives this predicited rate
        of neuron i in state j

    Returns
    -------
    alpha : np.array
        nStates x T matrix of forward probabilites. Each entry (i,j) gives
        P(Xt = i | o1,...,oj, pi)
    norms : np.array
        1 x T vector of norm used to normalize alpha to be a probability
        distribution and also to scale the outputs of the backward algorithm.
        norms(t) = sum(alpha(:,t))
    &#39;&#39;&#39;
    nTimeSteps = spikes.shape[1]
    nStates = A.shape[0]
    PI, A, B = fix_arrays(PI, A, B)

    # For each state, use the the initial state distribution and spike counts
    # to initialize alpha(:,1)
    #row = np.array([PI[i] * np.prod(poisson(B[:,i], spikes[:,0], dt))
    #row = np.array([np.log(PI[i]) + np.sum(np.log(poisson(B[:,i], spikes[:,0], dt)))
    #                for i in range(nStates)])
    a0 = [np.exp(np.log(PI[i]) + log_emission(B[:,i], spikes[:,0], dt))
          for i in range(nStates)]
    a0 = np.array(a0)
    alpha = np.zeros((nStates, nTimeSteps))
    norms = [np.sum(a0)]
    alpha[:, 0] = a0/norms[0]
    for t in range(1, nTimeSteps):
        for s in range(nStates):
            tmp_em = log_emission(B[:,s], spikes[:, t], dt)
            tmp_a = np.sum(np.exp(np.log(alpha[:, t-1]) + np.log(A[:,s])))
            tmp = np.exp(tmp_em + np.log(tmp_a))
            alpha[s,t] = tmp

        tmp_norm = np.sum(alpha[:,t])
        norms.append(tmp_norm)
        alpha[:, t] = alpha[:,t] / tmp_norm

    return alpha, norms</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.poissonHMM.generate_rate_array_from_state_seq"><code class="name flex">
<span>def <span class="ident">generate_rate_array_from_state_seq</span></span>(<span>bestPaths, B, dt, win_size, step_size=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@memory.cache
@njit
def generate_rate_array_from_state_seq(bestPaths, B, dt, win_size,
                                       step_size=None):
    if not step_size:
        step_size = win_size

    n_trials, n_steps = bestPaths.shape
    n_cells, n_states = B.shape
    rates = np.zeros((n_trials, n_cells, n_steps))
    for j in range(n_trials):
        seq = bestPaths[j, :].astype(np.int64)
        rates[j, :, :] = B[:, seq]

    n_pts = int(win_size / dt)
    n_step_pts = int(step_size/dt)
    win_starts = np.arange(0, n_steps, n_step_pts)
    mean_rates = np.zeros((n_trials, n_cells, len(win_starts)))
    for i, w in enumerate(win_starts):
        mean_rates[:, :, i] = np.sum(rates[:, : , w:w+n_pts], axis=2) / n_pts

    return mean_rates</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.poissonHMM.get_hmm_spike_data"><code class="name flex">
<span>def <span class="ident">get_hmm_spike_data</span></span>(<span>rec_dir, unit_type, channel, time_start=None, time_end=None, dt=None, trials=None, area=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@memory.cache
def get_hmm_spike_data(rec_dir, unit_type, channel, time_start=None,
                       time_end=None, dt=None, trials=None, area=None):
    # unit type can be &#39;single&#39;, &#39;pyramidal&#39;, or &#39;interneuron&#39;, or a list of unit names
    if isinstance(unit_type, str):
        units = query_units(rec_dir, unit_type, area=area)
    elif isinstance(unit_type, list):
        units = unit_type

    time, spike_array = h5io.get_spike_data(rec_dir, units, channel, trials=trials)
    spike_array = spike_array.astype(np.int32)
    if len(units) == 1:
        spike_array = np.expand_dims(spike_array, 1)

    time = time.astype(np.float64)
    curr_dt = np.unique(np.diff(time))[0] / 1000
    if dt is not None and curr_dt &lt; dt:
        print(&#39;%s: Rebinning Spike Array&#39; % os.getpid())
        spike_array, time = rebin_spike_array(spike_array, curr_dt, time, dt)
    elif dt is not None and curr_dt &gt; dt:
        raise ValueError(&#39;Cannot upsample spike array from %f sec &#39;
                         &#39;bins to %f sec bins&#39; % (dt, curr_dt))
    else:
        dt = curr_dt

    if time_start is not None and time_end is not None:
        print(&#39;%s: Trimming spike array&#39; % os.getpid())
        idx = np.where((time &gt;= time_start) &amp; (time &lt; time_end))[0]
        time = time[idx]
        spike_array = spike_array[:, :, idx]

    return spike_array, dt, time</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.poissonHMM.get_new_id"><code class="name flex">
<span>def <span class="ident">get_new_id</span></span>(<span>ids=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_new_id(ids=None):
    if ids is None or len(ids) == 0:
        return 0

    nums = np.arange(0, np.max(ids) + 2)
    diff_nums = [x for x in nums if x not in ids]
    return np.min(diff_nums)</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.poissonHMM.isConverged"><code class="name flex">
<span>def <span class="ident">isConverged</span></span>(<span>hmm, thresh)</span>
</code></dt>
<dd>
<div class="desc"><p>Check HMM convergence based on the log-likelihood
NOT WORKING YET</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def isConverged(hmm, thresh):
    &#39;&#39;&#39;Check HMM convergence based on the log-likelihood
    NOT WORKING YET
    &#39;&#39;&#39;
    pass</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.poissonHMM.load_hmm_from_hdf5"><code class="name flex">
<span>def <span class="ident">load_hmm_from_hdf5</span></span>(<span>h5_file, hmm_id)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_hmm_from_hdf5(h5_file, hmm_id):
    hmm_id = int(hmm_id)
    existing_hmm = hmmIO.read_hmm_from_hdf5(h5_file, hmm_id)
    if existing_hmm is None:
        return None, None, None

    PI, A, B, stat_arrays, params = existing_hmm
    hmm = PoissonHMM(params[&#39;n_states&#39;], hmm_id=hmm_id)
    hmm._init_history()
    hmm.initial_distribution = PI
    hmm.transition = A
    hmm.emission = B
    hmm.iteration = params[&#39;n_iterations&#39;]
    for k,v in stat_arrays.items():
        if k in hmm.stat_arrays.keys() and isinstance(hmm.stat_arrays[k], list):
            hmm.stat_arrays[k] = list(v)
        else:
            hmm.stat_arrays[k] = v

    hmm.BIC = params.pop(&#39;BIC&#39;)
    hmm.converged = params.pop(&#39;converged&#39;)
    hmm.fitted = params.pop(&#39;fitted&#39;)
    hmm.cost = params.pop(&#39;cost&#39;)
    hmm.fit_LL = params.pop(&#39;log_likelihood&#39;)
    hmm.max_log_prob = params.pop(&#39;max_log_prob&#39;)

    return hmm, stat_arrays[&#39;time&#39;], params</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.poissonHMM.log_emission"><code class="name flex">
<span>def <span class="ident">log_emission</span></span>(<span>rate, n, dt)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@njit
def log_emission(rate, n , dt):
    return np.sum(np.log(poisson(rate, n, dt)))</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.poissonHMM.match_states"><code class="name flex">
<span>def <span class="ident">match_states</span></span>(<span>emission1, emission2)</span>
</code></dt>
<dd>
<div class="desc"><p>Takes 2 Cell X State firing rate matrices and determines which states
are most similar. Returns dict mapping emission2 states to emission1 states</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def match_states(emission1, emission2):
    &#39;&#39;&#39;Takes 2 Cell X State firing rate matrices and determines which states
    are most similar. Returns dict mapping emission2 states to emission1 states
    &#39;&#39;&#39;
    distances = np.zeros((emission1.shape[1], emission2.shape[1]))
    for x, y in it.product(range(emission1.shape[1]), range(emission2.shape[1])):
        tmp = mt.euclidean(emission1[:, x], emission2[:, y])
        distances[x, y] = tmp

    states = list(range(emission2.shape[1]))
    out = {}
    for i in range(emission2.shape[1]):
        s = np.argmin(distances[:,i])
        r = np.argmin(distances[s, :])
        if r == i and s in states:
            out[i] = s
            idx = np.where(states == s)[0]
            states.pop(int(idx))

    for i in range(emission2.shape[1]):
        if i not in out:
            s = np.argmin(distances[states, i])
            out[i] = states[s]

    return out</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.poissonHMM.poisson"><code class="name flex">
<span>def <span class="ident">poisson</span></span>(<span>rate, n, dt)</span>
</code></dt>
<dd>
<div class="desc"><p>Gives probability of each neurons spike count assuming poisson spiking</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@njit
def poisson(rate, n, dt):
    &#39;&#39;&#39;Gives probability of each neurons spike count assuming poisson spiking
    &#39;&#39;&#39;
    #tmp = np.power(rate*dt, n) / np.array([fast_factorial(x) for x in n])
    #tmp = tmp * np.exp(-rate*dt)
    tmp = n*np.log(rate*dt) - np.array([np.log(fast_factorial(x)) for x in n])
    tmp = tmp - rate*dt
    return np.exp(tmp)</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.poissonHMM.poisson_viterbi"><code class="name flex">
<span>def <span class="ident">poisson_viterbi</span></span>(<span>spikes, dt, PI, A, B)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def poisson_viterbi(spikes, dt, PI, A, B):
    n_states = A.shape[0]
    PI, A, B = fix_arrays(PI, A, B)
    n_cells, n_steps = spikes.shape
    T1 = np.ones((n_states, n_steps))*1e-300
    T2 = np.zeros((n_states, n_steps))
    T1[:, 0] = [np.log(PI[i])+np.sum(np.log(poisson(B[:,i], spikes[:,0], dt)))
                for i in range(n_states)]
    #for t,s in it.product(range(1,n_steps), range(n_states)):
    for t in range(1,n_steps):
        for s in range(n_states):
            probs = np.sum(np.log(poisson(B[:,s], spikes[:,t], dt)))
            vec1 = T1[:,t-1]+np.log(A[:,s])+probs
            T1[s,t] = np.max(vec1)
            T2[s,t] = np.argmax(vec1)

    best_end_state = np.argmax(T1[:,-1])
    max_log_prob = T1[best_end_state, -1]
    bestPath = np.zeros((n_steps,))
    bestPath[-1] = best_end_state
    tStep = list(range(n_steps-1))
    tStep.reverse()
    for t in tStep:
        bestPath[t] = T2[int(bestPath[t+1]), t+1]

    return bestPath, max_log_prob, T1, T2</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.poissonHMM.poisson_viterbi_deprecated"><code class="name flex">
<span>def <span class="ident">poisson_viterbi_deprecated</span></span>(<span>spikes, dt, PI, A, B)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>spikes</code></strong> :&ensp;<code>np.array, Neuron X Time matrix</code> of <code>spike counts</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>PI</code></strong> :&ensp;<code>np.array, nStates x 1 vector</code> of <code>initial state probabilities</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>A</code></strong> :&ensp;<code>np.array, nStates X nStates matric</code> of <code>state transition probabilities</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>B</code></strong> :&ensp;<code>np.array, Neuron X States matrix</code> of <code>estimated firing rates</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>dt</code></strong> :&ensp;<code>float, time step size in seconds</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>bestPath</code></strong> :&ensp;<code>np.array</code></dt>
<dd>1 x Time vector of states representing the most likely hidden state
sequence</dd>
<dt><strong><code>maxPathLogProb</code></strong> :&ensp;<code>float</code></dt>
<dd>Log probability of the most likely state sequence</dd>
<dt><strong><code>T1</code></strong> :&ensp;<code>np.array</code></dt>
<dd>State X Time matrix where each entry (i,j) gives the log probability of
the the most likely path so far ending in state i that generates
observations o1,&hellip;, oj</dd>
<dt><strong><code>T2</code></strong> :&ensp;<code>np.array</code></dt>
<dd>State X Time matrix of back pointers where each entry (i,j) gives the
state x(j-1) on the most likely path so far ending in state i</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def poisson_viterbi_deprecated(spikes, dt, PI, A, B):
    &#39;&#39;&#39;
    Parameters
    ----------
    spikes : np.array, Neuron X Time matrix of spike counts
    PI : np.array, nStates x 1 vector of initial state probabilities
    A : np.array, nStates X nStates matric of state transition probabilities
    B : np.array, Neuron X States matrix of estimated firing rates
    dt : float, time step size in seconds

    Returns
    -------
    bestPath : np.array
        1 x Time vector of states representing the most likely hidden state
        sequence
    maxPathLogProb : float
        Log probability of the most likely state sequence
    T1 : np.array
        State X Time matrix where each entry (i,j) gives the log probability of
        the the most likely path so far ending in state i that generates
        observations o1,..., oj
    T2: np.array
        State X Time matrix of back pointers where each entry (i,j) gives the
        state x(j-1) on the most likely path so far ending in state i
    &#39;&#39;&#39;
    if A.shape[0] != A.shape[1]:
        raise ValueError(&#39;Transition matrix is not square&#39;)

    nStates = A.shape[0]
    nCells, nTimeSteps = spikes.shape
    # get rid of zeros for computation 
    A[np.where(A==0)] = 1e-300
    T1 = np.zeros((nStates, nTimeSteps))
    T2 = np.zeros((nStates, nTimeSteps))
    T1[:,0] = np.array([np.log(PI[i]) +
                        np.log(np.prod(poisson(B[:,i], spikes[:, 1], dt)))
                        for i in range(nStates)])
    for t, s in it.product(range(1,nTimeSteps), range(nStates)):
        probs = np.log(np.prod(poisson(B[:, s], spikes[:, t], dt)))
        vec2 = T1[:, t-1] + np.log(A[:,s])
        vec1 = vec2 + probs
        T1[s, t] = np.max(vec1)
        idx = np.argmax(vec1)
        T2[s, t] = idx

    bestPathEndState = np.argmax(T1[:, -1])
    maxPathLogProb = T1[bestPathEndState, -1]
    bestPath = np.zeros((nTimeSteps,))
    bestPath[-1] = bestPathEndState
    tStep = list(range(nTimeSteps-1))
    tStep.reverse()
    for t in tStep:
        bestPath[t] = T2[int(bestPath[t+1]), t+1]

    return bestPath, maxPathLogProb, T1, T2</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.poissonHMM.query_units"><code class="name flex">
<span>def <span class="ident">query_units</span></span>(<span>dat, unit_type, area=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the units names of all units in the dataset that match unit_type</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dat</code></strong> :&ensp;<code>blechpy.dataset</code> or <code>str</code></dt>
<dd>Can either be a dataset object or the str path to the recording
directory containing that data .h5 object</dd>
<dt><strong><code>unit_type</code></strong> :&ensp;<code>str, {'single', 'pyramidal', 'interneuron', 'all'}</code></dt>
<dd>determines whether to return 'single' units, 'pyramidal' (regular
spiking single) units, 'interneuron' (fast spiking single) units, or
'all' units</dd>
<dt><strong><code>area</code></strong> :&ensp;<code>str</code></dt>
<dd>brain area of cells to return, must match area in
dataset.electrode_mapping</dd>
</dl>
<h2 id="returns">Returns</h2>
<pre><code>list of str : unit_names
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@memory.cache
def query_units(dat, unit_type, area=None):
    &#39;&#39;&#39;Returns the units names of all units in the dataset that match unit_type

    Parameters
    ----------
    dat : blechpy.dataset or str
        Can either be a dataset object or the str path to the recording
        directory containing that data .h5 object
    unit_type : str, {&#39;single&#39;, &#39;pyramidal&#39;, &#39;interneuron&#39;, &#39;all&#39;}
        determines whether to return &#39;single&#39; units, &#39;pyramidal&#39; (regular
        spiking single) units, &#39;interneuron&#39; (fast spiking single) units, or
        &#39;all&#39; units
    area : str
        brain area of cells to return, must match area in
        dataset.electrode_mapping

    Returns
    -------
        list of str : unit_names
    &#39;&#39;&#39;
    if isinstance(dat, str):
        units = h5io.get_unit_table(dat)
        el_map = h5io.get_electrode_mapping(dat)
    else:
        units = dat.get_unit_table()
        el_map = dat.electrode_mapping.copy()

    u_str = unit_type.lower()
    q_str = &#39;&#39;
    if u_str == &#39;single&#39;:
        q_str = &#39;single_unit == True&#39;
    elif u_str == &#39;pyramidal&#39;:
        q_str = &#39;single_unit == True and regular_spiking == True&#39;
    elif u_str == &#39;interneuron&#39;:
        q_str = &#39;single_unit == True and fast_spiking == True&#39;
    elif u_str == &#39;all&#39;:
        return units[&#39;unit_name&#39;].tolist()
    else:
        raise ValueError(&#39;Invalid unit_type %s. Must be &#39;
                         &#39;single, pyramidal, interneuron or all&#39; % u_str)

    units = units.query(q_str)
    if area is None or area == &#39;&#39; or area == &#39;None&#39;:
        return units[&#39;unit_name&#39;].to_list()

    out = []
    el_map = el_map.set_index(&#39;Electrode&#39;)
    for i, row in units.iterrows():
        if el_map.loc[row[&#39;electrode&#39;], &#39;area&#39;] == area:
            out.append(row[&#39;unit_name&#39;])

    return out</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.poissonHMM.rebin_spike_array"><code class="name flex">
<span>def <span class="ident">rebin_spike_array</span></span>(<span>spikes, dt, time, new_dt)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@memory.cache
@njit
def rebin_spike_array(spikes, dt, time, new_dt):
    if dt == new_dt:
        return spikes, time

    n_trials, n_cells, n_steps = spikes.shape
    n_bins = int(new_dt/dt)
    new_time = np.arange(time[0], time[-1], n_bins)
    new_spikes = np.zeros((n_trials, n_cells, len(new_time)))
    for i, w in enumerate(new_time):
        idx = np.where((time &gt;= w) &amp; (time &lt; w+new_dt))[0]
        new_spikes[:,:,i] = np.sum(spikes[:,:,idx], axis=-1)

    return new_spikes.astype(np.int32), new_time</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.poissonHMM.roll_back_hmm_to_best"><code class="name flex">
<span>def <span class="ident">roll_back_hmm_to_best</span></span>(<span>hmm, spikes, dt, thresh)</span>
</code></dt>
<dd>
<div class="desc"><p>Looks at the log likelihood over fitting and determines the best
iteration to have stopped at by choosing a local maxima during a period
where the smoothed LL trace has plateaued</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def roll_back_hmm_to_best(hmm, spikes, dt, thresh):
    &#39;&#39;&#39;Looks at the log likelihood over fitting and determines the best
    iteration to have stopped at by choosing a local maxima during a period
    where the smoothed LL trace has plateaued
    &#39;&#39;&#39;
    ll_hist = np.array(hmm.stat_arrays[&#39;max_log_prob&#39;])
    idx = np.where(np.isfinite(ll_hist))[0]
    if len(idx) == 0:
        return hmm

    iterations = np.array(hmm.stat_arrays[&#39;iterations&#39;])
    ll_hist = ll_hist[idx]
    iterations = iterations[idx]
    filt_ll = gaussian_filter1d(ll_hist, 4)
    diff_ll = np.diff(filt_ll)
    below = np.where(np.abs(diff_ll) &lt; thresh)[0] + 1 # since diff_ll is 1 smaller than ll_hist
    # Exclude maxima less than 50 iterations since its pretty spikey early on
    below = [x for x in below if (iterations[x] &gt; 50)]
    # If there are none that fit criteria, just pick best past 50
    if len(below) == 0:
        below = np.where(iterations &gt; 50)[0]

    if len(below) == 0:
        below = np.arange(len(iterations))

    below = below[below&gt;2]

    tmp = [x for x in below if check_ll_trend(hmm, thresh, n_iter=iterations[x]) == &#39;plateau&#39;]
    if len(tmp) != 0:
        below = tmp

    maxima = np.argmax(ll_hist[below]) # this gives the index in below
    maxima = iterations[below[maxima]] # this is the iteration at which the maxima occurred
    hmm.roll_back(maxima, spikes=spikes, dt=dt)
    return hmm</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.poissonHMM.sequential_constraint"><code class="name flex">
<span>def <span class="ident">sequential_constraint</span></span>(<span>PI, A, B)</span>
</code></dt>
<dd>
<div class="desc"><p>Forces all states to occur sequentially
Can be passed to HmmHandler.run() or fit_hmm_mp as the constraint_func
argument</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>PI</code></strong> :&ensp;<code>np.ndarray, initial state probability vector</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>A</code></strong> :&ensp;<code>np.ndarray, transition matrix</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>B</code></strong> :&ensp;<code>np.ndarray, emission</code> or <code>rate matrix</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np, ndarray, np.ndarray, np.ndarray : PI, A, B</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sequential_constraint(PI, A, B):
    &#39;&#39;&#39;Forces all states to occur sequentially
    Can be passed to HmmHandler.run() or fit_hmm_mp as the constraint_func
    argument

    Parameters
    ----------
    PI: np.ndarray, initial state probability vector
    A: np.ndarray, transition matrix
    B: np.ndarray, emission or rate matrix

    Returns
    -------
    np, ndarray, np.ndarray, np.ndarray : PI, A, B
    &#39;&#39;&#39;
    n_states = len(PI)
    PI[0] = 1.0
    PI[1:] = 0.0
    for i in np.arange(n_states):
        if i &gt; 0:
            A[i, :i] = 0.0

        if i &lt; n_states-2:
            A[i, i+2:] = 0.0

        A[i, :] = A[i,:]/np.sum(A[i,:])

    A[-1, :] = 0.0
    A[-1, -1] = 1.0

    return PI, A, B</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="blechpy.analysis.poissonHMM.ConstrainedHMM"><code class="flex name class">
<span>class <span class="ident">ConstrainedHMM</span></span>
<span>(</span><span>n_tastes, n_baseline=3, hmm_id=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ConstrainedHMM(PoissonHMM):
    def __init__(self, n_tastes, n_baseline=3, hmm_id=None):
        self.stat_arrays = {} # dict of cumulative stats to keep while fitting
                              # iterations, max_log_likelihood, fit log
                              # likelihood, cost, best_sequences, gamma
                              # probabilities, time, row_id
        self.n_tastes = n_tastes
        self.n_baseline = n_baseline
        n_states = n_baseline + 2*n_tastes
        super().__init__(n_states, hmm_id=hmm_id)

    def randomize(self, spikes, dt, time, row_id=None, constraint_func=None):
        # setup parameters 
        # make transition matrix
        # all baseline states have equal probability of staying or changing
        # into each other and the early states
        # each early state has high stay probability and low chance to transition into 
        n_trials, n_cells, n_steps = spikes.shape
        n_tastes = self.n_tastes
        n_baseline = self.n_baseline
        n_states = n_baseline + n_tastes*2

        # Transition Matrix: state X state, A[i,j] is prob to go from state i to state j
        unit = 1/(n_baseline + n_tastes)
        A0 = np.random.normal(unit, 0.01, (n_baseline, n_baseline)).astype(&#39;float64&#39;)
        A1 = np.vstack([[unit, 0]*n_tastes]*n_baseline).astype(&#39;float64&#39;)
        A2 = np.zeros((n_tastes*2, n_baseline)).astype(&#39;float64&#39;)
        A3 = np.zeros((n_tastes*2, n_tastes*2)).astype(&#39;float64&#39;)
        for i in range(n_tastes):
            j = 2*i
            A3[j, j] = np.min((0.999, np.random.normal(0.98, 0.01, 1)))
            A3[j, j+1] = 1-A3[j,j]
            A3[j+1, j] = 0
            A3[j+1, j+1] = 1

        A = np.hstack((np.vstack((A0, A2)), np.vstack((A1, A3))))

        # Rate Matrix: cells X states, Bij is firing rate of cell i in state j
        b_idx = np.where(time &lt; 0)[0]
        e_idx = np.where((time &gt;= 0) &amp; (time &lt; np.max(time)/2))[0]
        l_idx = np.where(time &gt;= np.max(time)/2)[0]
        if len(b_idx) == 0:
            b_idx = np.arange(n_steps)

        baseline = np.mean(np.sum(spikes[:, :, b_idx], axis=2), axis=0) / (len(b_idx)*dt)
        b_sd = np.std(np.sum(spikes[:, :, b_idx], axis=2), axis=0) / (len(b_idx)*dt)
        early = np.mean(np.sum(spikes[:, :, e_idx], axis=2), axis=0) / (len(e_idx)*dt)
        e_sd = np.std(np.sum(spikes[:, :, e_idx], axis=2), axis=0) / (len(e_idx)*dt)
        late = np.mean(np.sum(spikes[:, :, l_idx], axis=2), axis=0) / (len(l_idx)*dt)
        l_sd = np.std(np.sum(spikes[:, :, l_idx], axis=2), axis=0) / (len(l_idx)*dt)

        rates = np.zeros((n_cells, n_states))
        minFR = 1/n_steps
        for i in range(n_cells):
            row = [np.random.normal(baseline[i], b_sd[i], n_baseline)]
            for j in range(n_tastes):
                row.append(np.random.normal(early[i], e_sd[i], 1))
                row.append(np.random.normal(late[i], l_sd[i], 1))

            row = np.hstack(row)
            rates[i, :] = np.array([np.max((x, minFR)) for x in row])

        # Initial probabilities
        # Equal prob of all baseline states
        unit = 1/n_baseline
        PI = np.hstack([[np.random.normal(unit, 0.02, 1)[0]
                         for x in range(n_baseline)],
                        np.zeros((n_tastes*2,))])
        PI = PI/np.sum(PI)

        self.transition = A
        self.emission = rates
        self.initial_distribution = PI
        self.fitted = False
        self.converged = False
        self.iteration = 0
        self.stat_arrays[&#39;row_id&#39;] = row_id
        self._init_history()
        self.stat_arrays[&#39;gamma_probabilities&#39;] = self.get_gamma_probabilities(spikes, dt)
        self.stat_arrays[&#39;time&#39;] = time
        self._update_cost(spikes, dt)
        self.fit_LL = self.max_log_prob
        self._update_history()
        self._update_history()

    def fit(self, spikes, dt, time, max_iter = 500, threshold=1e-5, parallel=False):
        &#39;&#39;&#39;using parallels for processing trials actually seems to slow down
        processing (with 15 trials). Might still be useful if there is a very
        large nubmer of trials
        &#39;&#39;&#39;
        spikes = spikes.astype(&#39;int32&#39;)
        if (self.initial_distribution is None or
            self.transition is None or
            self.emission is None):
            raise ValueError(&#39;Must first initialize fit matrices either manually or via randomize&#39;)

        converged = False
        last_logl = None
        self.stat_arrays[&#39;time&#39;] = time
        while (not converged and (self.iteration &lt; max_iter)):
            self.fit_LL = self._step(spikes, dt, parallel=parallel)
            self._update_history()
            # if self.iteration &gt;= 100:
            #     trend = check_ll_trend(self, threshold)
            #     if trend == &#39;decreasing&#39;:
            #         return False
            #     elif trend == &#39;plateau&#39;:
            #         converged = True

            if last_logl is None:
                delta_ll = np.abs(self.fit_LL)
            else:
                delta_ll = np.abs((last_logl - self.fit_LL)/self.fit_LL)

            if (last_logl is not None and
                np.isfinite(delta_ll) and
                delta_ll &lt; threshold and
                np.isfinite(self.fit_LL) and
                self.iteration&gt;2):
                converged = True
                print(&#39;%s: %s: Change in log likelihood converged&#39; % (os.getpid(), self.hmm_id))

            last_logl = self.fit_LL

            # Convergence check is replaced by checking LL trend for plateau
            # converged = self.isConverged(convergence_thresh)
            print(&#39;%s: %s: Iter #%i complete. Log-likelihood is %.2E. Delta is %.2E&#39;
                  % (os.getpid(), self.hmm_id, self.iteration, self.fit_LL, delta_ll))

        self.fitted = True
        self.converged = converged
        return True

    def get_baseline_states(self):
        return np.arange(self.n_baseline)

    def get_early_states(self):
        return np.arange(self.n_baseline, self.n_states, 2)

    def get_late_states(self):
        return np.arange(self.n_baseline+1, self.n_states, 2)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="blechpy.analysis.poissonHMM.PoissonHMM" href="#blechpy.analysis.poissonHMM.PoissonHMM">PoissonHMM</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="blechpy.analysis.poissonHMM.ConstrainedHMM.get_baseline_states"><code class="name flex">
<span>def <span class="ident">get_baseline_states</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_baseline_states(self):
    return np.arange(self.n_baseline)</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.poissonHMM.ConstrainedHMM.get_early_states"><code class="name flex">
<span>def <span class="ident">get_early_states</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_early_states(self):
    return np.arange(self.n_baseline, self.n_states, 2)</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.poissonHMM.ConstrainedHMM.get_late_states"><code class="name flex">
<span>def <span class="ident">get_late_states</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_late_states(self):
    return np.arange(self.n_baseline+1, self.n_states, 2)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="blechpy.analysis.poissonHMM.PoissonHMM" href="#blechpy.analysis.poissonHMM.PoissonHMM">PoissonHMM</a></b></code>:
<ul class="hlist">
<li><code><a title="blechpy.analysis.poissonHMM.PoissonHMM.fit" href="#blechpy.analysis.poissonHMM.PoissonHMM.fit">fit</a></code></li>
<li><code><a title="blechpy.analysis.poissonHMM.PoissonHMM.randomize" href="#blechpy.analysis.poissonHMM.PoissonHMM.randomize">randomize</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="blechpy.analysis.poissonHMM.HmmHandler"><code class="flex name class">
<span>class <span class="ident">HmmHandler</span></span>
<span>(</span><span>dat, save_dir=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Takes a blechpy dataset object and fits HMMs for each tastant</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dat</code></strong> :&ensp;<code>blechpy.dataset</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>params</code></strong> :&ensp;<code>dict</code> or <code>list</code> of <code>dicts</code></dt>
<dd>each dict must have fields:
time_window: list of int, time window to cut around stimuli in ms
convergence_thresh: float
max_iter: int
n_repeats: int
unit_type: str, {'single', 'pyramidal', 'interneuron', 'all'}
bin_size: time bin for spike array when fitting in seconds
n_states: predicted number of states to fit</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class HmmHandler(object):
    def __init__(self, dat, save_dir=None):
        &#39;&#39;&#39;Takes a blechpy dataset object and fits HMMs for each tastant

        Parameters
        ----------
        dat: blechpy.dataset
        params: dict or list of dicts
            each dict must have fields:
                time_window: list of int, time window to cut around stimuli in ms
                convergence_thresh: float
                max_iter: int
                n_repeats: int
                unit_type: str, {&#39;single&#39;, &#39;pyramidal&#39;, &#39;interneuron&#39;, &#39;all&#39;}
                bin_size: time bin for spike array when fitting in seconds
                n_states: predicted number of states to fit
        &#39;&#39;&#39;
        if isinstance(dat, str):
            fd = dat
            dat = load_dataset(dat)
            if os.path.realpath(fd) != os.path.realpath(dat.root_dir):
                print(&#39;Changing dataset root_dir to match local directory&#39;)
                dat._change_root(fd)

            if dat is None:
                raise FileNotFoundError(&#39;No dataset.p file found given directory&#39;)

        if save_dir is None:
            save_dir = os.path.join(dat.root_dir,
                                    &#39;%s_analysis&#39; % dat.data_name)

        self._dataset = dat
        self.root_dir = dat.root_dir
        self.save_dir = save_dir
        self.h5_file = os.path.join(save_dir, &#39;%s_HMM_Analysis.hdf5&#39; % dat.data_name)
        self.load_params()

        if not os.path.isdir(save_dir):
            os.makedirs(save_dir)

        self.plot_dir = os.path.join(save_dir, &#39;HMM_Plots&#39;)
        if not os.path.isdir(self.plot_dir):
            os.makedirs(self.plot_dir)

        hmmIO.setup_hmm_hdf5(self.h5_file)
        # this function can be edited to account for parameters added in the
        # future
        # hmmIO.fix_hmm_overview(self.h5_file)

    def load_params(self):
        self._data_params = []
        self._fit_params = []
        h5_file = self.h5_file
        if not os.path.isfile(h5_file):
            return

        overview = self.get_data_overview()
        if overview.empty:
            return

        for i in overview.hmm_id:
            _, _, _, _, p = hmmIO.read_hmm_from_hdf5(h5_file, i)
            for k in list(p.keys()):
                if k not in HMM_PARAMS.keys():
                    _ = p.pop(k)

            self.add_params(p)

    def get_parameter_overview(self):
        df = pd.DataFrame(self._data_params)
        return df

    def get_data_overview(self):
        return hmmIO.get_hmm_overview_from_hdf5(self.h5_file)

    def run(self, parallel=True, overwrite=False, constraint_func=None):
        h5_file = self.h5_file
        rec_dir = self.root_dir
        if overwrite:
            fit_params = self._fit_params
        else:
            fit_params = [x for x in self._fit_params if not x[&#39;fitted&#39;]]

        if len(fit_params) == 0:
            return

        print(&#39;Running fittings&#39;)
        if parallel:
            n_cpu = np.min((cpu_count()-1, len(fit_params)))
        else:
            n_cpu = 1

        results = Parallel(n_jobs=n_cpu, verbose=100)(delayed(fit_hmm_mp)
                                                     (rec_dir, p, h5_file,
                                                      constraint_func)
                                                     for p in fit_params)


        memory.clear(warn=False)
        print(&#39;=&#39;*80)
        print(&#39;Fitting Complete&#39;)
        print(&#39;=&#39;*80)
        print(&#39;HMMs written to hdf5:&#39;)
        for hmm_id, written in results:
            print(&#39;%s : %s&#39; % (hmm_id, written))

        #self.plot_saved_models()
        self.load_params()

    def plot_saved_models(self):
        print(&#39;Plotting saved models&#39;)
        data = self.get_data_overview().set_index(&#39;hmm_id&#39;)
        rec_dir = self.root_dir
        for i, row in data.iterrows():
            hmm, _, params = load_hmm_from_hdf5(self.h5_file, i)
            spikes, dt, time = get_hmm_spike_data(rec_dir, params[&#39;unit_type&#39;],
                                                  params[&#39;channel&#39;],
                                                  time_start=params[&#39;time_start&#39;],
                                                  time_end=params[&#39;time_end&#39;],
                                                  dt=params[&#39;dt&#39;],
                                                  trials=params[&#39;n_trials&#39;],
                                                  area=params[&#39;area&#39;])
            plot_dir = os.path.join(self.plot_dir, &#39;hmm_%s&#39; % i)
            if not os.path.isdir(plot_dir):
                os.makedirs(plot_dir)

            print(&#39;Plotting HMM %s...&#39; % i)
            hmmplt.plot_hmm_figures(hmm, spikes, dt, time, save_dir=plot_dir)

    def add_params(self, params):
        if isinstance(params, list):
            for p in params:
                self.add_params(p)

            return
        elif not isinstance(params, dict):
            raise ValueError(&#39;Input must  be a dict or list of dicts&#39;)

        # Fill in blanks with defaults
        for k, v in HMM_PARAMS.items():
            if k not in params.keys():
                params[k] = v
                print(&#39;Parameter %s not provided. Using default value: %s&#39;
                      % (k, repr(v)))

        # Grab existing parameters
        data_params = self._data_params
        fit_params = self._fit_params

        # Get taste and trial info from dataset
        dat = self._dataset
        dim = dat.dig_in_mapping.query(&#39;exclude == False and spike_array == True&#39;)
        if params[&#39;taste&#39;] is None:
            tastes = dim[&#39;name&#39;].tolist()
            single_taste = True
        elif isinstance(params[&#39;taste&#39;], list):
            tastes = [t for t in params[&#39;taste&#39;] if any(dim[&#39;name&#39;] == t)]
            single_taste = False
        elif params[&#39;taste&#39;] == &#39;all&#39;:
            tastes = dim[&#39;name&#39;].tolist()
            single_taste = False
        else:
            tastes = [params[&#39;taste&#39;]]
            single_taste = True

        dim = dim.set_index(&#39;name&#39;)
        if not hasattr(dat, &#39;dig_in_trials&#39;):
            dat.create_trial_list()

        trials = dat.dig_in_trials
        hmm_ids = [x[&#39;hmm_id&#39;] for x in data_params]
        if single_taste:
            for t in tastes:
                p = params.copy()

                p[&#39;taste&#39;] = t
                # Skip if parameter is already in parameter set
                if any([hmmIO.compare_hmm_params(p, dp) for dp in data_params]):
                    print(&#39;Parameter set already in data_params, &#39;
                          &#39;to re-fit run with overwrite=True&#39;)
                    continue

                if t not in dim.index:
                    print(&#39;Taste %s not found in dig_in_mapping or marked to exclude. Skipping...&#39; % t)
                    continue

                if p[&#39;hmm_id&#39;] is None:
                    hid = get_new_id(hmm_ids)
                    p[&#39;hmm_id&#39;] = hid
                    hmm_ids.append(hid)

                p[&#39;channel&#39;] = dim.loc[t, &#39;channel&#39;]
                unit_names = query_units(dat, p[&#39;unit_type&#39;], area=p[&#39;area&#39;])
                p[&#39;n_cells&#39;] = len(unit_names)
                if p[&#39;n_trials&#39;] is None:
                    p[&#39;n_trials&#39;] = len(trials.query(&#39;name == @t&#39;))

                data_params.append(p)
                for i in range(p[&#39;n_repeats&#39;]):
                    fit_params.append(p.copy())

        else:
            if any([hmmIO.compare_hmm_params(p, dp) for dp in data_params]):
                print(&#39;Parameter set already in data_params, &#39;
                      &#39;to re-fit run with overwrite=True&#39;)
                return

            channels = [dim.loc[x,&#39;channel&#39;] for x in tastes]
            params[&#39;taste&#39;] = tastes
            params[&#39;channel&#39;] = channels

            # this is basically meaningless right now, since this if clause
            # should only be used with ConstrainedHMM which will fit 5
            # baseline states and 2 states per taste
            params[&#39;n_states&#39;] = params[&#39;n_states&#39;]*len(tastes)

            if params[&#39;hmm_id&#39;] is None:
                hid = get_new_id(hmm_ids)
                params[&#39;hmm_id&#39;] = hid
                hmm_ids.append(hid)

            unit_names = query_units(dat, params[&#39;unit_type&#39;],
                                     area=params[&#39;area&#39;])
            params[&#39;n_cells&#39;] = len(unit_names)
            if params[&#39;n_trials&#39;] is None:
                params[&#39;n_trials&#39;] = len(trials.query(&#39;name == @t&#39;))

            data_params.append(params)
            for i in range(params[&#39;n_repeats&#39;]):
                fit_params.append(params.copy())

        self._data_params = data_params
        self._fit_params = fit_params

    def get_hmm(self, hmm_id):
        return load_hmm_from_hdf5(self.h5_file, hmm_id)

    def delete_hmm(self, **kwargs):
        &#39;&#39;&#39;Deletes any HMMs whose parameters match the kwargs. i.e. n_states=2,
        taste=&#34;Saccharin&#34; would delete all 2-state HMMs for Saccharin trials
        also reload parameters from hdf5, so any added but un-fit params will
        be lost
        &#39;&#39;&#39;
        hmmIO.delete_hmm_from_hdf5(self.h5_file, **kwargs)
        self.load_params()</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="blechpy.analysis.poissonHMM.HmmHandler.add_params"><code class="name flex">
<span>def <span class="ident">add_params</span></span>(<span>self, params)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_params(self, params):
    if isinstance(params, list):
        for p in params:
            self.add_params(p)

        return
    elif not isinstance(params, dict):
        raise ValueError(&#39;Input must  be a dict or list of dicts&#39;)

    # Fill in blanks with defaults
    for k, v in HMM_PARAMS.items():
        if k not in params.keys():
            params[k] = v
            print(&#39;Parameter %s not provided. Using default value: %s&#39;
                  % (k, repr(v)))

    # Grab existing parameters
    data_params = self._data_params
    fit_params = self._fit_params

    # Get taste and trial info from dataset
    dat = self._dataset
    dim = dat.dig_in_mapping.query(&#39;exclude == False and spike_array == True&#39;)
    if params[&#39;taste&#39;] is None:
        tastes = dim[&#39;name&#39;].tolist()
        single_taste = True
    elif isinstance(params[&#39;taste&#39;], list):
        tastes = [t for t in params[&#39;taste&#39;] if any(dim[&#39;name&#39;] == t)]
        single_taste = False
    elif params[&#39;taste&#39;] == &#39;all&#39;:
        tastes = dim[&#39;name&#39;].tolist()
        single_taste = False
    else:
        tastes = [params[&#39;taste&#39;]]
        single_taste = True

    dim = dim.set_index(&#39;name&#39;)
    if not hasattr(dat, &#39;dig_in_trials&#39;):
        dat.create_trial_list()

    trials = dat.dig_in_trials
    hmm_ids = [x[&#39;hmm_id&#39;] for x in data_params]
    if single_taste:
        for t in tastes:
            p = params.copy()

            p[&#39;taste&#39;] = t
            # Skip if parameter is already in parameter set
            if any([hmmIO.compare_hmm_params(p, dp) for dp in data_params]):
                print(&#39;Parameter set already in data_params, &#39;
                      &#39;to re-fit run with overwrite=True&#39;)
                continue

            if t not in dim.index:
                print(&#39;Taste %s not found in dig_in_mapping or marked to exclude. Skipping...&#39; % t)
                continue

            if p[&#39;hmm_id&#39;] is None:
                hid = get_new_id(hmm_ids)
                p[&#39;hmm_id&#39;] = hid
                hmm_ids.append(hid)

            p[&#39;channel&#39;] = dim.loc[t, &#39;channel&#39;]
            unit_names = query_units(dat, p[&#39;unit_type&#39;], area=p[&#39;area&#39;])
            p[&#39;n_cells&#39;] = len(unit_names)
            if p[&#39;n_trials&#39;] is None:
                p[&#39;n_trials&#39;] = len(trials.query(&#39;name == @t&#39;))

            data_params.append(p)
            for i in range(p[&#39;n_repeats&#39;]):
                fit_params.append(p.copy())

    else:
        if any([hmmIO.compare_hmm_params(p, dp) for dp in data_params]):
            print(&#39;Parameter set already in data_params, &#39;
                  &#39;to re-fit run with overwrite=True&#39;)
            return

        channels = [dim.loc[x,&#39;channel&#39;] for x in tastes]
        params[&#39;taste&#39;] = tastes
        params[&#39;channel&#39;] = channels

        # this is basically meaningless right now, since this if clause
        # should only be used with ConstrainedHMM which will fit 5
        # baseline states and 2 states per taste
        params[&#39;n_states&#39;] = params[&#39;n_states&#39;]*len(tastes)

        if params[&#39;hmm_id&#39;] is None:
            hid = get_new_id(hmm_ids)
            params[&#39;hmm_id&#39;] = hid
            hmm_ids.append(hid)

        unit_names = query_units(dat, params[&#39;unit_type&#39;],
                                 area=params[&#39;area&#39;])
        params[&#39;n_cells&#39;] = len(unit_names)
        if params[&#39;n_trials&#39;] is None:
            params[&#39;n_trials&#39;] = len(trials.query(&#39;name == @t&#39;))

        data_params.append(params)
        for i in range(params[&#39;n_repeats&#39;]):
            fit_params.append(params.copy())

    self._data_params = data_params
    self._fit_params = fit_params</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.poissonHMM.HmmHandler.delete_hmm"><code class="name flex">
<span>def <span class="ident">delete_hmm</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Deletes any HMMs whose parameters match the kwargs. i.e. n_states=2,
taste="Saccharin" would delete all 2-state HMMs for Saccharin trials
also reload parameters from hdf5, so any added but un-fit params will
be lost</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def delete_hmm(self, **kwargs):
    &#39;&#39;&#39;Deletes any HMMs whose parameters match the kwargs. i.e. n_states=2,
    taste=&#34;Saccharin&#34; would delete all 2-state HMMs for Saccharin trials
    also reload parameters from hdf5, so any added but un-fit params will
    be lost
    &#39;&#39;&#39;
    hmmIO.delete_hmm_from_hdf5(self.h5_file, **kwargs)
    self.load_params()</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.poissonHMM.HmmHandler.get_data_overview"><code class="name flex">
<span>def <span class="ident">get_data_overview</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_data_overview(self):
    return hmmIO.get_hmm_overview_from_hdf5(self.h5_file)</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.poissonHMM.HmmHandler.get_hmm"><code class="name flex">
<span>def <span class="ident">get_hmm</span></span>(<span>self, hmm_id)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_hmm(self, hmm_id):
    return load_hmm_from_hdf5(self.h5_file, hmm_id)</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.poissonHMM.HmmHandler.get_parameter_overview"><code class="name flex">
<span>def <span class="ident">get_parameter_overview</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_parameter_overview(self):
    df = pd.DataFrame(self._data_params)
    return df</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.poissonHMM.HmmHandler.load_params"><code class="name flex">
<span>def <span class="ident">load_params</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_params(self):
    self._data_params = []
    self._fit_params = []
    h5_file = self.h5_file
    if not os.path.isfile(h5_file):
        return

    overview = self.get_data_overview()
    if overview.empty:
        return

    for i in overview.hmm_id:
        _, _, _, _, p = hmmIO.read_hmm_from_hdf5(h5_file, i)
        for k in list(p.keys()):
            if k not in HMM_PARAMS.keys():
                _ = p.pop(k)

        self.add_params(p)</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.poissonHMM.HmmHandler.plot_saved_models"><code class="name flex">
<span>def <span class="ident">plot_saved_models</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_saved_models(self):
    print(&#39;Plotting saved models&#39;)
    data = self.get_data_overview().set_index(&#39;hmm_id&#39;)
    rec_dir = self.root_dir
    for i, row in data.iterrows():
        hmm, _, params = load_hmm_from_hdf5(self.h5_file, i)
        spikes, dt, time = get_hmm_spike_data(rec_dir, params[&#39;unit_type&#39;],
                                              params[&#39;channel&#39;],
                                              time_start=params[&#39;time_start&#39;],
                                              time_end=params[&#39;time_end&#39;],
                                              dt=params[&#39;dt&#39;],
                                              trials=params[&#39;n_trials&#39;],
                                              area=params[&#39;area&#39;])
        plot_dir = os.path.join(self.plot_dir, &#39;hmm_%s&#39; % i)
        if not os.path.isdir(plot_dir):
            os.makedirs(plot_dir)

        print(&#39;Plotting HMM %s...&#39; % i)
        hmmplt.plot_hmm_figures(hmm, spikes, dt, time, save_dir=plot_dir)</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.poissonHMM.HmmHandler.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>self, parallel=True, overwrite=False, constraint_func=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run(self, parallel=True, overwrite=False, constraint_func=None):
    h5_file = self.h5_file
    rec_dir = self.root_dir
    if overwrite:
        fit_params = self._fit_params
    else:
        fit_params = [x for x in self._fit_params if not x[&#39;fitted&#39;]]

    if len(fit_params) == 0:
        return

    print(&#39;Running fittings&#39;)
    if parallel:
        n_cpu = np.min((cpu_count()-1, len(fit_params)))
    else:
        n_cpu = 1

    results = Parallel(n_jobs=n_cpu, verbose=100)(delayed(fit_hmm_mp)
                                                 (rec_dir, p, h5_file,
                                                  constraint_func)
                                                 for p in fit_params)


    memory.clear(warn=False)
    print(&#39;=&#39;*80)
    print(&#39;Fitting Complete&#39;)
    print(&#39;=&#39;*80)
    print(&#39;HMMs written to hdf5:&#39;)
    for hmm_id, written in results:
        print(&#39;%s : %s&#39; % (hmm_id, written))

    #self.plot_saved_models()
    self.load_params()</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="blechpy.analysis.poissonHMM.PoissonHMM"><code class="flex name class">
<span>class <span class="ident">PoissonHMM</span></span>
<span>(</span><span>n_states, hmm_id=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PoissonHMM(object):
    def __init__(self, n_states, hmm_id=None):
        self.stat_arrays = {} # dict of cumulative stats to keep while fitting
                              # iterations, max_log_likelihood, fit log
                              # likelihood, cost, best_sequences, gamma
                              # probabilities, time, row_id
        self.n_states = n_states
        self.hmm_id = hmm_id

        self.transition = None
        self.emission = None
        self.initial_distribution = None

        self.fitted = False
        self.converged = False

        self.cost = None
        self.BIC = None
        self.max_log_prob = None
        self.fit_LL = None

    def randomize(self, spikes, dt, time, row_id=None, constraint_func=None):
        &#39;&#39;&#39;Initialize and randomize HMM matrices: initial_distribution (PI),
        transition (A) and emission/rates (B)
        Parameters
        ----------
        spikes : np.ndarray, dtype=int
            matrix of spike counts with dimensions trials x cells x time with binsize dt
        dt : float
            time step of spikes matrix in seconds
        time : np.ndarray
            1-D time vector corresponding to final dimension of spikes matrix,
            in milliseconds
        row_id : np.ndarray
            array to uniquely identify each row of the spikes array. This will
            thus identify each row of the best_sequences and gamma_probability
            matrices that are computed and stored
            useful when fitting a single HMM to trials with differing stimuli
        constrain_func : function
            user can provide a function that is used after randomization to
            constrain the PI, A and B matrices. The function must take PI, A, B
            as arguments and return PI, A, B.
        &#39;&#39;&#39;
        # setup parameters 
        # make transition matrix
        # all baseline states have equal probability of staying or changing
        # into each other and the early states
        # each early state has high stay probability and low chance to transition into 
        np.random.seed(None)
        n_trials, n_cells, n_steps = spikes.shape
        n_states = self.n_states

        # Initialize transition matrix with high stay probability
        # A is prob from going from state row to state column
        print(&#39;%s: Randomizing&#39; % os.getpid())
        # Design transition matrix with large diagnonal and small everything else
        diag = np.abs(np.random.normal(.99, .01, n_states))
        A = np.abs(np.random.normal(0.01/(n_states-1), 0.01, (n_states, n_states)))
        for i in range(n_states):
            A[i, i] = diag[i]
            A[i,:] = A[i,:] / np.sum(A[i,:]) # normalize row to sum to 1

        # Initialize rate matrix (&#34;Emission&#34; matrix)
        spike_counts = np.sum(spikes, axis=2) / (len(time)*dt)
        mean_rates = np.mean(spike_counts, axis=0)
        std_rates = np.std(spike_counts, axis=0)
        B = np.vstack([np.abs(np.random.normal(x, y, n_states))
                       for x,y in zip(mean_rates, std_rates)])
        PI = np.ones((n_states,)) / n_states

        # RN10 preCTA fit better without constraining initial firing rate
        # mr = np.mean(np.sum(spikes[:, :, :int(500/dt)], axis=2), axis=0)
        # sr = np.std(np.sum(spikes[:, :, :int(500/dt)], axis=2), axis=0)
        # B[:, 0] = [np.abs(np.random.normal(x, y, 1))[0] for x,y in zip(mr, sr)]

        if constraint_func is not None:
            PI, A, B = constraint_func(PI, A, B)

        self.transition = A
        self.emission = B
        self.initial_distribution = PI
        self.fitted = False
        self.converged = False
        self.iteration = 0
        self.stat_arrays[&#39;row_id&#39;] = row_id
        self._init_history()
        self.stat_arrays[&#39;gamma_probabilities&#39;] = self.get_gamma_probabilities(spikes, dt)
        self.stat_arrays[&#39;time&#39;] = time
        self._update_cost(spikes, dt)
        self.fit_LL = self.max_log_prob
        self._update_history()

    def _init_history(self):
        self.stat_arrays[&#39;cost&#39;] = []
        self.stat_arrays[&#39;BIC&#39;] = []
        self.stat_arrays[&#39;max_log_prob&#39;] = []
        self.stat_arrays[&#39;fit_LL&#39;] = []
        self.stat_arrays[&#39;iterations&#39;] = []
        self.history = {&#39;A&#39;: [], &#39;B&#39;: [], &#39;PI&#39;: [], &#39;iterations&#39;:[]}

    def _update_history(self):
        itr = self.iteration
        self.history[&#39;A&#39;].append(self.transition)
        self.history[&#39;B&#39;].append(self.emission)
        self.history[&#39;PI&#39;].append(self.initial_distribution)
        self.history[&#39;iterations&#39;].append(itr)

        self.stat_arrays[&#39;cost&#39;].append(self.cost)
        self.stat_arrays[&#39;BIC&#39;].append(self.BIC)
        self.stat_arrays[&#39;max_log_prob&#39;].append(self.max_log_prob)
        self.stat_arrays[&#39;fit_LL&#39;].append(self.fit_LL)
        self.stat_arrays[&#39;iterations&#39;].append(itr)

    def fit(self, spikes, dt, time, max_iter = 500, threshold=1e-5, parallel=False):
        &#39;&#39;&#39;using parallels for processing trials actually seems to slow down
        processing (with 15 trials). Might still be useful if there is a very
        large nubmer of trials
        &#39;&#39;&#39;
        spikes = spikes.astype(&#39;int32&#39;)
        if (self.initial_distribution is None or
            self.transition is None or
            self.emission is None):
            raise ValueError(&#39;Must first initialize fit matrices either manually or via randomize&#39;)

        converged = False
        last_logl = None
        self.stat_arrays[&#39;time&#39;] = time
        while (not converged and (self.iteration &lt; max_iter)):
            self.fit_LL = self._step(spikes, dt, parallel=parallel)
            self._update_history()
            # if self.iteration &gt;= 100:
            #     trend = check_ll_trend(self, threshold)
            #     if trend == &#39;decreasing&#39;:
            #         return False
            #     elif trend == &#39;plateau&#39;:
            #         converged = True

            if last_logl is None:
                delta_ll = np.abs(self.fit_LL)
            else:
                delta_ll = np.abs((last_logl - self.fit_LL)/self.fit_LL)

            if (last_logl is not None and
                np.isfinite(delta_ll) and
                delta_ll &lt; threshold and
                np.isfinite(self.fit_LL) and
                self.iteration&gt;2):
                # This log likelihood measure doesn&#39;t look right, the change
                # seems to always be 0
                # 8/24/20: Fixed, this is now a good measure
                converged = True
                print(&#39;%s: %s: Change in log likelihood converged&#39; % (os.getpid(), self.hmm_id))

            last_logl = self.fit_LL

            # Convergence check is replaced by checking LL trend for plateau
            # converged = self.isConverged(convergence_thresh)
            print(&#39;%s: %s: Iter #%i complete. Log-likelihood is %.2E. Delta is %.2E&#39;
                  % (os.getpid(), self.hmm_id, self.iteration, self.fit_LL, delta_ll))

        self.fitted = True
        self.converged = converged
        return True

    def _step(self, spikes, dt, parallel=False):
        if len(spikes.shape) == 2:
            spikes = np.expand_dims(spikes, 0)

        nTrials, nCells, nTimeSteps = spikes.shape

        A = self.transition
        B = self.emission
        PI = self.initial_distribution
        nStates = self.n_states

        # For multiple trials need to cmpute gamma and epsilon for every trial
        # and then update
        if parallel:
            n_cores = cpu_count() - 1
        else:
            n_cores = 1

        results = Parallel(n_jobs=n_cores)(delayed(baum_welch)(trial, dt, PI, A, B)
                                           for trial in spikes)
        gammas, epsilons, norms = zip(*results)
        gammas = np.array(gammas)
        epsilons = np.array(epsilons)
        norms = np.array(norms)
        #logl = np.sum(norms)
        logl = np.sum(np.log(norms))

        PI, A, B = compute_new_matrices(spikes, dt, gammas, epsilons)
        # Make sure rates are non-zeros for computations
        # B[np.where(B==0)] = 1e-300
        A[A &lt; 1e-50] = 0.0
        for i in range(self.n_states):
            A[i,:] = A[i,:] / np.sum(A[i,:])

        self.transition = A
        self.emission = B
        self.initial_distribution = PI
        self.stat_arrays[&#39;gamma_probabilities&#39;] = gammas
        self.iteration = self.iteration + 1
        self._update_cost(spikes, dt)
        return logl

    def get_best_paths(self, spikes, dt):
        if &#39;best_sequences&#39; is self.stat_arrays.keys():
            return self.stat_arrays[&#39;best_sequences&#39;], self.max_log_prob

        PI = self.initial_distribution
        A = self.transition
        B = self.emission

        bestPaths, pathProbs = compute_best_paths(spikes, dt, PI, A, B)
        return bestPaths, np.sum(pathProbs)

    def get_forward_probabilities(self, spikes, dt, parallel=False):
        PI = self.initial_distribution
        A = self.transition
        B = self.emission
        if parallel:
            n_cpu = cpu_count() -1
        else:
            n_cpu = 1

        a_results = Parallel(n_jobs=n_cpu)(delayed(forward)
                                           (trial, dt, PI, A, B)
                                           for trial in spikes)
        alphas, norms = zip(*a_results)
        return np.array(alphas), np.array(norms)

    def get_backward_probabilities(self, spikes, dt, parallel=False):
        PI = self.initial_distribution
        A = self.transition
        B = self.emission
        betas = []
        if parallel:
            n_cpu = cpu_count() -1
        else:
            n_cpu = 1

        a_results = Parallel(n_jobs=n_cpu)(delayed(forward)(trial, dt, PI, A, B)
                                         for trial in spikes)
        _, norms = zip(*a_results)
        b_results = Parallel(n_jobs=n_cpu)(delayed(backward)(trial, dt, A, B, n)
                                           for trial, n in zip(spikes, norms))
        betas = np.array(b_results)

        return betas

    def get_gamma_probabilities(self, spikes, dt, parallel=False):
        PI = self.initial_distribution
        A = self.transition
        B = self.emission
        if parallel:
            n_cpu = cpu_count()-1
        else:
            n_cpu = 1

        results = Parallel(n_jobs=n_cpu)(delayed(baum_welch)(trial, dt, PI, A, B)
                                         for trial in spikes)
        gammas, _, _ = zip(*results)
        return np.array(gammas)

    def _update_cost(self, spikes, dt):
        spikes = spikes.astype(&#39;int&#39;)
        PI = self.initial_distribution
        A  = self.transition
        B  = self.emission
        cost, BIC, bestPaths, maxLogProb = compute_hmm_cost(spikes, dt, PI, A, B)
        self.cost = cost
        self.BIC = BIC
        self.max_log_prob = maxLogProb
        self.stat_arrays[&#39;best_sequences&#39;] = bestPaths

    def roll_back(self, iteration, spikes=None, dt=None):
        itrs = self.history[&#39;iterations&#39;]
        idx = np.where(itrs == iteration)[0]
        if len(idx) == 0:
            raise ValueError(&#39;Iteration %i not found in history&#39; % iteration)

        idx = idx[0]
        self.emission = self.history[&#39;B&#39;][idx]
        self.transition = self.history[&#39;A&#39;][idx]
        self.initial_distribution = self.history[&#39;PI&#39;][idx]
        self.iteration = iteration

        itrs = self.stat_arrays[&#39;iterations&#39;]
        idx = np.where(itrs == iteration)[0][0]
        self.fit_LL = self.stat_arrays[&#39;fit_LL&#39;][idx]
        self.max_log_prob = self.stat_arrays[&#39;max_log_prob&#39;][idx]
        self.BIC = self.stat_arrays[&#39;BIC&#39;][idx]
        self.cost = self.stat_arrays[&#39;cost&#39;][idx]
        if spikes is not None and dt is not None:
            self.stat_arrays[&#39;gamma_probabilities&#39;] = self.get_gamma_probabilities(spikes, dt)
            self._update_cost(spikes, dt)

        self._update_history()</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="blechpy.analysis.poissonHMM.ConstrainedHMM" href="#blechpy.analysis.poissonHMM.ConstrainedHMM">ConstrainedHMM</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="blechpy.analysis.poissonHMM.PoissonHMM.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, spikes, dt, time, max_iter=500, threshold=1e-05, parallel=False)</span>
</code></dt>
<dd>
<div class="desc"><p>using parallels for processing trials actually seems to slow down
processing (with 15 trials). Might still be useful if there is a very
large nubmer of trials</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, spikes, dt, time, max_iter = 500, threshold=1e-5, parallel=False):
    &#39;&#39;&#39;using parallels for processing trials actually seems to slow down
    processing (with 15 trials). Might still be useful if there is a very
    large nubmer of trials
    &#39;&#39;&#39;
    spikes = spikes.astype(&#39;int32&#39;)
    if (self.initial_distribution is None or
        self.transition is None or
        self.emission is None):
        raise ValueError(&#39;Must first initialize fit matrices either manually or via randomize&#39;)

    converged = False
    last_logl = None
    self.stat_arrays[&#39;time&#39;] = time
    while (not converged and (self.iteration &lt; max_iter)):
        self.fit_LL = self._step(spikes, dt, parallel=parallel)
        self._update_history()
        # if self.iteration &gt;= 100:
        #     trend = check_ll_trend(self, threshold)
        #     if trend == &#39;decreasing&#39;:
        #         return False
        #     elif trend == &#39;plateau&#39;:
        #         converged = True

        if last_logl is None:
            delta_ll = np.abs(self.fit_LL)
        else:
            delta_ll = np.abs((last_logl - self.fit_LL)/self.fit_LL)

        if (last_logl is not None and
            np.isfinite(delta_ll) and
            delta_ll &lt; threshold and
            np.isfinite(self.fit_LL) and
            self.iteration&gt;2):
            # This log likelihood measure doesn&#39;t look right, the change
            # seems to always be 0
            # 8/24/20: Fixed, this is now a good measure
            converged = True
            print(&#39;%s: %s: Change in log likelihood converged&#39; % (os.getpid(), self.hmm_id))

        last_logl = self.fit_LL

        # Convergence check is replaced by checking LL trend for plateau
        # converged = self.isConverged(convergence_thresh)
        print(&#39;%s: %s: Iter #%i complete. Log-likelihood is %.2E. Delta is %.2E&#39;
              % (os.getpid(), self.hmm_id, self.iteration, self.fit_LL, delta_ll))

    self.fitted = True
    self.converged = converged
    return True</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.poissonHMM.PoissonHMM.get_backward_probabilities"><code class="name flex">
<span>def <span class="ident">get_backward_probabilities</span></span>(<span>self, spikes, dt, parallel=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_backward_probabilities(self, spikes, dt, parallel=False):
    PI = self.initial_distribution
    A = self.transition
    B = self.emission
    betas = []
    if parallel:
        n_cpu = cpu_count() -1
    else:
        n_cpu = 1

    a_results = Parallel(n_jobs=n_cpu)(delayed(forward)(trial, dt, PI, A, B)
                                     for trial in spikes)
    _, norms = zip(*a_results)
    b_results = Parallel(n_jobs=n_cpu)(delayed(backward)(trial, dt, A, B, n)
                                       for trial, n in zip(spikes, norms))
    betas = np.array(b_results)

    return betas</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.poissonHMM.PoissonHMM.get_best_paths"><code class="name flex">
<span>def <span class="ident">get_best_paths</span></span>(<span>self, spikes, dt)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_best_paths(self, spikes, dt):
    if &#39;best_sequences&#39; is self.stat_arrays.keys():
        return self.stat_arrays[&#39;best_sequences&#39;], self.max_log_prob

    PI = self.initial_distribution
    A = self.transition
    B = self.emission

    bestPaths, pathProbs = compute_best_paths(spikes, dt, PI, A, B)
    return bestPaths, np.sum(pathProbs)</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.poissonHMM.PoissonHMM.get_forward_probabilities"><code class="name flex">
<span>def <span class="ident">get_forward_probabilities</span></span>(<span>self, spikes, dt, parallel=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_forward_probabilities(self, spikes, dt, parallel=False):
    PI = self.initial_distribution
    A = self.transition
    B = self.emission
    if parallel:
        n_cpu = cpu_count() -1
    else:
        n_cpu = 1

    a_results = Parallel(n_jobs=n_cpu)(delayed(forward)
                                       (trial, dt, PI, A, B)
                                       for trial in spikes)
    alphas, norms = zip(*a_results)
    return np.array(alphas), np.array(norms)</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.poissonHMM.PoissonHMM.get_gamma_probabilities"><code class="name flex">
<span>def <span class="ident">get_gamma_probabilities</span></span>(<span>self, spikes, dt, parallel=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_gamma_probabilities(self, spikes, dt, parallel=False):
    PI = self.initial_distribution
    A = self.transition
    B = self.emission
    if parallel:
        n_cpu = cpu_count()-1
    else:
        n_cpu = 1

    results = Parallel(n_jobs=n_cpu)(delayed(baum_welch)(trial, dt, PI, A, B)
                                     for trial in spikes)
    gammas, _, _ = zip(*results)
    return np.array(gammas)</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.poissonHMM.PoissonHMM.randomize"><code class="name flex">
<span>def <span class="ident">randomize</span></span>(<span>self, spikes, dt, time, row_id=None, constraint_func=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize and randomize HMM matrices: initial_distribution (PI),
transition (A) and emission/rates (B)
Parameters</p>
<hr>
<dl>
<dt><strong><code>spikes</code></strong> :&ensp;<code>np.ndarray, dtype=int</code></dt>
<dd>matrix of spike counts with dimensions trials x cells x time with binsize dt</dd>
<dt><strong><code>dt</code></strong> :&ensp;<code>float</code></dt>
<dd>time step of spikes matrix in seconds</dd>
<dt><strong><code>time</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>1-D time vector corresponding to final dimension of spikes matrix,
in milliseconds</dd>
<dt><strong><code>row_id</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>array to uniquely identify each row of the spikes array. This will
thus identify each row of the best_sequences and gamma_probability
matrices that are computed and stored
useful when fitting a single HMM to trials with differing stimuli</dd>
<dt><strong><code>constrain_func</code></strong> :&ensp;<code>function</code></dt>
<dd>user can provide a function that is used after randomization to
constrain the PI, A and B matrices. The function must take PI, A, B
as arguments and return PI, A, B.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def randomize(self, spikes, dt, time, row_id=None, constraint_func=None):
    &#39;&#39;&#39;Initialize and randomize HMM matrices: initial_distribution (PI),
    transition (A) and emission/rates (B)
    Parameters
    ----------
    spikes : np.ndarray, dtype=int
        matrix of spike counts with dimensions trials x cells x time with binsize dt
    dt : float
        time step of spikes matrix in seconds
    time : np.ndarray
        1-D time vector corresponding to final dimension of spikes matrix,
        in milliseconds
    row_id : np.ndarray
        array to uniquely identify each row of the spikes array. This will
        thus identify each row of the best_sequences and gamma_probability
        matrices that are computed and stored
        useful when fitting a single HMM to trials with differing stimuli
    constrain_func : function
        user can provide a function that is used after randomization to
        constrain the PI, A and B matrices. The function must take PI, A, B
        as arguments and return PI, A, B.
    &#39;&#39;&#39;
    # setup parameters 
    # make transition matrix
    # all baseline states have equal probability of staying or changing
    # into each other and the early states
    # each early state has high stay probability and low chance to transition into 
    np.random.seed(None)
    n_trials, n_cells, n_steps = spikes.shape
    n_states = self.n_states

    # Initialize transition matrix with high stay probability
    # A is prob from going from state row to state column
    print(&#39;%s: Randomizing&#39; % os.getpid())
    # Design transition matrix with large diagnonal and small everything else
    diag = np.abs(np.random.normal(.99, .01, n_states))
    A = np.abs(np.random.normal(0.01/(n_states-1), 0.01, (n_states, n_states)))
    for i in range(n_states):
        A[i, i] = diag[i]
        A[i,:] = A[i,:] / np.sum(A[i,:]) # normalize row to sum to 1

    # Initialize rate matrix (&#34;Emission&#34; matrix)
    spike_counts = np.sum(spikes, axis=2) / (len(time)*dt)
    mean_rates = np.mean(spike_counts, axis=0)
    std_rates = np.std(spike_counts, axis=0)
    B = np.vstack([np.abs(np.random.normal(x, y, n_states))
                   for x,y in zip(mean_rates, std_rates)])
    PI = np.ones((n_states,)) / n_states

    # RN10 preCTA fit better without constraining initial firing rate
    # mr = np.mean(np.sum(spikes[:, :, :int(500/dt)], axis=2), axis=0)
    # sr = np.std(np.sum(spikes[:, :, :int(500/dt)], axis=2), axis=0)
    # B[:, 0] = [np.abs(np.random.normal(x, y, 1))[0] for x,y in zip(mr, sr)]

    if constraint_func is not None:
        PI, A, B = constraint_func(PI, A, B)

    self.transition = A
    self.emission = B
    self.initial_distribution = PI
    self.fitted = False
    self.converged = False
    self.iteration = 0
    self.stat_arrays[&#39;row_id&#39;] = row_id
    self._init_history()
    self.stat_arrays[&#39;gamma_probabilities&#39;] = self.get_gamma_probabilities(spikes, dt)
    self.stat_arrays[&#39;time&#39;] = time
    self._update_cost(spikes, dt)
    self.fit_LL = self.max_log_prob
    self._update_history()</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.poissonHMM.PoissonHMM.roll_back"><code class="name flex">
<span>def <span class="ident">roll_back</span></span>(<span>self, iteration, spikes=None, dt=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def roll_back(self, iteration, spikes=None, dt=None):
    itrs = self.history[&#39;iterations&#39;]
    idx = np.where(itrs == iteration)[0]
    if len(idx) == 0:
        raise ValueError(&#39;Iteration %i not found in history&#39; % iteration)

    idx = idx[0]
    self.emission = self.history[&#39;B&#39;][idx]
    self.transition = self.history[&#39;A&#39;][idx]
    self.initial_distribution = self.history[&#39;PI&#39;][idx]
    self.iteration = iteration

    itrs = self.stat_arrays[&#39;iterations&#39;]
    idx = np.where(itrs == iteration)[0][0]
    self.fit_LL = self.stat_arrays[&#39;fit_LL&#39;][idx]
    self.max_log_prob = self.stat_arrays[&#39;max_log_prob&#39;][idx]
    self.BIC = self.stat_arrays[&#39;BIC&#39;][idx]
    self.cost = self.stat_arrays[&#39;cost&#39;][idx]
    if spikes is not None and dt is not None:
        self.stat_arrays[&#39;gamma_probabilities&#39;] = self.get_gamma_probabilities(spikes, dt)
        self._update_cost(spikes, dt)

    self._update_history()</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="blechpy.analysis" href="index.html">blechpy.analysis</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="blechpy.analysis.poissonHMM.backward" href="#blechpy.analysis.poissonHMM.backward">backward</a></code></li>
<li><code><a title="blechpy.analysis.poissonHMM.baum_welch" href="#blechpy.analysis.poissonHMM.baum_welch">baum_welch</a></code></li>
<li><code><a title="blechpy.analysis.poissonHMM.check_ll_trend" href="#blechpy.analysis.poissonHMM.check_ll_trend">check_ll_trend</a></code></li>
<li><code><a title="blechpy.analysis.poissonHMM.compute_BIC" href="#blechpy.analysis.poissonHMM.compute_BIC">compute_BIC</a></code></li>
<li><code><a title="blechpy.analysis.poissonHMM.compute_baum_welch" href="#blechpy.analysis.poissonHMM.compute_baum_welch">compute_baum_welch</a></code></li>
<li><code><a title="blechpy.analysis.poissonHMM.compute_best_paths" href="#blechpy.analysis.poissonHMM.compute_best_paths">compute_best_paths</a></code></li>
<li><code><a title="blechpy.analysis.poissonHMM.compute_hmm_cost" href="#blechpy.analysis.poissonHMM.compute_hmm_cost">compute_hmm_cost</a></code></li>
<li><code><a title="blechpy.analysis.poissonHMM.compute_new_matrices" href="#blechpy.analysis.poissonHMM.compute_new_matrices">compute_new_matrices</a></code></li>
<li><code><a title="blechpy.analysis.poissonHMM.compute_rate_rmse" href="#blechpy.analysis.poissonHMM.compute_rate_rmse">compute_rate_rmse</a></code></li>
<li><code><a title="blechpy.analysis.poissonHMM.convert_path_state_numbers" href="#blechpy.analysis.poissonHMM.convert_path_state_numbers">convert_path_state_numbers</a></code></li>
<li><code><a title="blechpy.analysis.poissonHMM.convert_spikes_to_rates" href="#blechpy.analysis.poissonHMM.convert_spikes_to_rates">convert_spikes_to_rates</a></code></li>
<li><code><a title="blechpy.analysis.poissonHMM.fast_factorial" href="#blechpy.analysis.poissonHMM.fast_factorial">fast_factorial</a></code></li>
<li><code><a title="blechpy.analysis.poissonHMM.fit_hmm_mp" href="#blechpy.analysis.poissonHMM.fit_hmm_mp">fit_hmm_mp</a></code></li>
<li><code><a title="blechpy.analysis.poissonHMM.fix_arrays" href="#blechpy.analysis.poissonHMM.fix_arrays">fix_arrays</a></code></li>
<li><code><a title="blechpy.analysis.poissonHMM.forward" href="#blechpy.analysis.poissonHMM.forward">forward</a></code></li>
<li><code><a title="blechpy.analysis.poissonHMM.generate_rate_array_from_state_seq" href="#blechpy.analysis.poissonHMM.generate_rate_array_from_state_seq">generate_rate_array_from_state_seq</a></code></li>
<li><code><a title="blechpy.analysis.poissonHMM.get_hmm_spike_data" href="#blechpy.analysis.poissonHMM.get_hmm_spike_data">get_hmm_spike_data</a></code></li>
<li><code><a title="blechpy.analysis.poissonHMM.get_new_id" href="#blechpy.analysis.poissonHMM.get_new_id">get_new_id</a></code></li>
<li><code><a title="blechpy.analysis.poissonHMM.isConverged" href="#blechpy.analysis.poissonHMM.isConverged">isConverged</a></code></li>
<li><code><a title="blechpy.analysis.poissonHMM.load_hmm_from_hdf5" href="#blechpy.analysis.poissonHMM.load_hmm_from_hdf5">load_hmm_from_hdf5</a></code></li>
<li><code><a title="blechpy.analysis.poissonHMM.log_emission" href="#blechpy.analysis.poissonHMM.log_emission">log_emission</a></code></li>
<li><code><a title="blechpy.analysis.poissonHMM.match_states" href="#blechpy.analysis.poissonHMM.match_states">match_states</a></code></li>
<li><code><a title="blechpy.analysis.poissonHMM.poisson" href="#blechpy.analysis.poissonHMM.poisson">poisson</a></code></li>
<li><code><a title="blechpy.analysis.poissonHMM.poisson_viterbi" href="#blechpy.analysis.poissonHMM.poisson_viterbi">poisson_viterbi</a></code></li>
<li><code><a title="blechpy.analysis.poissonHMM.poisson_viterbi_deprecated" href="#blechpy.analysis.poissonHMM.poisson_viterbi_deprecated">poisson_viterbi_deprecated</a></code></li>
<li><code><a title="blechpy.analysis.poissonHMM.query_units" href="#blechpy.analysis.poissonHMM.query_units">query_units</a></code></li>
<li><code><a title="blechpy.analysis.poissonHMM.rebin_spike_array" href="#blechpy.analysis.poissonHMM.rebin_spike_array">rebin_spike_array</a></code></li>
<li><code><a title="blechpy.analysis.poissonHMM.roll_back_hmm_to_best" href="#blechpy.analysis.poissonHMM.roll_back_hmm_to_best">roll_back_hmm_to_best</a></code></li>
<li><code><a title="blechpy.analysis.poissonHMM.sequential_constraint" href="#blechpy.analysis.poissonHMM.sequential_constraint">sequential_constraint</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="blechpy.analysis.poissonHMM.ConstrainedHMM" href="#blechpy.analysis.poissonHMM.ConstrainedHMM">ConstrainedHMM</a></code></h4>
<ul class="">
<li><code><a title="blechpy.analysis.poissonHMM.ConstrainedHMM.get_baseline_states" href="#blechpy.analysis.poissonHMM.ConstrainedHMM.get_baseline_states">get_baseline_states</a></code></li>
<li><code><a title="blechpy.analysis.poissonHMM.ConstrainedHMM.get_early_states" href="#blechpy.analysis.poissonHMM.ConstrainedHMM.get_early_states">get_early_states</a></code></li>
<li><code><a title="blechpy.analysis.poissonHMM.ConstrainedHMM.get_late_states" href="#blechpy.analysis.poissonHMM.ConstrainedHMM.get_late_states">get_late_states</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="blechpy.analysis.poissonHMM.HmmHandler" href="#blechpy.analysis.poissonHMM.HmmHandler">HmmHandler</a></code></h4>
<ul class="">
<li><code><a title="blechpy.analysis.poissonHMM.HmmHandler.add_params" href="#blechpy.analysis.poissonHMM.HmmHandler.add_params">add_params</a></code></li>
<li><code><a title="blechpy.analysis.poissonHMM.HmmHandler.delete_hmm" href="#blechpy.analysis.poissonHMM.HmmHandler.delete_hmm">delete_hmm</a></code></li>
<li><code><a title="blechpy.analysis.poissonHMM.HmmHandler.get_data_overview" href="#blechpy.analysis.poissonHMM.HmmHandler.get_data_overview">get_data_overview</a></code></li>
<li><code><a title="blechpy.analysis.poissonHMM.HmmHandler.get_hmm" href="#blechpy.analysis.poissonHMM.HmmHandler.get_hmm">get_hmm</a></code></li>
<li><code><a title="blechpy.analysis.poissonHMM.HmmHandler.get_parameter_overview" href="#blechpy.analysis.poissonHMM.HmmHandler.get_parameter_overview">get_parameter_overview</a></code></li>
<li><code><a title="blechpy.analysis.poissonHMM.HmmHandler.load_params" href="#blechpy.analysis.poissonHMM.HmmHandler.load_params">load_params</a></code></li>
<li><code><a title="blechpy.analysis.poissonHMM.HmmHandler.plot_saved_models" href="#blechpy.analysis.poissonHMM.HmmHandler.plot_saved_models">plot_saved_models</a></code></li>
<li><code><a title="blechpy.analysis.poissonHMM.HmmHandler.run" href="#blechpy.analysis.poissonHMM.HmmHandler.run">run</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="blechpy.analysis.poissonHMM.PoissonHMM" href="#blechpy.analysis.poissonHMM.PoissonHMM">PoissonHMM</a></code></h4>
<ul class="">
<li><code><a title="blechpy.analysis.poissonHMM.PoissonHMM.fit" href="#blechpy.analysis.poissonHMM.PoissonHMM.fit">fit</a></code></li>
<li><code><a title="blechpy.analysis.poissonHMM.PoissonHMM.get_backward_probabilities" href="#blechpy.analysis.poissonHMM.PoissonHMM.get_backward_probabilities">get_backward_probabilities</a></code></li>
<li><code><a title="blechpy.analysis.poissonHMM.PoissonHMM.get_best_paths" href="#blechpy.analysis.poissonHMM.PoissonHMM.get_best_paths">get_best_paths</a></code></li>
<li><code><a title="blechpy.analysis.poissonHMM.PoissonHMM.get_forward_probabilities" href="#blechpy.analysis.poissonHMM.PoissonHMM.get_forward_probabilities">get_forward_probabilities</a></code></li>
<li><code><a title="blechpy.analysis.poissonHMM.PoissonHMM.get_gamma_probabilities" href="#blechpy.analysis.poissonHMM.PoissonHMM.get_gamma_probabilities">get_gamma_probabilities</a></code></li>
<li><code><a title="blechpy.analysis.poissonHMM.PoissonHMM.randomize" href="#blechpy.analysis.poissonHMM.PoissonHMM.randomize">randomize</a></code></li>
<li><code><a title="blechpy.analysis.poissonHMM.PoissonHMM.roll_back" href="#blechpy.analysis.poissonHMM.PoissonHMM.roll_back">roll_back</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>
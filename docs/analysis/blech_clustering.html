<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.7.1" />
<title>blechpy.analysis.blech_clustering API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>blechpy.analysis.blech_clustering</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import os
import shutil
import numpy as np
import pandas as pd
import itertools as it
import umap
from copy import deepcopy
from scipy.spatial.distance import mahalanobis
from scipy import linalg
from scipy.signal import find_peaks
from scipy.stats import sem
from sklearn.mixture import GaussianMixture
from sklearn.decomposition import PCA
from blechpy.utils import write_tools as wt, print_tools as pt, math_tools as mt, userIO
from blechpy.dio import h5io
from blechpy.analysis import clustering
from blechpy.plotting import data_plot as dplt
import datetime as dt


def detect_spikes(filt_el, spike_snapshot = [0.5, 1.0], fs = 30000.0):
    &#39;&#39;&#39;Detects spikes in the filtered electrode trace and return the waveforms
    and spike_times

    Parameters
    ----------
    filt_el : np.array, 1-D
        filtered electrode trace
    spike_snapshot : list
        2-elements, [ms before spike minimum, ms after spike minimum] 
        time around spike to snap as waveform
    fs : float, sampling rate in Hz

    Returns
    -------
    waves : np.array
        matrix of de-jittered, spike waveforms, upsampled by 10x, row for each spike
    times : np.array
        array of spike times in samples
    &#39;&#39;&#39;
    # get indices of spike snapshot, expand by .1 ms in each direction
    snapshot = np.arange(-(spike_snapshot[0]+0.1)*fs/1000,
                         1+(spike_snapshot[1]+0.1)*fs/1000).astype(&#39;int64&#39;)
    m = np.mean(filt_el)
    th = 5.0*np.median(np.abs(filt_el)/0.6745)
    pos = np.where(filt_el &lt;= m-th)[0]
    consecutive = mt.group_consecutives(pos)

    waves = []
    times = []
    for idx in consecutive:
        minimum = idx[np.argmin(filt_el[idx])]
        spike_idx = minimum + snapshot
        if spike_idx[0] &gt;= 0 and spike_idx[-1] &lt; len(filt_el):
            waves.append(filt_el[spike_idx])
            times.append(minimum)

    if len(waves) == 0:
        return None, None

    waves_dj, times_dj = clustering.dejitter(np.array(waves), np.array(times), spike_snapshot, fs)
    return waves_dj, times_dj


def implement_pca(scaled_slices):
    pca = PCA()
    pca_slices = pca.fit_transform(scaled_slices)
    return pca_slices, pca.explained_variance_ratio_


def implement_umap(waves, n_pc=3, n_neighbors=10, min_dist=0.0):
    reducer = umap.UMAP(n_components=n_pc,
                        n_neighbors=n_neighbors,
                        min_dist=min_dist)
    return reducer.fit_transform(waves)


def compute_waveform_metrics(waves, n_pc=3, umap=False):
    &#39;&#39;&#39;Make clustering data array with columns:
         - amplitudes, energy, slope, pc1, pc2, pc3, etc
    Parameters
    ----------
    waves : np.array
        waveforms with a row for each spike waveform
    n_pc : int (optional)
        number of principal components to include in data array

    Returns
    -------
    np.array
    &#39;&#39;&#39;
    data = np.zeros((waves.shape[0], 3))
    for i, wave in enumerate(waves):
        data[i,0] = np.min(wave)
        data[i,1] = np.sqrt(np.sum(wave**2))/len(wave)
        peaks = find_peaks(wave)[0]
        minima = np.argmin(wave)
        if not any(peaks &lt; minima):
            maxima = np.argmax(wave[:minima])
        else:
            maxima = max(peaks[np.where(peaks &lt; minima)[0]])

        data[i,2] = (wave[minima]-wave[maxima])/(minima-maxima)

    # Scale waveforms to energy before running PCA
    if umap:
        pc_waves = implement_umap(waves, n_pc=n_pc)
    else:
        scaled_waves = scale_waveforms(waves, energy=data[:,1])
        pc_waves, _ = implement_pca(scaled_waves)

    data = np.hstack((data, pc_waves[:,:n_pc]))
    data_columns = [&#39;amplitude&#39;, &#39;energy&#39;, &#39;spike_slope&#39;]
    data_columns.extend([&#39;PC%i&#39; % i for i in range(n_pc)])
    return data, data_columns


def get_waveform_amplitudes(waves):
    &#39;&#39;&#39;Returns array of waveform amplitudes

    Parameters
    ----------
    waves : np.array, matrix of waveforms, with row for each spike

    Returns
    -------
    np.array
    &#39;&#39;&#39;
    return np.min(waves,axis = 1)


def get_waveform_energy(waves):
    &#39;&#39;&#39;Returns array of waveform energies

    Parameters
    ----------
    waves : np.array, matrix of waveforms, with row for each spike

    Returns
    -------
    np.array
    &#39;&#39;&#39;
    energy = np.sqrt(np.sum(waves**2, axis=1))/waves.shape[1]
    return energy


def get_spike_slopes(waves):
    &#39;&#39;&#39;Returns array of spike slopes (initial downward slope of spike)

    Parameters
    ----------
    waves : np.array, matrix of waveforms, with row for each spike

    Returns
    -------
    np.array
    &#39;&#39;&#39;
    slopes = np.zeros((waves.shape[0],))
    for i, wave in enumerate(waves):
        peaks = find_peaks(wave)[0]
        minima = np.argmin(wave)
        if not any(peaks &lt; minima):
            maxima = np.argmax(wave[:minima])
        else:
            maxima = max(peaks[np.where(peaks &lt; minima)[0]])

        slopes[i] = (wave[minima]-wave[maxima])/(minima-maxima)

    return slopes


def get_ISI_and_violations(spike_times, fs, rec_map=None):
    &#39;&#39;&#39;returns array of ISIs in ms and # of 1ms and 2ms violations

    Parameters
    ----------
    spike_time  numpy.array
    fs : float, sampling rate in Hz
    rec_map : np.array (optional)
        if not passed, it is assumed all spike times are from same recording
        if passed, spike times are split into recordings and ISIs are computed
        per recording.
        If fs is different for each recording, fs should be a dict with keys as
        rec ids in rec_map

    Returns
    -------
    np.array : ISIs
    int : 1ms violations
    int : 2ms violations
    &#39;&#39;&#39;
    if rec_map is not None:
        if not isinstance(fs, dict):
            fs = dict.fromkeys(np.unique(rec_map), fs)

        ISIs = np.array([])
        violations1 = 0
        violations2 = 0
        for i in np.unique(rec_map):
            idx = np.where(rec_map == i)[0]
            tmp_isi, v1, v2 = get_ISI_and_violations(spike_times[idx], fs[i])
            violations1 += v1
            violations2 += v2
            ISIs = np.concatenate((ISIs, tmp_isi))

    else:
        fs = float(fs/1000.0)
        ISIs = np.ediff1d(np.sort(spike_times))/fs
        violations1 = np.sum(ISIs &lt; 1.0)
        violations2 = np.sum(ISIs &lt; 2.0)

    return ISIs, violations1, violations2


def scale_waveforms(waves, energy=None):
    &#39;&#39;&#39;Scales each waveform to its own energy

    Parameters
    ----------
    waves : np.array, matrix of waveforms, with row for each spike
    energy : np.array (optional)
        array of waveform energies, saves computation time

    Returns
    -------
    np.array
    &#39;&#39;&#39;
    if energy is None:
        energy = get_waveform_energy(waves)
    elif len(energy) != waves.shape[0]:
        raise ValueError((&#39;Energies must correspond to each waveforms.&#39;
                          &#39;Different lengths are not allowed&#39;))

    scaled_slices = np.zeros(waves.shape)
    for i, w in enumerate(zip(waves, energy)):
        scaled_slices[i] = w[0]/w[1]

    return scaled_slices


def get_mahalanobis_distances_to_cluster(data, model, clusters, target_cluster):
    &#39;&#39;&#39;computes mahalanobis distance from spikes in target_cluster to all clusters
    in GMM model

    Parameters
    ----------
    data : np.array, data used to train GMM
    model : fitted GMM model
    clusters : np.array, maps data points to clusters
    target_cluster : int, cluster for which to compute distances

    Returns
    -------
    np.array
    &#39;&#39;&#39;
    unique_clusters = np.unique(abs(clusters))
    out_distances = dict.fromkeys(unique_clusters)
    cluster_idx = np.where(clusters == target_cluster)[0]
    for other_cluster in unique_clusters:
        mahalanobis_dist = np.zeros((len(cluster_idx),))
        other_cluster_mean = model.means_[other_cluster, :]
        other_cluster_covar_I = linalg.inv(model.covariances_[other_cluster, :, :])
        for i, idx in enumerate(cluster_idx):
            mahalanobis_dist[i] = mahalanobis(data[idx, :],
                                              other_cluster_mean,
                                              other_cluster_covar_I)

        out_distances[other_cluster] = mahalanobis_dist

    return out_distances


def get_recording_cutoff(filt_el, sampling_rate, voltage_cutoff,
                         max_breach_rate, max_secs_above_cutoff,
                         max_mean_breach_rate_persec, **kwargs):
    breach_idx = np.where(filt_el &gt; voltage_cutoff)[0]
    breach_rate = float(len(breach_idx)*int(sampling_rate))/len(filt_el)
    # truncate to nearest second and make 1 sec bins
    filt_el = filt_el[:int(sampling_rate)*int(len(filt_el)/sampling_rate)]
    test_el = np.reshape(filt_el, (-1, int(sampling_rate)))
    breaches_per_sec = [len(np.where(test_el[i] &gt; voltage_cutoff)[0])
                        for i in range(len(test_el))]
    breaches_per_sec = np.array(breaches_per_sec)
    secs_above_cutoff = len(np.where(breaches_per_sec &gt; 0)[0])
    if secs_above_cutoff == 0:
        mean_breach_rate_persec = 0
    else:
        mean_breach_rate_persec = np.mean(breaches_per_sec[np.where(breaches_per_sec &gt; 0)[0]])

    # And if they all exceed the cutoffs, assume that the headstage fell off mid-experiment
    recording_cutoff = int(len(filt_el)/sampling_rate) # cutoff in seconds
    if (breach_rate &gt;= max_breach_rate and
        secs_above_cutoff &gt;= max_secs_above_cutoff and
        mean_breach_rate_persec &gt;= max_mean_breach_rate_persec):
        # Find the first 1 second epoch where the number of cutoff breaches is
        # higher than the maximum allowed mean breach rate 
        recording_cutoff = np.where(breaches_per_sec &gt; max_mean_breach_rate_persec)[0][0]
        # cutoff is still in seconds since 1 sec bins

    return recording_cutoff


class SpikeDetection(object):
    &#39;&#39;&#39;Interface to manage spike detection and data extraction in preparation
    for GMM clustering. Intended to help create and access the neccessary
    files. If object will detect is file already exist to avoid re-creation
    unless overwrite is specified as True.
    &#39;&#39;&#39;

    def __init__(self, file_dir, electrode, params=None, overwrite=False):
        # Setup paths to files and directories needed
        self._file_dir = file_dir
        self._electrode = electrode
        self._out_dir = os.path.join(file_dir, &#39;spike_detection&#39;,
                                     &#39;electrode_%i&#39; % electrode)
        self._data_dir = os.path.join(self._out_dir, &#39;data&#39;)
        self._plot_dir = os.path.join(self._out_dir, &#39;plots&#39;)
        self._files = {&#39;params&#39;: os.path.join(file_dir,&#39;analysis_params&#39;, &#39;spike_detection_params.json&#39;),
                       &#39;spike_waveforms&#39;: os.path.join(self._data_dir, &#39;spike_waveforms.npy&#39;),
                       &#39;spike_times&#39; : os.path.join(self._data_dir, &#39;spike_times.npy&#39;),
                       &#39;energy&#39; : os.path.join(self._data_dir, &#39;energy.npy&#39;),
                       &#39;spike_amplitudes&#39; : os.path.join(self._data_dir, &#39;spike_amplitudes.npy&#39;),
                       &#39;pca_waveforms&#39; : os.path.join(self._data_dir, &#39;pca_waveforms.npy&#39;),
                       &#39;slopes&#39; : os.path.join(self._data_dir, &#39;spike_slopes.npy&#39;),
                       &#39;recording_cutoff&#39; : os.path.join(self._data_dir, &#39;cutoff_time.txt&#39;)}

        self._status = dict.fromkeys(self._files.keys(), False)
        self._referenced = True

        # Delete existing data if overwrite is True
        if overwrite and os.path.isdir(self._out_dir):
            shutil.rmtree(self._out_dir)

        # See what data already exists
        self._check_existing_files()

        # Make directories if needed
        if not os.path.isdir(self._out_dir):
            os.makedirs(self._out_dir)

        if not os.path.isdir(self._data_dir):
            os.makedirs(self._data_dir)

        if not os.path.isdir(self._plot_dir):
            os.makedirs(self._plot_dir)

        if not os.path.isdir(os.path.join(file_dir, &#39;analysis_params&#39;)):
            os.makedirs(os.path.join(file_dir, &#39;analysis_params&#39;))

        # grab recording cutoff time if it already exists
        # cutoff should be in seconds
        self.recording_cutoff = None
        if os.path.isfile(self._files[&#39;recording_cutoff&#39;]):
            self._status[&#39;recording_cutoff&#39;] = True
            with open(self._files[&#39;recording_cutoff&#39;], &#39;r&#39;) as f:
                self.recording_cutoff = float(f.read())

        # Read in parameters
        # Parameters passed as an argument will overshadow parameters saved in file
        # Input parameters should be formatted as dataset.clustering_parameters
        if params is None and os.path.isfile(self._files[&#39;params&#39;]):
            self.params = wt.read_dict_from_json(self._files[&#39;params&#39;])
        elif params is None:
            raise FileNotFoundError(&#39;params must be provided if spike_detection_params.json does not exist.&#39;)
        else:
            self.params = {}
            self.params[&#39;voltage_cutoff&#39;] = params[&#39;data_params&#39;][&#39;V_cutoff for disconnected headstage&#39;]
            self.params[&#39;max_breach_rate&#39;] = params[&#39;data_params&#39;][&#39;Max rate of cutoff breach per second&#39;]
            self.params[&#39;max_secs_above_cutoff&#39;] = params[&#39;data_params&#39;][&#39;Max allowed seconds with a breach&#39;]
            self.params[&#39;max_mean_breach_rate_persec&#39;] = params[&#39;data_params&#39;][&#39;Max allowed breaches per second&#39;]
            band_lower = params[&#39;bandpass_params&#39;][&#39;Lower freq cutoff&#39;]
            band_upper = params[&#39;bandpass_params&#39;][&#39;Upper freq cutoff&#39;]
            self.params[&#39;bandpass&#39;] = [band_lower, band_upper]
            snapshot_pre = params[&#39;spike_snapshot&#39;][&#39;Time before spike (ms)&#39;]
            snapshot_post = params[&#39;spike_snapshot&#39;][&#39;Time after spike (ms)&#39;]
            self.params[&#39;spike_snapshot&#39;] = [snapshot_pre, snapshot_post]
            self.params[&#39;sampling_rate&#39;] = params[&#39;sampling_rate&#39;]
            # Write params to json file
            wt.write_dict_to_json(self.params, self._files[&#39;params&#39;])
            self._status[&#39;params&#39;] = True

    def _check_existing_files(self):
        &#39;&#39;&#39;Checks which files already exist and updates _status so as to avoid
        re-creation later
        &#39;&#39;&#39;
        for k, v in self._files.items():
            if os.path.isfile(v):
                self._status[k] = True
            else:
                self._status[k] = False

    def run(self):
        status = self._status
        file_dir = self._file_dir
        electrode = self._electrode
        params = self.params
        fs = params[&#39;sampling_rate&#39;]

        # Check if this even needs to be run
        if all(status.values()):
            return electrode, 1, self.recording_cutoff

        # Grab referenced electrode or raw if ref is not available
        ref_el = h5io.get_referenced_trace(file_dir, electrode)
        if ref_el is None:
            print(&#39;Could not find referenced data for electrode %i. Using raw.&#39; % electrode)
            self._referenced = False
            ref_el = h5io.get_raw_trace(file_dir, electrode)
            if ref_el is None:
                raise KeyError(&#39;Neither referenced nor raw data found for electrode %i in %s&#39; % (electrode, file_dir))

        # Filter electrode trace
        filt_el = clustering.get_filtered_electrode(ref_el, freq=params[&#39;bandpass&#39;],
                                               sampling_rate = fs)
        del ref_el
        # Get recording cutoff
        if not status[&#39;recording_cutoff&#39;]:
            self.recording_cutoff = get_recording_cutoff(filt_el, **params)
            with open(self._files[&#39;recording_cutoff&#39;], &#39;w&#39;) as f:
                f.write(str(self.recording_cutoff))

            status[&#39;recording_cutoff&#39;] = True
            fn = os.path.join(self._plot_dir, &#39;cutoff_time.png&#39;)
            dplt.plot_recording_cutoff(filt_el, fs, self.recording_cutoff,
                                       out_file=fn)

        # Truncate electrode trace, deal with early cutoff (&lt;60s)
        if self.recording_cutoff &lt; 60:
            print(&#39;Immediate Cutoff for electrode %i...exiting&#39; % electrode)
            return electrode, 0, self.recording_cutoff

        filt_el = filt_el[:int(self.recording_cutoff*fs)]

        if status[&#39;spike_waveforms&#39;] and status[&#39;spike_times&#39;]:
            waves = np.load(self._files[&#39;spike_waveforms&#39;])
            times = np.load(self._files[&#39;spike_times&#39;])
        else:
            # Detect spikes and get dejittered times and waveforms
            # detect_spikes returns waveforms upsampled by 10x and times in units
            # of samples
            waves, times = detect_spikes(filt_el, params[&#39;spike_snapshot&#39;], fs)
            if waves is None:
                print(&#39;No waveforms detected on electrode %i&#39; % electrode)
                return electrode, 0, self.recording_cutoff

            # Save waveforms and times
            np.save(self._files[&#39;spike_waveforms&#39;], waves)
            np.save(self._files[&#39;spike_times&#39;], times)
            status[&#39;spike_waveforms&#39;] = True
            status[&#39;spike_times&#39;] = True

        # Get various metrics and scale waveforms
        if not status[&#39;spike_amplitudes&#39;]:
            amplitudes = get_waveform_amplitudes(waves)
            np.save(self._files[&#39;spike_amplitudes&#39;], amplitudes)
            status[&#39;spike_amplitudes&#39;] = True

        if not status[&#39;slopes&#39;]:
            slopes = get_spike_slopes(waves)
            np.save(self._files[&#39;slopes&#39;], slopes)
            status[&#39;slopes&#39;] = True

        if not status[&#39;energy&#39;]:
            energy = get_waveform_energy(waves)
            np.save(self._files[&#39;energy&#39;], energy)
            status[&#39;energy&#39;] = True
        else:
            energy=None

        # get pca of scaled waveforms
        if not status[&#39;pca_waveforms&#39;]:
            scaled_waves = scale_waveforms(waves, energy=energy)
            pca_waves, explained_variance_ratio = implement_pca(scaled_waves)

            # Plot explained variance
            fn = os.path.join(self._plot_dir, &#39;pca_variance.png&#39;)
            dplt.plot_explained_pca_variance(explained_variance_ratio,
                                             out_file = fn)

        return electrode, 1, self.recording_cutoff

    def get_spike_waveforms(self):
        &#39;&#39;&#39;Returns spike waveforms if they have been extracted, None otherwise
        Dejittered waveforms upsampled to 10 x sampling_rate

        Returns
        -------
        numpy.array
        &#39;&#39;&#39;
        if os.path.isfile(self._files[&#39;spike_waveforms&#39;]):
            return np.load(self._files[&#39;spike_waveforms&#39;])
        else:
            return None

    def get_spike_times(self):
        &#39;&#39;&#39;Returns spike times if they have been extracted, None otherwise
        In units of samples.

        Returns
        -------
        numpy.array
        &#39;&#39;&#39;
        if os.path.isfile(self._files[&#39;spike_times&#39;]):
            return np.load(self._files[&#39;spike_times&#39;])
        else:
            return None

    def get_energy(self):
        &#39;&#39;&#39;Returns spike energies if they have been extracted, None otherwise

        Returns
        -------
        numpy.array
        &#39;&#39;&#39;
        if os.path.isfile(self._files[&#39;energy&#39;]):
            return np.load(self._files[&#39;energy&#39;])
        else:
            return None

    def get_spike_amplitudes(self):
        &#39;&#39;&#39;Returns spike amplitudes if they have been extracted, None otherwise

        Returns
        -------
        numpy.array
        &#39;&#39;&#39;
        if os.path.isfile(self._files[&#39;spike_amplitudes&#39;]):
            return np.load(self._files[&#39;spike_amplitudes&#39;])
        else:
            return None

    def get_spike_slopes(self):
        &#39;&#39;&#39;Returns spike slopes if they have been extracted, None otherwise

        Returns
        -------
        numpy.array
        &#39;&#39;&#39;
        if os.path.isfile(self._files[&#39;slopes&#39;]):
            return np.load(self._files[&#39;slopes&#39;])
        else:
            return None

    def get_pca_waveforms(self):
        &#39;&#39;&#39;Returns pca of sclaed spike waveforms if they have been extracted,
        None otherwise
        Dejittered waveforms upsampled to 10 x sampling_rate, scaled to energy
        and transformed via PCA

        Returns
        -------
        numpy.array
        &#39;&#39;&#39;
        if os.path.isfile(self._files[&#39;spike_waveforms&#39;]):
            return np.load(self._files[&#39;spike_waveforms&#39;])
        else:
            return None

    def get_clustering_metrics(self, n_pc=3):
        &#39;&#39;&#39;Returns array of metrics to use for feature based clustering
        Row for each waveform with columns:
            - amplitude, energy, spike slope, PC1, PC2, etc
        &#39;&#39;&#39;
        amplitude = self.get_spike_amplitudes()
        energy = self.get_energy()
        slopes = self.get_spike_slopes()
        pca_waves = self.get_pca_waveforms()
        out = np.vstack((amplitude, energy, slopes)).T
        out = np.hstack((out, pca_waves[:,:n_pc]))
        return out

    def __str__(self):
        out = []
        out.append(&#39;SpikeDetection\n--------------&#39;)
        out.append(&#39;Recording Directory: %s&#39; % self._file_dir)
        out.append(&#39;Electrode: %i&#39; % self._electrode)
        out.append(&#39;Output Directory: %s&#39; % self._out_dir)
        out.append(&#39;###################################\n&#39;)
        out.append(&#39;Status:&#39;)
        out.append(pt.print_dict(self._status))
        out.append(&#39;-------------------\n&#39;)
        out.append(&#39;Parameters:&#39;)
        out.append(pt.print_dict(self.params))
        out.append(&#39;-------------------\n&#39;)
        out.append(&#39;Data files:&#39;)
        out.append(pt.print_dict(self._files))
        return &#39;\n&#39;.join(out)


class BlechClust(object):
    def __init__(self, rec_dirs, electrode, out_dir=None, params=None,
                 overwrite=False, no_write=False):
        &#39;&#39;&#39;Recording directories should be ordered to make spike sorting easier later on
        &#39;&#39;&#39;
        if isinstance(rec_dirs, str):
            rec_dirs = [rec_dirs]

        rec_dirs = [x[:-1] if x.endswith(os.sep) else x for x in rec_dirs]
        self.rec_dirs = rec_dirs
        self.electrode = electrode
        if out_dir is None:
            if len(rec_dirs) &gt; 1:
                top = os.path.dirname(rec_dirs[0])
                out_dir = os.path.join(top, &#39;BlechClust&#39;, &#39;electrode_%i&#39; % electrode)
            else:
                out_dir = os.path.join(rec_dirs[0], &#39;BlechClust&#39;, &#39;electrode_%i&#39; % electrode)

        if overwrite:
            shutil.rmtree(out_dir)

        # Make directories
        self.out_dir = out_dir
        self._plot_dir = os.path.join(out_dir, &#39;plots&#39;)
        self._data_dir = os.path.join(out_dir, &#39;clustering_results&#39;)
        if not os.path.isdir(out_dir):
            os.makedirs(out_dir)

        if not os.path.isdir(self._data_dir):
            os.mkdir(self._data_dir)

        if not os.path.isdir(self._plot_dir):
            os.mkdir(self._plot_dir)

        # Check files
        params_file = os.path.join(out_dir, &#39;BlechClust_params.json&#39;)
        map_file = os.path.join(self._data_dir, &#39;spike_id.npy&#39;)
        key_file = os.path.join(self._data_dir, &#39;rec_key.json&#39;)
        results_file = os.path.join(self._data_dir, &#39;clustering_results.json&#39;)
        self._files = {&#39;params&#39;: params_file, &#39;spike_map&#39;: map_file,
                       &#39;rec_key&#39;: key_file, &#39;clustering_results&#39;: results_file}
        self.params = params
        self._load_existsing_data()

        if self._rec_key is None and not no_write:
            # Create new rec key
            rec_key = {x:y for x,y in enumerate(self.rec_dirs)}
            self._rec_key = rec_key
            wt.write_dict_to_json(rec_key, self._files[&#39;rec_key&#39;])
        elif self._rec_key is None:
            ValueError(&#39;Existing rec_key not found and no_write is enabled&#39;)

        # Check to see if spike detection is already completed on all recording directories
        spike_check = self._check_spike_detection()
        if not all(spike_check):
            invalid = [rec_dirs[i] for i, x in enumerate(spike_check) if x==False]
            error_str = &#39;\n\t&#39;.join(invalid)
            raise ValueError(&#39;Spike detection has not been run on:\n\t%s&#39; % error_str)

    def _load_existsing_data(self):
        params = self.params
        file_check = self._check_existing_files()

        # Check params files and create if new params are passed
        if file_check[&#39;params&#39;]:
            self.params = wt.read_dict_from_json(self._files[&#39;params&#39;])

        # Make new params or overwrite existing with passed params
        if params is None and not file_check[&#39;params&#39;]:
            raise ValueError((&#39;Params file does not exists at %s. Must provide&#39;
                              &#39; clustering parameters.&#39;) % self._files[&#39;params&#39;])
        elif params is not None:
            self.params[&#39;max_clusters&#39;] = params[&#39;clustering_params&#39;][&#39;Max Number of Clusters&#39;]
            self.params[&#39;max_iterations&#39;] = params[&#39;clustering_params&#39;][&#39;Max Number of Iterations&#39;]
            self.params[&#39;threshold&#39;] = params[&#39;clustering_params&#39;][&#39;Convergence Criterion&#39;]
            self.params[&#39;num_restarts&#39;] = params[&#39;clustering_params&#39;][&#39;GMM random restarts&#39;]
            self.params[&#39;wf_amplitude_sd_cutoff&#39;] = params[&#39;data_params&#39;][&#39;Intra-cluster waveform amp SD cutoff&#39;]
            wt.write_dict_to_json(self.params, self._files[&#39;params&#39;])

        # Deal with existing rec key
        if file_check[&#39;rec_key&#39;]:
            rec_key = wt.read_dict_from_json(self._files[&#39;rec_key&#39;])
            rec_key = {int(x): y for x,y in rec_key.items()}
            inverted = {v:k for k,v in rec_key.items()}
            self.rec_dirs = sorted(self.rec_dirs, key=lambda i: inverted[i])
            self._rec_key = rec_key
        else:
            self._rec_key = None

        # Check is clustering has already been done, load results
        if file_check[&#39;clustering_results&#39;]:
            self.results = wt.read_pandas_from_table(self._files[&#39;clustering_results&#39;])
            self.clustered = True
        else:
            self.results = None
            self.clustered = False

    def _check_existing_files(self):
        out = dict.fromkeys(self._files.keys(), False)
        for k,v in self._files.items():
            if os.path.isfile(v):
                out[k] = True

        return out

    def _check_spike_detection(self):
        &#39;&#39;&#39;Check to see if spike detection has been run on all recording directories
        &#39;&#39;&#39;
        out = []
        for rec in  self.rec_dirs:
            try:
                spike_detect = SpikeDetection(rec, self.electrode)
                if all(spike_detect._status):
                    out.append(True)
                else:
                    out.append(False)

            except FileNotFoundError:
                out.append(False)

        return out

    def run(self, n_pc=3, overwrite=False):
        if self.clustered and not overwrite:
            return True

        GMM = ClusterGMM(self.params[&#39;max_iterations&#39;],
                         self.params[&#39;num_restarts&#39;], self.params[&#39;threshold&#39;])

        # Collect data from all recordings
        waveforms, spike_times, spike_map, fs, offsets = self.get_spike_data()

        # Save array to map spikes and predictions back to original recordings
        np.save(self._files[&#39;spike_map&#39;], spike_map)

        data, data_columns = compute_waveform_metrics(waveforms, n_pc)
        amplitudes = get_waveform_amplitudes(waveforms)

        # Run GMM for each number of clusters from 2 to max_clusters
        tested_clusters = np.arange(2, self.params[&#39;max_clusters&#39;]+1)
        clust_results = pd.DataFrame(columns=[&#39;clusters&#39;,&#39;converged&#39;,
                                              &#39;BIC&#39;,&#39;spikes_per_cluster&#39;],
                                     index=tested_clusters)
        for n_clust in tested_clusters:
            data_dir = os.path.join(self._data_dir, &#39;%i_clusters&#39; % n_clust)
            plot_dir = os.path.join(self._plot_dir, &#39;%i_clusters&#39; % n_clust)
            wave_plot_dir = os.path.join(self._plot_dir, &#39;%i_clusters_waveforms_ISIs&#39; % n_clust)
            bic_file = os.path.join(data_dir, &#39;bic.npy&#39;)
            pred_file = os.path.join(data_dir, &#39;predictions.npy&#39;)

            if os.path.isfile(bic_file) and os.path.isfile(pred_file):
                bic = np.load(bic_file)
                predictions = np.load(pred_file)
                spikes_per_clust = [len(np.where(predictions == c)[0])
                                    for c in np.unique(predictions)]
                clust_results.loc[n_clust] = [n_clust, True, bic, spikes_per_clust]
                continue

            if not os.path.isdir(wave_plot_dir):
                os.makedirs(wave_plot_dir)

            if not os.path.isdir(data_dir):
                os.makedirs(data_dir)

            if not os.path.isdir(plot_dir):
                os.makedirs(plot_dir)

            model, predictions, bic = GMM.fit(data, n_clust)
            if model is None:
                clust_results.loc[n_clust] = [n_clust, bic, False, [0]]
                # Nothing converged
                continue

            # Go through each cluster and throw out any spikes too far from the
            # mean
            spikes_per_clust = []
            for c in range(n_clust):
                idx = np.where(predictions == c)[0]
                mean_amp = np.mean(amplitudes[idx])
                sd_amp = np.std(amplitudes[idx])
                cutoff_amp = mean_amp - (sd_amp * self.params[&#39;wf_amplitude_sd_cutoff&#39;])
                rejected_idx = np.array([i for i in idx if amplitudes[i] &lt;= cutoff_amp])
                if len(rejected_idx) &gt; 0:
                    predictions[rejected_idx] = -1

                idx = np.where(predictions == c)[0]
                spikes_per_clust.append(len(idx))

                if len(idx) == 0:
                    continue

                # Plot waveforms and ISIs of cluster
                ISIs, violations_1ms, violations_2ms = get_ISI_and_violations(spike_times[idx], fs, spike_map[idx])
                cluster_waves = waveforms[idx]
                cluster_times = spike_times[idx]
                isi_fn = os.path.join(wave_plot_dir, &#39;Cluster%i_ISI.png&#39; % c)
                wave_fn = os.path.join(wave_plot_dir, &#39;Cluster%i_waveforms.png&#39; % c)
                title_str = (&#39;Cluster%i\nviolations_1ms = %i, &#39;
                             &#39;violations_2ms = %i\n&#39;
                             &#39;Number of waveforms = %i&#39; %
                             (c, violations_1ms, violations_2ms, len(idx)))
                dplt.plot_waveforms(cluster_waves, title=title_str, save_file=wave_fn)
                dplt.plot_ISIs(ISIs, total_spikes=len(idx), save_file=isi_fn)


            clust_results.loc[n_clust] = [n_clust, True, bic, spikes_per_clust]

            # Plot feature pairs
            feature_pairs = it.combinations(list(range(data.shape[1])), 2)
            for f1, f2 in feature_pairs:
                fn = &#39;%sVS%s.png&#39; % (data_columns[f1], data_columns[f2])
                fn = os.path.join(plot_dir, fn)
                dplt.plot_cluster_features(data[:, [f1,f2]], predictions,
                                           x_label = data_columns[f1],
                                           y_label = data_columns[f2],
                                           save_file = fn)

            # For each cluster plot mahanalobis distances to all other clusters
            for c in range(n_clust):
                distances = get_mahalanobis_distances_to_cluster(data,  model,
                                                                 predictions, c)
                fn = os.path.join(plot_dir, &#39;Mahalanobis_cluster%i.png&#39; % c)
                title = (&#39;Mahalanobis distance of Cluster %i from all other clusters&#39; % c)
                dplt.plot_mahalanobis_to_cluster(distances, title=title, save_file=fn)

            # Save data
            np.save(bic_file, bic)
            np.save(pred_file, predictions)

        # Save results table
        self.results = clust_results
        wt.write_pandas_to_table(clust_results, self._files[&#39;clustering_results&#39;])
        self.clustered = True
        return True

    def get_spike_data(self):
        # Collect data from all recordings
        tmp_waves = []
        tmp_times = []
        tmp_id = []
        fs = dict.fromkeys(self._rec_key.keys())
        offsets = dict.fromkeys(self._rec_key.keys())
        offset = 0
        for i in sorted(self._rec_key.keys()):
            rec = self._rec_key[i]
            spike_detect = SpikeDetection(rec, self.electrode)
            t = spike_detect.get_spike_times()
            fs[i] = spike_detect.params[&#39;sampling_rate&#39;]
            if t is None:
                offsets[i] = int(offset)
                offset = offset + 3*fs[i]
                continue

            tmp_waves.append(spike_detect.get_spike_waveforms())
            tmp_times.append(t)
            tmp_id.append(np.ones((t.shape[0],))*i)
            offsets[i] = int(offset)
            offset = offset + max(t) + 3*fs[i]

        waveforms = np.vstack(tmp_waves)
        spike_times = np.hstack(tmp_times)
        spike_map = np.hstack(tmp_id)

        # Double check that spike_map matches up with existing spike_map
        if os.path.isfile(self._files[&#39;spike_map&#39;]):
            orig_map = np.load(self._files[&#39;spike_map&#39;])
            if len(orig_map) != len(spike_map):
                raise ValueError(&#39;Spike detection has changed, please re-cluster with overwrite=True&#39;)

        return waveforms, spike_times, spike_map, fs, offsets

    def get_clusters(self, solution_num, cluster_nums):
        if not isinstance(cluster_nums, list):
            cluster_nums = [cluster_nums]

        waveforms, times, spike_map, fs, offsets = self.get_spike_data()
        predictions = self.get_predictions(solution_num)
        out = []
        for c in cluster_nums:
            idx = np.where(predictions == c)[0]
            if len(idx)==0:
                continue

            tmp_clust = {&#39;Cluster Name&#39;: &#39;Cluster_%i&#39; % c,
                         &#39;solution_num&#39;: solution_num,
                         &#39;cluster_num&#39;: c,
                         &#39;cluster_id&#39;: 1,
                         &#39;spike_waveforms&#39;: waveforms[idx],
                         &#39;spike_times&#39;: times[idx],
                         &#39;spike_map&#39;: spike_map[idx],
                         &#39;rec_key&#39;: self._rec_key.copy(),
                         &#39;fs&#39;: fs,
                         &#39;offsets&#39;: offsets,
                         &#39;manipulations&#39;: &#39;&#39;}
            out.append(tmp_clust)

        return out

    def get_predictions(self, n_clusters):
        fn = os.path.join(self._data_dir, &#39;%i_clusters&#39; % n_clusters,
                          &#39;predictions.npy&#39;)
        if os.path.isfile(fn):
            return np.load(fn)
        else:
            return None


class ClusterGMM(object):
    def __init__(self, n_iters, n_restarts, thresh):
        self.params = {&#39;iterations&#39;: n_iters,
                       &#39;restarts&#39;: n_restarts,
                       &#39;thresh&#39;: thresh}

    def fit(self, data, n_clusters):
        min_bic = None
        best_model = None
        if n_clusters is not None:
            self.params[&#39;clusters&#39;] = n_clusters

        for i in range(self.params[&#39;restarts&#39;]):
            model = GaussianMixture(n_components = self.params[&#39;clusters&#39;],
                                    covariance_type = &#39;full&#39;,
                                    tol = self.params[&#39;thresh&#39;],
                                    random_state = i,
                                    max_iter = self.params[&#39;iterations&#39;])
            model.fit(data)
            if model.converged_:
                new_bic = model.bic(data)
                if min_bic is None:
                    min_bic = model.bic(data)
                    best_model = model
                elif new_bic &lt; min_bic:
                    best_model = model
                    min_bic = new_bic

        predictions = best_model.predict(data)
        self._model = best_model
        self._predictions = predictions
        self._bic = min_bic
        return best_model, predictions, min_bic


class SpikeSorter(object):
    def __init__(self, rec_dirs, electrode, clustering_dir=None, shell=False):
        if isinstance(rec_dirs, str):
            rec_dirs = [rec_dirs]

        rec_dirs = [x[:-1] if x.endswith(os.sep) else x for x in rec_dirs]
        self.rec_dirs = rec_dirs
        self.electrode = electrode
        if clustering_dir is None:
            if len(rec_dirs) &gt; 1:
                top = os.path.dirname(rec_dirs[0])
                clustering_dir = os.path.join(top, &#39;BlechClust&#39;, &#39;electrode_%i&#39; % electrode)
            else:
                clustering_dir = os.path.join(rec_dirs[0], &#39;BlechClust&#39;, &#39;electrode_%i&#39; % electrode)

        self.clustering_dir = clustering_dir
        try:
            clust = BlechClust(rec_dirs, electrode, out_dir = clustering_dir, no_write=True)
        except FileNotFoundError:
            clust = None

        if clust is None or not clust.clustered:
            raise ValueError(&#39;Recordings have not been clustered yet.&#39;)

        # Match recording directory ordering to clustering
        self.rec_dirs = clust.rec_dirs
        self.clustering = clust
        self._active = None
        self._last_saved = None
        self._previous = None
        self._shell = shell
        self._split_results = None
        self._split_starter = None
        self._split_index = None

    def set_active_clusters(self, solution_num):
        cluster_nums = list(range(solution_num))
        clusters = self.clustering.get_clusters(solution_num, cluster_nums)
        if len(clusters) == 0:
            raise ValueError(&#39;Solution or clusters not found&#39;)

        self._active = clusters

    def save_clusters(self, target_clusters, single_unit, pyramidal, interneuron):
        &#39;&#39;&#39;Saves active clusters as cells, write them to the h5_files in the
        appropriate recording directories

        Parameters
        ----------
        target_clusters: list of int
            indicies of active clusters to save
        single_unit : list of bool
            elements in list must correspond to elements in active clusters
        pyramidal : list of bool
        interneuron : list of bool
        &#39;&#39;&#39;
        if self._active is None:
            return

        if any([i &gt;= len(self._active) for i in target_clusters]):
            raise ValueError(&#39;Target cluster is out of range.&#39;)

        n_clusters = len(target_clusters)
        if (len(single_unit) != n_clusters or len(pyramidal) != n_clusters or
            len(interneuron) != n_clusters):
            raise ValueError(&#39;Length of input lists must match number of &#39;
                             &#39;active clusters. Expected %i&#39; % n_clusters)

        clusters = [self._active[i] for i in target_clusters]
        rec_key = self.clustering._rec_key
        self._last_saved = dict.fromkeys(rec_key.keys(), None)

        for clust, single, pyr, intr in zip(clusters, single_unit,
                                            pyramidal, interneuron):
            for i, rec in rec_key.items():
                idx = np.where(clust[&#39;spike_map&#39;] == i)[0]
                waves = clust[&#39;spike_waveforms&#39;][idx]
                times = clust[&#39;spike_times&#39;][idx]
                unit_name = h5io.add_new_unit(rec, self.electrode, waves,
                                              times, single, pyr, intr)
                if self._last_saved[i] is None:
                    self._last_saved[i] = [unit_name]
                else:
                    self._last_saved[i].append(unit_name)

                metrics_dir = os.path.join(rec,&#39;sorted_unit_metrics&#39;, unit_name)
                if not os.path.isdir(metrics_dir):
                    os.makedirs(metrics_dir)

                # Write cluster info to file
                print_clust = clust.copy()
                for k,v in clust.items():
                    if isinstance(v, np.ndarray):
                        print_clust.pop(k)

                print_clust.pop(&#39;rec_key&#39;)
                print_clust.pop(&#39;fs&#39;)
                clust_info_file = os.path.join(metrics_dir, &#39;cluster.info&#39;)
                with open(clust_info_file, &#39;a+&#39;) as log:
                    print(&#39;%s sorted on %s&#39;
                          % (unit_name,
                             dt.datetime.today().strftime(&#39;%m/%d/%y %H:%M&#39;)),
                          file=log)
                    print(&#39;Cluster info: \n----------&#39;, file=log)
                    print(pt.print_dict(print_clust), file=log)
                    print(&#39;Saved metrics to %s&#39; % metrics_dir, file=log)
                    print(&#39;--------------\n&#39;, file=log)

        userIO.tell_user(&#39;Target clusters successfully saved to recording &#39;
                         &#39;directories.&#39;, shell=True)
        self._active = [self._active[i] for i in range(len(self._active))
                        if i not in target_clusters]
        self._previous = clusters

    def undo_last_save(self):
        if self._last_saved is None:
            return

        rec_key = self.clustering._rec_key
        last_saved = self._last_saved
        for i, rec in rec_key.items():
            for unit in reversed(np.sort(last_saved[i])):
                h5io.delete_unit(rec, unit)

        self._active.extend(self._previous)
        self._last_saved = None
        self._previous = None

    def split_cluster(self, target_clust, n_iter, n_restart, thresh, n_clust,
                      store_split=False, umap=False):
        &#39;&#39;&#39;splits the target active cluster using a GMM
        &#39;&#39;&#39;
        if target_clust &gt;= len(self._active):
            raise ValueError(&#39;Invalid target. Only %i active clusters&#39; % len(self._active))

        cluster = self._active.pop(target_clust)
        self._split_starter = cluster
        GMM = ClusterGMM(n_iter, n_restart, thresh)
        waves = cluster[&#39;spike_waveforms&#39;]
        data, data_columns = compute_waveform_metrics(waves, umap=umap)
        model, predictions, bic = GMM.fit(data, n_clust)
        new_clusts = []
        for i in np.unique(predictions):
            idx = np.where(predictions == i)[0]
            edit_str = (cluster[&#39;manipulations&#39;] + &#39;Split %s into %i &#39;
                        &#39;clusters. This is sub-cluster %i&#39;
                        % (cluster[&#39;manipulations&#39;], n_clust, i))
            tmp_clust = {&#39;Cluster Name&#39;: cluster[&#39;Cluster Name&#39;] + &#39;-%i&#39; % i,
                         &#39;solution_num&#39;: cluster[&#39;solution_num&#39;],
                         &#39;cluster_num&#39;: cluster[&#39;cluster_num&#39;],
                         &#39;cluster_id&#39;: cluster[&#39;cluster_id&#39;]*10+i,
                         &#39;spike_waveforms&#39;: waves[idx],
                         &#39;spike_times&#39;: cluster[&#39;spike_times&#39;][idx],
                         &#39;spike_map&#39;: cluster[&#39;spike_map&#39;][idx],
                         &#39;rec_key&#39;: cluster[&#39;rec_key&#39;].copy(),
                         &#39;fs&#39;: cluster[&#39;fs&#39;],
                         &#39;offsets&#39;: cluster[&#39;offsets&#39;],
                         &#39;manipulations&#39;: edit_str}
            new_clusts.append(tmp_clust)

        # Plot cluster and ask to choose which to keep
        figs = []
        for i, c in enumerate(new_clusts):
            _, viol_1ms, viol_2ms = get_ISI_and_violations(c[&#39;spike_times&#39;], c[&#39;fs&#39;], c[&#39;spike_map&#39;])
            plot_title = (&#39;Index: %i\n1ms violations: %i, 2ms violations: %i\n&#39;
                          &#39;Total Waveforms: %i&#39;
                          % (i, viol_1ms, viol_2ms, len(c[&#39;spike_times&#39;])))
            tmp_fig, _ = dplt.plot_waveforms(c[&#39;spike_waveforms&#39;], title=plot_title)
            figs.append(tmp_fig)
            tmp_fig.show()

        f2 = dplt.plot_waveforms_pca([c[&#39;spike_waveforms&#39;] for c in new_clusts])
        figs.append(f2)
        f2.show()

        if store_split:
            self._split_results = new_clusts
            self._split_index = target_clust
            return new_clusts
        else:
            self._split_starter = None
            selection_list = [&#39;all&#39;] + [&#39;%i&#39; % i for i in range(len(new_clusts))]
            prompt = &#39;Select split clusters to keep\nCancel to reset.&#39;
            ans = userIO.select_from_list(prompt, selection_list,
                                          multi_select=True, shell=self._shell)
            if ans is None or &#39;all&#39; in ans:
                print(&#39;Reset to before split&#39;)
                self._active.insert(target_clust, cluster)
            else:
                keepers = [new_clusts[int(i)] for i in ans]
                self._active.extend(keepers)

            return True

    def set_split(self, choices):
        if self._split_starter is None:
            raise ValueError(&#39;Not split stored.&#39;)

        if len(choices) == 0:
            self._active.insert(self._split_index, self._split_starter)
        else:
            keepers = [self._split_results[i] for i in choices]
            self._active.extend(keepers)

        self._split_index = None
        self._split_results = None
        self._split_starter = None

    def merge_clusters(self, target_clusters):
        if any([i &gt;= len(self._active) for i in target_clusters]):
            raise ValueError(&#39;Target cluster is out of range.&#39;)

        new_clust = []
        for c in target_clusters:
            if len(new_clust) == 0:
                new_clust = deepcopy(self._active[c])
                continue

            clust = self._active[c]
            sm1 = new_clust[&#39;spike_map&#39;]
            sm2 = clust[&#39;spike_map&#39;]
            st1 = new_clust[&#39;spike_times&#39;]
            st2 = clust[&#39;spike_times&#39;]
            sw1 = new_clust[&#39;spike_waveforms&#39;]
            sw2 = clust[&#39;spike_waveforms&#39;]

            spike_map = np.hstack((sm1, sm2))
            spike_times = np.hstack((st1, st2))
            spike_waveforms = np.vstack((sw1, sw2))

            # Re-order to spike_map
            idx = np.argsort(spike_map)
            spike_map = spike_map[idx]
            spike_times = spike_times[idx]
            spike_waveforms = spike_waveforms[idx]

            # Re-order so spike_times within a reocrding are in order
            times = []
            waves = []
            new_map = []
            for i in np.unique(spike_map):
                idx = np.where(spike_map == i)[0]
                st = spike_times[idx]
                sw = spike_waveforms[idx]
                sm = spike_map[idx]
                idx2 = np.argsort(st)
                st = st[idx2]
                sw = sw[idx2]
                sm = sm[idx2]
                times.append(st)
                waves.append(sw)
                new_map.append(sm)

            times = np.hstack(times)
            waves = np.vstack(waves)
            spike_map = np.hstack(new_map)
            del new_map, spike_times, spike_waveforms

            new_clust[&#39;spike_map&#39;] = spike_map
            new_clust[&#39;spike_times&#39;] = times
            new_clust[&#39;spike_waveforms&#39;] = waves
            new_clust[&#39;manipulations&#39;] += &#39;\nMerged with %s.&#39; % clust[&#39;Cluster Name&#39;]
            new_clust[&#39;Cluster Name&#39;] += &#39;+&#39; + clust[&#39;Cluster Name&#39;].replace(&#39;Cluster_&#39;,&#39;&#39;)

        self._active = [self._active[i] for i in range(len(self._active))
                        if i not in target_clusters]

        self._active.append(new_clust)

    def discard_clusters(self, target_clusters):
        self._active = [self._active[i] for i in range(len(self._active))
                        if i not in target_clusters]

    def plot_clusters_waveforms(self, target_clusters):
        if len(target_clusters) == 0:
            return

        for i in target_clusters:
            c = self._active[i]
            isi, v1, v2 = get_ISI_and_violations(c[&#39;spike_times&#39;], c[&#39;fs&#39;], c[&#39;spike_map&#39;])
            title = (&#39;Index : %i\n1ms violations: %0.1f, 2ms violations: %0.1f&#39;
                     &#39;\ntotal waveforms: %i&#39;
                     % (i, v1, v2, len(c[&#39;spike_waveforms&#39;])))
            fig, ax = dplt.plot_waveforms(c[&#39;spike_waveforms&#39;], title=title)
            fig.show()

    def plot_clusters_pca(self, target_clusters):
        if len(target_clusters) == 0:
            return

        waves = [self._active[i][&#39;spike_waveforms&#39;] for i in target_clusters]
        fig = dplt.plot_waveforms_pca(waves, cluster_ids=target_clusters)
        fig.show()

    def plot_clusters_umap(self, target_clusters):
        if len(target_clusters) == 0:
            return

        waves = [self._active[i][&#39;spike_waveforms&#39;] for i in target_clusters]
        fig = dplt.plot_waveforms_umap(waves, cluster_ids=target_clusters)
        fig.show()

    def plot_clusters_raster(self, target_clusters):
        if len(target_clusters) == 0:
            return

        clusters = [self._active[i] for i in target_clusters]
        spike_times = []
        spike_waves = []
        vlines = {}
        for c in clusters:
            # Adjust spike times by offset so recordings are not overlapping
            sm = c[&#39;spike_map&#39;]
            st = c[&#39;spike_times&#39;].copy().astype(&#39;float64&#39;)
            for i in np.unique(sm):
                idx = np.where(sm==i)[0]
                st[idx] += c[&#39;offsets&#39;][i]
                st[idx] = st[idx]/c[&#39;fs&#39;][i]  # convert to seconds
                if vlines.get(i) is None and c[&#39;offsets&#39;][i] != 0:
                    vlines[i] = c[&#39;offsets&#39;][i]/c[&#39;fs&#39;][i]


            spike_times.append(st)
            spike_waves.append(c[&#39;spike_waveforms&#39;])

        fig, ax = dplt.plot_spike_raster(spike_times, spike_waves, target_clusters)
        ax.set_xlabel(&#39;Time (s)&#39;)
        for x in vlines.values():
            ax.axvline(x, color=&#39;black&#39;, linewidth=2)

        fig.show()

    def plot_clusters_ISI(self, target_clusters):
        if len(target_clusters) == 0:
            return

        figs = []
        for i in target_clusters:
            cluster = self._active[i]
            isi, v1, v2 = get_ISI_and_violations(cluster[&#39;spike_times&#39;],
                                                 cluster[&#39;fs&#39;],
                                                 cluster[&#39;spike_map&#39;])
            fig, ax = dplt.plot_ISIs(isi, total_spikes=len(cluster[&#39;spike_times&#39;]))
            title= ax.get_title()
            title = &#39;Index: %i\n%s&#39; % (i, title)
            ax.set_title(title)
            fig.show()

    def get_mean_waveform(self, target_cluster):
        &#39;&#39;&#39;Returns mean waveform of target_cluster in active clusters. Also
        returns St. Dev. of waveforms
        &#39;&#39;&#39;
        cluster = self._active[target_cluster]
        mean_wave = np.mean(cluster[&#39;spike_waveforms&#39;], axis=0)
        std_wave = np.std(cluster[&#39;spike_waveforms&#39;], axis=0)
        return mean_wave, std_wave

    def get_possible_solutions(self):
        results = self.clustering.results.dropna()
        converged = list(results[results[&#39;converged&#39;]].index)
        return converged</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="blechpy.analysis.blech_clustering.compute_waveform_metrics"><code class="name flex">
<span>def <span class="ident">compute_waveform_metrics</span></span>(<span>waves, n_pc=3, umap=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Make clustering data array with columns:
- amplitudes, energy, slope, pc1, pc2, pc3, etc
Parameters</p>
<hr>
<dl>
<dt><strong><code>waves</code></strong> :&ensp;<code>np.array</code></dt>
<dd>waveforms with a row for each spike waveform</dd>
<dt><strong><code>n_pc</code></strong> :&ensp;<code>int</code> (optional)</dt>
<dd>number of principal components to include in data array</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.array</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_waveform_metrics(waves, n_pc=3, umap=False):
    &#39;&#39;&#39;Make clustering data array with columns:
         - amplitudes, energy, slope, pc1, pc2, pc3, etc
    Parameters
    ----------
    waves : np.array
        waveforms with a row for each spike waveform
    n_pc : int (optional)
        number of principal components to include in data array

    Returns
    -------
    np.array
    &#39;&#39;&#39;
    data = np.zeros((waves.shape[0], 3))
    for i, wave in enumerate(waves):
        data[i,0] = np.min(wave)
        data[i,1] = np.sqrt(np.sum(wave**2))/len(wave)
        peaks = find_peaks(wave)[0]
        minima = np.argmin(wave)
        if not any(peaks &lt; minima):
            maxima = np.argmax(wave[:minima])
        else:
            maxima = max(peaks[np.where(peaks &lt; minima)[0]])

        data[i,2] = (wave[minima]-wave[maxima])/(minima-maxima)

    # Scale waveforms to energy before running PCA
    if umap:
        pc_waves = implement_umap(waves, n_pc=n_pc)
    else:
        scaled_waves = scale_waveforms(waves, energy=data[:,1])
        pc_waves, _ = implement_pca(scaled_waves)

    data = np.hstack((data, pc_waves[:,:n_pc]))
    data_columns = [&#39;amplitude&#39;, &#39;energy&#39;, &#39;spike_slope&#39;]
    data_columns.extend([&#39;PC%i&#39; % i for i in range(n_pc)])
    return data, data_columns</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.blech_clustering.detect_spikes"><code class="name flex">
<span>def <span class="ident">detect_spikes</span></span>(<span>filt_el, spike_snapshot=[0.5, 1.0], fs=30000.0)</span>
</code></dt>
<dd>
<section class="desc"><p>Detects spikes in the filtered electrode trace and return the waveforms
and spike_times</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>filt_el</code></strong> :&ensp;<code>np.array</code>, <code>1</code>-<code>D</code></dt>
<dd>filtered electrode trace</dd>
<dt><strong><code>spike_snapshot</code></strong> :&ensp;<code>list</code></dt>
<dd>2-elements, [ms before spike minimum, ms after spike minimum]
time around spike to snap as waveform</dd>
<dt><strong><code>fs</code></strong> :&ensp;<code>float</code>, <code>sampling</code> <code>rate</code> <code>in</code> <code>Hz</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>waves</code></strong> :&ensp;<code>np.array</code></dt>
<dd>matrix of de-jittered, spike waveforms, upsampled by 10x, row for each spike</dd>
<dt><strong><code>times</code></strong> :&ensp;<code>np.array</code></dt>
<dd>array of spike times in samples</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def detect_spikes(filt_el, spike_snapshot = [0.5, 1.0], fs = 30000.0):
    &#39;&#39;&#39;Detects spikes in the filtered electrode trace and return the waveforms
    and spike_times

    Parameters
    ----------
    filt_el : np.array, 1-D
        filtered electrode trace
    spike_snapshot : list
        2-elements, [ms before spike minimum, ms after spike minimum] 
        time around spike to snap as waveform
    fs : float, sampling rate in Hz

    Returns
    -------
    waves : np.array
        matrix of de-jittered, spike waveforms, upsampled by 10x, row for each spike
    times : np.array
        array of spike times in samples
    &#39;&#39;&#39;
    # get indices of spike snapshot, expand by .1 ms in each direction
    snapshot = np.arange(-(spike_snapshot[0]+0.1)*fs/1000,
                         1+(spike_snapshot[1]+0.1)*fs/1000).astype(&#39;int64&#39;)
    m = np.mean(filt_el)
    th = 5.0*np.median(np.abs(filt_el)/0.6745)
    pos = np.where(filt_el &lt;= m-th)[0]
    consecutive = mt.group_consecutives(pos)

    waves = []
    times = []
    for idx in consecutive:
        minimum = idx[np.argmin(filt_el[idx])]
        spike_idx = minimum + snapshot
        if spike_idx[0] &gt;= 0 and spike_idx[-1] &lt; len(filt_el):
            waves.append(filt_el[spike_idx])
            times.append(minimum)

    if len(waves) == 0:
        return None, None

    waves_dj, times_dj = clustering.dejitter(np.array(waves), np.array(times), spike_snapshot, fs)
    return waves_dj, times_dj</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.blech_clustering.get_ISI_and_violations"><code class="name flex">
<span>def <span class="ident">get_ISI_and_violations</span></span>(<span>spike_times, fs, rec_map=None)</span>
</code></dt>
<dd>
<section class="desc"><p>returns array of ISIs in ms and # of 1ms and 2ms violations</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt>spike_time
numpy.array</dt>
<dt><strong><code>fs</code></strong> :&ensp;<code>float</code>, <code>sampling</code> <code>rate</code> <code>in</code> <code>Hz</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>rec_map</code></strong> :&ensp;<code>np.array</code> (optional)</dt>
<dd>if not passed, it is assumed all spike times are from same recording
if passed, spike times are split into recordings and ISIs are computed
per recording.
If fs is different for each recording, fs should be a dict with keys as
rec ids in rec_map</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.array</code> : <code>ISIs</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>int</code></strong> :&ensp;<code>1ms</code> <code>violations</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>int</code></strong> :&ensp;<code>2ms</code> <code>violations</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_ISI_and_violations(spike_times, fs, rec_map=None):
    &#39;&#39;&#39;returns array of ISIs in ms and # of 1ms and 2ms violations

    Parameters
    ----------
    spike_time  numpy.array
    fs : float, sampling rate in Hz
    rec_map : np.array (optional)
        if not passed, it is assumed all spike times are from same recording
        if passed, spike times are split into recordings and ISIs are computed
        per recording.
        If fs is different for each recording, fs should be a dict with keys as
        rec ids in rec_map

    Returns
    -------
    np.array : ISIs
    int : 1ms violations
    int : 2ms violations
    &#39;&#39;&#39;
    if rec_map is not None:
        if not isinstance(fs, dict):
            fs = dict.fromkeys(np.unique(rec_map), fs)

        ISIs = np.array([])
        violations1 = 0
        violations2 = 0
        for i in np.unique(rec_map):
            idx = np.where(rec_map == i)[0]
            tmp_isi, v1, v2 = get_ISI_and_violations(spike_times[idx], fs[i])
            violations1 += v1
            violations2 += v2
            ISIs = np.concatenate((ISIs, tmp_isi))

    else:
        fs = float(fs/1000.0)
        ISIs = np.ediff1d(np.sort(spike_times))/fs
        violations1 = np.sum(ISIs &lt; 1.0)
        violations2 = np.sum(ISIs &lt; 2.0)

    return ISIs, violations1, violations2</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.blech_clustering.get_mahalanobis_distances_to_cluster"><code class="name flex">
<span>def <span class="ident">get_mahalanobis_distances_to_cluster</span></span>(<span>data, model, clusters, target_cluster)</span>
</code></dt>
<dd>
<section class="desc"><p>computes mahalanobis distance from spikes in target_cluster to all clusters
in GMM model</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>np.array</code>, <code>data</code> <code>used</code> <code>to</code> <code>train</code> <code>GMM</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>model</code></strong> :&ensp;<code>fitted</code> <code>GMM</code> <code>model</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>clusters</code></strong> :&ensp;<code>np.array</code>, <code>maps</code> <code>data</code> <code>points</code> <code>to</code> <code>clusters</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>target_cluster</code></strong> :&ensp;<code>int</code>, <code>cluster</code> <code>for</code> <code>which</code> <code>to</code> <code>compute</code> <code>distances</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.array</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_mahalanobis_distances_to_cluster(data, model, clusters, target_cluster):
    &#39;&#39;&#39;computes mahalanobis distance from spikes in target_cluster to all clusters
    in GMM model

    Parameters
    ----------
    data : np.array, data used to train GMM
    model : fitted GMM model
    clusters : np.array, maps data points to clusters
    target_cluster : int, cluster for which to compute distances

    Returns
    -------
    np.array
    &#39;&#39;&#39;
    unique_clusters = np.unique(abs(clusters))
    out_distances = dict.fromkeys(unique_clusters)
    cluster_idx = np.where(clusters == target_cluster)[0]
    for other_cluster in unique_clusters:
        mahalanobis_dist = np.zeros((len(cluster_idx),))
        other_cluster_mean = model.means_[other_cluster, :]
        other_cluster_covar_I = linalg.inv(model.covariances_[other_cluster, :, :])
        for i, idx in enumerate(cluster_idx):
            mahalanobis_dist[i] = mahalanobis(data[idx, :],
                                              other_cluster_mean,
                                              other_cluster_covar_I)

        out_distances[other_cluster] = mahalanobis_dist

    return out_distances</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.blech_clustering.get_recording_cutoff"><code class="name flex">
<span>def <span class="ident">get_recording_cutoff</span></span>(<span>filt_el, sampling_rate, voltage_cutoff, max_breach_rate, max_secs_above_cutoff, max_mean_breach_rate_persec, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_recording_cutoff(filt_el, sampling_rate, voltage_cutoff,
                         max_breach_rate, max_secs_above_cutoff,
                         max_mean_breach_rate_persec, **kwargs):
    breach_idx = np.where(filt_el &gt; voltage_cutoff)[0]
    breach_rate = float(len(breach_idx)*int(sampling_rate))/len(filt_el)
    # truncate to nearest second and make 1 sec bins
    filt_el = filt_el[:int(sampling_rate)*int(len(filt_el)/sampling_rate)]
    test_el = np.reshape(filt_el, (-1, int(sampling_rate)))
    breaches_per_sec = [len(np.where(test_el[i] &gt; voltage_cutoff)[0])
                        for i in range(len(test_el))]
    breaches_per_sec = np.array(breaches_per_sec)
    secs_above_cutoff = len(np.where(breaches_per_sec &gt; 0)[0])
    if secs_above_cutoff == 0:
        mean_breach_rate_persec = 0
    else:
        mean_breach_rate_persec = np.mean(breaches_per_sec[np.where(breaches_per_sec &gt; 0)[0]])

    # And if they all exceed the cutoffs, assume that the headstage fell off mid-experiment
    recording_cutoff = int(len(filt_el)/sampling_rate) # cutoff in seconds
    if (breach_rate &gt;= max_breach_rate and
        secs_above_cutoff &gt;= max_secs_above_cutoff and
        mean_breach_rate_persec &gt;= max_mean_breach_rate_persec):
        # Find the first 1 second epoch where the number of cutoff breaches is
        # higher than the maximum allowed mean breach rate 
        recording_cutoff = np.where(breaches_per_sec &gt; max_mean_breach_rate_persec)[0][0]
        # cutoff is still in seconds since 1 sec bins

    return recording_cutoff</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.blech_clustering.get_spike_slopes"><code class="name flex">
<span>def <span class="ident">get_spike_slopes</span></span>(<span>waves)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns array of spike slopes (initial downward slope of spike)</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>waves</code></strong> :&ensp;<code>np.array</code>, <code>matrix</code> of <code>waveforms</code>, <code>with</code> <code>row</code> <code>for</code> <code>each</code> <code>spike</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.array</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_spike_slopes(waves):
    &#39;&#39;&#39;Returns array of spike slopes (initial downward slope of spike)

    Parameters
    ----------
    waves : np.array, matrix of waveforms, with row for each spike

    Returns
    -------
    np.array
    &#39;&#39;&#39;
    slopes = np.zeros((waves.shape[0],))
    for i, wave in enumerate(waves):
        peaks = find_peaks(wave)[0]
        minima = np.argmin(wave)
        if not any(peaks &lt; minima):
            maxima = np.argmax(wave[:minima])
        else:
            maxima = max(peaks[np.where(peaks &lt; minima)[0]])

        slopes[i] = (wave[minima]-wave[maxima])/(minima-maxima)

    return slopes</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.blech_clustering.get_waveform_amplitudes"><code class="name flex">
<span>def <span class="ident">get_waveform_amplitudes</span></span>(<span>waves)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns array of waveform amplitudes</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>waves</code></strong> :&ensp;<code>np.array</code>, <code>matrix</code> of <code>waveforms</code>, <code>with</code> <code>row</code> <code>for</code> <code>each</code> <code>spike</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.array</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_waveform_amplitudes(waves):
    &#39;&#39;&#39;Returns array of waveform amplitudes

    Parameters
    ----------
    waves : np.array, matrix of waveforms, with row for each spike

    Returns
    -------
    np.array
    &#39;&#39;&#39;
    return np.min(waves,axis = 1)</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.blech_clustering.get_waveform_energy"><code class="name flex">
<span>def <span class="ident">get_waveform_energy</span></span>(<span>waves)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns array of waveform energies</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>waves</code></strong> :&ensp;<code>np.array</code>, <code>matrix</code> of <code>waveforms</code>, <code>with</code> <code>row</code> <code>for</code> <code>each</code> <code>spike</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.array</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_waveform_energy(waves):
    &#39;&#39;&#39;Returns array of waveform energies

    Parameters
    ----------
    waves : np.array, matrix of waveforms, with row for each spike

    Returns
    -------
    np.array
    &#39;&#39;&#39;
    energy = np.sqrt(np.sum(waves**2, axis=1))/waves.shape[1]
    return energy</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.blech_clustering.implement_pca"><code class="name flex">
<span>def <span class="ident">implement_pca</span></span>(<span>scaled_slices)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def implement_pca(scaled_slices):
    pca = PCA()
    pca_slices = pca.fit_transform(scaled_slices)
    return pca_slices, pca.explained_variance_ratio_</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.blech_clustering.implement_umap"><code class="name flex">
<span>def <span class="ident">implement_umap</span></span>(<span>waves, n_pc=3, n_neighbors=10, min_dist=0.0)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def implement_umap(waves, n_pc=3, n_neighbors=10, min_dist=0.0):
    reducer = umap.UMAP(n_components=n_pc,
                        n_neighbors=n_neighbors,
                        min_dist=min_dist)
    return reducer.fit_transform(waves)</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.blech_clustering.scale_waveforms"><code class="name flex">
<span>def <span class="ident">scale_waveforms</span></span>(<span>waves, energy=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Scales each waveform to its own energy</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>waves</code></strong> :&ensp;<code>np.array</code>, <code>matrix</code> of <code>waveforms</code>, <code>with</code> <code>row</code> <code>for</code> <code>each</code> <code>spike</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>energy</code></strong> :&ensp;<code>np.array</code> (optional)</dt>
<dd>array of waveform energies, saves computation time</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.array</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def scale_waveforms(waves, energy=None):
    &#39;&#39;&#39;Scales each waveform to its own energy

    Parameters
    ----------
    waves : np.array, matrix of waveforms, with row for each spike
    energy : np.array (optional)
        array of waveform energies, saves computation time

    Returns
    -------
    np.array
    &#39;&#39;&#39;
    if energy is None:
        energy = get_waveform_energy(waves)
    elif len(energy) != waves.shape[0]:
        raise ValueError((&#39;Energies must correspond to each waveforms.&#39;
                          &#39;Different lengths are not allowed&#39;))

    scaled_slices = np.zeros(waves.shape)
    for i, w in enumerate(zip(waves, energy)):
        scaled_slices[i] = w[0]/w[1]

    return scaled_slices</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="blechpy.analysis.blech_clustering.BlechClust"><code class="flex name class">
<span>class <span class="ident">BlechClust</span></span>
<span>(</span><span>rec_dirs, electrode, out_dir=None, params=None, overwrite=False, no_write=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Recording directories should be ordered to make spike sorting easier later on</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BlechClust(object):
    def __init__(self, rec_dirs, electrode, out_dir=None, params=None,
                 overwrite=False, no_write=False):
        &#39;&#39;&#39;Recording directories should be ordered to make spike sorting easier later on
        &#39;&#39;&#39;
        if isinstance(rec_dirs, str):
            rec_dirs = [rec_dirs]

        rec_dirs = [x[:-1] if x.endswith(os.sep) else x for x in rec_dirs]
        self.rec_dirs = rec_dirs
        self.electrode = electrode
        if out_dir is None:
            if len(rec_dirs) &gt; 1:
                top = os.path.dirname(rec_dirs[0])
                out_dir = os.path.join(top, &#39;BlechClust&#39;, &#39;electrode_%i&#39; % electrode)
            else:
                out_dir = os.path.join(rec_dirs[0], &#39;BlechClust&#39;, &#39;electrode_%i&#39; % electrode)

        if overwrite:
            shutil.rmtree(out_dir)

        # Make directories
        self.out_dir = out_dir
        self._plot_dir = os.path.join(out_dir, &#39;plots&#39;)
        self._data_dir = os.path.join(out_dir, &#39;clustering_results&#39;)
        if not os.path.isdir(out_dir):
            os.makedirs(out_dir)

        if not os.path.isdir(self._data_dir):
            os.mkdir(self._data_dir)

        if not os.path.isdir(self._plot_dir):
            os.mkdir(self._plot_dir)

        # Check files
        params_file = os.path.join(out_dir, &#39;BlechClust_params.json&#39;)
        map_file = os.path.join(self._data_dir, &#39;spike_id.npy&#39;)
        key_file = os.path.join(self._data_dir, &#39;rec_key.json&#39;)
        results_file = os.path.join(self._data_dir, &#39;clustering_results.json&#39;)
        self._files = {&#39;params&#39;: params_file, &#39;spike_map&#39;: map_file,
                       &#39;rec_key&#39;: key_file, &#39;clustering_results&#39;: results_file}
        self.params = params
        self._load_existsing_data()

        if self._rec_key is None and not no_write:
            # Create new rec key
            rec_key = {x:y for x,y in enumerate(self.rec_dirs)}
            self._rec_key = rec_key
            wt.write_dict_to_json(rec_key, self._files[&#39;rec_key&#39;])
        elif self._rec_key is None:
            ValueError(&#39;Existing rec_key not found and no_write is enabled&#39;)

        # Check to see if spike detection is already completed on all recording directories
        spike_check = self._check_spike_detection()
        if not all(spike_check):
            invalid = [rec_dirs[i] for i, x in enumerate(spike_check) if x==False]
            error_str = &#39;\n\t&#39;.join(invalid)
            raise ValueError(&#39;Spike detection has not been run on:\n\t%s&#39; % error_str)

    def _load_existsing_data(self):
        params = self.params
        file_check = self._check_existing_files()

        # Check params files and create if new params are passed
        if file_check[&#39;params&#39;]:
            self.params = wt.read_dict_from_json(self._files[&#39;params&#39;])

        # Make new params or overwrite existing with passed params
        if params is None and not file_check[&#39;params&#39;]:
            raise ValueError((&#39;Params file does not exists at %s. Must provide&#39;
                              &#39; clustering parameters.&#39;) % self._files[&#39;params&#39;])
        elif params is not None:
            self.params[&#39;max_clusters&#39;] = params[&#39;clustering_params&#39;][&#39;Max Number of Clusters&#39;]
            self.params[&#39;max_iterations&#39;] = params[&#39;clustering_params&#39;][&#39;Max Number of Iterations&#39;]
            self.params[&#39;threshold&#39;] = params[&#39;clustering_params&#39;][&#39;Convergence Criterion&#39;]
            self.params[&#39;num_restarts&#39;] = params[&#39;clustering_params&#39;][&#39;GMM random restarts&#39;]
            self.params[&#39;wf_amplitude_sd_cutoff&#39;] = params[&#39;data_params&#39;][&#39;Intra-cluster waveform amp SD cutoff&#39;]
            wt.write_dict_to_json(self.params, self._files[&#39;params&#39;])

        # Deal with existing rec key
        if file_check[&#39;rec_key&#39;]:
            rec_key = wt.read_dict_from_json(self._files[&#39;rec_key&#39;])
            rec_key = {int(x): y for x,y in rec_key.items()}
            inverted = {v:k for k,v in rec_key.items()}
            self.rec_dirs = sorted(self.rec_dirs, key=lambda i: inverted[i])
            self._rec_key = rec_key
        else:
            self._rec_key = None

        # Check is clustering has already been done, load results
        if file_check[&#39;clustering_results&#39;]:
            self.results = wt.read_pandas_from_table(self._files[&#39;clustering_results&#39;])
            self.clustered = True
        else:
            self.results = None
            self.clustered = False

    def _check_existing_files(self):
        out = dict.fromkeys(self._files.keys(), False)
        for k,v in self._files.items():
            if os.path.isfile(v):
                out[k] = True

        return out

    def _check_spike_detection(self):
        &#39;&#39;&#39;Check to see if spike detection has been run on all recording directories
        &#39;&#39;&#39;
        out = []
        for rec in  self.rec_dirs:
            try:
                spike_detect = SpikeDetection(rec, self.electrode)
                if all(spike_detect._status):
                    out.append(True)
                else:
                    out.append(False)

            except FileNotFoundError:
                out.append(False)

        return out

    def run(self, n_pc=3, overwrite=False):
        if self.clustered and not overwrite:
            return True

        GMM = ClusterGMM(self.params[&#39;max_iterations&#39;],
                         self.params[&#39;num_restarts&#39;], self.params[&#39;threshold&#39;])

        # Collect data from all recordings
        waveforms, spike_times, spike_map, fs, offsets = self.get_spike_data()

        # Save array to map spikes and predictions back to original recordings
        np.save(self._files[&#39;spike_map&#39;], spike_map)

        data, data_columns = compute_waveform_metrics(waveforms, n_pc)
        amplitudes = get_waveform_amplitudes(waveforms)

        # Run GMM for each number of clusters from 2 to max_clusters
        tested_clusters = np.arange(2, self.params[&#39;max_clusters&#39;]+1)
        clust_results = pd.DataFrame(columns=[&#39;clusters&#39;,&#39;converged&#39;,
                                              &#39;BIC&#39;,&#39;spikes_per_cluster&#39;],
                                     index=tested_clusters)
        for n_clust in tested_clusters:
            data_dir = os.path.join(self._data_dir, &#39;%i_clusters&#39; % n_clust)
            plot_dir = os.path.join(self._plot_dir, &#39;%i_clusters&#39; % n_clust)
            wave_plot_dir = os.path.join(self._plot_dir, &#39;%i_clusters_waveforms_ISIs&#39; % n_clust)
            bic_file = os.path.join(data_dir, &#39;bic.npy&#39;)
            pred_file = os.path.join(data_dir, &#39;predictions.npy&#39;)

            if os.path.isfile(bic_file) and os.path.isfile(pred_file):
                bic = np.load(bic_file)
                predictions = np.load(pred_file)
                spikes_per_clust = [len(np.where(predictions == c)[0])
                                    for c in np.unique(predictions)]
                clust_results.loc[n_clust] = [n_clust, True, bic, spikes_per_clust]
                continue

            if not os.path.isdir(wave_plot_dir):
                os.makedirs(wave_plot_dir)

            if not os.path.isdir(data_dir):
                os.makedirs(data_dir)

            if not os.path.isdir(plot_dir):
                os.makedirs(plot_dir)

            model, predictions, bic = GMM.fit(data, n_clust)
            if model is None:
                clust_results.loc[n_clust] = [n_clust, bic, False, [0]]
                # Nothing converged
                continue

            # Go through each cluster and throw out any spikes too far from the
            # mean
            spikes_per_clust = []
            for c in range(n_clust):
                idx = np.where(predictions == c)[0]
                mean_amp = np.mean(amplitudes[idx])
                sd_amp = np.std(amplitudes[idx])
                cutoff_amp = mean_amp - (sd_amp * self.params[&#39;wf_amplitude_sd_cutoff&#39;])
                rejected_idx = np.array([i for i in idx if amplitudes[i] &lt;= cutoff_amp])
                if len(rejected_idx) &gt; 0:
                    predictions[rejected_idx] = -1

                idx = np.where(predictions == c)[0]
                spikes_per_clust.append(len(idx))

                if len(idx) == 0:
                    continue

                # Plot waveforms and ISIs of cluster
                ISIs, violations_1ms, violations_2ms = get_ISI_and_violations(spike_times[idx], fs, spike_map[idx])
                cluster_waves = waveforms[idx]
                cluster_times = spike_times[idx]
                isi_fn = os.path.join(wave_plot_dir, &#39;Cluster%i_ISI.png&#39; % c)
                wave_fn = os.path.join(wave_plot_dir, &#39;Cluster%i_waveforms.png&#39; % c)
                title_str = (&#39;Cluster%i\nviolations_1ms = %i, &#39;
                             &#39;violations_2ms = %i\n&#39;
                             &#39;Number of waveforms = %i&#39; %
                             (c, violations_1ms, violations_2ms, len(idx)))
                dplt.plot_waveforms(cluster_waves, title=title_str, save_file=wave_fn)
                dplt.plot_ISIs(ISIs, total_spikes=len(idx), save_file=isi_fn)


            clust_results.loc[n_clust] = [n_clust, True, bic, spikes_per_clust]

            # Plot feature pairs
            feature_pairs = it.combinations(list(range(data.shape[1])), 2)
            for f1, f2 in feature_pairs:
                fn = &#39;%sVS%s.png&#39; % (data_columns[f1], data_columns[f2])
                fn = os.path.join(plot_dir, fn)
                dplt.plot_cluster_features(data[:, [f1,f2]], predictions,
                                           x_label = data_columns[f1],
                                           y_label = data_columns[f2],
                                           save_file = fn)

            # For each cluster plot mahanalobis distances to all other clusters
            for c in range(n_clust):
                distances = get_mahalanobis_distances_to_cluster(data,  model,
                                                                 predictions, c)
                fn = os.path.join(plot_dir, &#39;Mahalanobis_cluster%i.png&#39; % c)
                title = (&#39;Mahalanobis distance of Cluster %i from all other clusters&#39; % c)
                dplt.plot_mahalanobis_to_cluster(distances, title=title, save_file=fn)

            # Save data
            np.save(bic_file, bic)
            np.save(pred_file, predictions)

        # Save results table
        self.results = clust_results
        wt.write_pandas_to_table(clust_results, self._files[&#39;clustering_results&#39;])
        self.clustered = True
        return True

    def get_spike_data(self):
        # Collect data from all recordings
        tmp_waves = []
        tmp_times = []
        tmp_id = []
        fs = dict.fromkeys(self._rec_key.keys())
        offsets = dict.fromkeys(self._rec_key.keys())
        offset = 0
        for i in sorted(self._rec_key.keys()):
            rec = self._rec_key[i]
            spike_detect = SpikeDetection(rec, self.electrode)
            t = spike_detect.get_spike_times()
            fs[i] = spike_detect.params[&#39;sampling_rate&#39;]
            if t is None:
                offsets[i] = int(offset)
                offset = offset + 3*fs[i]
                continue

            tmp_waves.append(spike_detect.get_spike_waveforms())
            tmp_times.append(t)
            tmp_id.append(np.ones((t.shape[0],))*i)
            offsets[i] = int(offset)
            offset = offset + max(t) + 3*fs[i]

        waveforms = np.vstack(tmp_waves)
        spike_times = np.hstack(tmp_times)
        spike_map = np.hstack(tmp_id)

        # Double check that spike_map matches up with existing spike_map
        if os.path.isfile(self._files[&#39;spike_map&#39;]):
            orig_map = np.load(self._files[&#39;spike_map&#39;])
            if len(orig_map) != len(spike_map):
                raise ValueError(&#39;Spike detection has changed, please re-cluster with overwrite=True&#39;)

        return waveforms, spike_times, spike_map, fs, offsets

    def get_clusters(self, solution_num, cluster_nums):
        if not isinstance(cluster_nums, list):
            cluster_nums = [cluster_nums]

        waveforms, times, spike_map, fs, offsets = self.get_spike_data()
        predictions = self.get_predictions(solution_num)
        out = []
        for c in cluster_nums:
            idx = np.where(predictions == c)[0]
            if len(idx)==0:
                continue

            tmp_clust = {&#39;Cluster Name&#39;: &#39;Cluster_%i&#39; % c,
                         &#39;solution_num&#39;: solution_num,
                         &#39;cluster_num&#39;: c,
                         &#39;cluster_id&#39;: 1,
                         &#39;spike_waveforms&#39;: waveforms[idx],
                         &#39;spike_times&#39;: times[idx],
                         &#39;spike_map&#39;: spike_map[idx],
                         &#39;rec_key&#39;: self._rec_key.copy(),
                         &#39;fs&#39;: fs,
                         &#39;offsets&#39;: offsets,
                         &#39;manipulations&#39;: &#39;&#39;}
            out.append(tmp_clust)

        return out

    def get_predictions(self, n_clusters):
        fn = os.path.join(self._data_dir, &#39;%i_clusters&#39; % n_clusters,
                          &#39;predictions.npy&#39;)
        if os.path.isfile(fn):
            return np.load(fn)
        else:
            return None</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="blechpy.analysis.blech_clustering.BlechClust.get_clusters"><code class="name flex">
<span>def <span class="ident">get_clusters</span></span>(<span>self, solution_num, cluster_nums)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_clusters(self, solution_num, cluster_nums):
    if not isinstance(cluster_nums, list):
        cluster_nums = [cluster_nums]

    waveforms, times, spike_map, fs, offsets = self.get_spike_data()
    predictions = self.get_predictions(solution_num)
    out = []
    for c in cluster_nums:
        idx = np.where(predictions == c)[0]
        if len(idx)==0:
            continue

        tmp_clust = {&#39;Cluster Name&#39;: &#39;Cluster_%i&#39; % c,
                     &#39;solution_num&#39;: solution_num,
                     &#39;cluster_num&#39;: c,
                     &#39;cluster_id&#39;: 1,
                     &#39;spike_waveforms&#39;: waveforms[idx],
                     &#39;spike_times&#39;: times[idx],
                     &#39;spike_map&#39;: spike_map[idx],
                     &#39;rec_key&#39;: self._rec_key.copy(),
                     &#39;fs&#39;: fs,
                     &#39;offsets&#39;: offsets,
                     &#39;manipulations&#39;: &#39;&#39;}
        out.append(tmp_clust)

    return out</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.blech_clustering.BlechClust.get_predictions"><code class="name flex">
<span>def <span class="ident">get_predictions</span></span>(<span>self, n_clusters)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_predictions(self, n_clusters):
    fn = os.path.join(self._data_dir, &#39;%i_clusters&#39; % n_clusters,
                      &#39;predictions.npy&#39;)
    if os.path.isfile(fn):
        return np.load(fn)
    else:
        return None</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.blech_clustering.BlechClust.get_spike_data"><code class="name flex">
<span>def <span class="ident">get_spike_data</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_spike_data(self):
    # Collect data from all recordings
    tmp_waves = []
    tmp_times = []
    tmp_id = []
    fs = dict.fromkeys(self._rec_key.keys())
    offsets = dict.fromkeys(self._rec_key.keys())
    offset = 0
    for i in sorted(self._rec_key.keys()):
        rec = self._rec_key[i]
        spike_detect = SpikeDetection(rec, self.electrode)
        t = spike_detect.get_spike_times()
        fs[i] = spike_detect.params[&#39;sampling_rate&#39;]
        if t is None:
            offsets[i] = int(offset)
            offset = offset + 3*fs[i]
            continue

        tmp_waves.append(spike_detect.get_spike_waveforms())
        tmp_times.append(t)
        tmp_id.append(np.ones((t.shape[0],))*i)
        offsets[i] = int(offset)
        offset = offset + max(t) + 3*fs[i]

    waveforms = np.vstack(tmp_waves)
    spike_times = np.hstack(tmp_times)
    spike_map = np.hstack(tmp_id)

    # Double check that spike_map matches up with existing spike_map
    if os.path.isfile(self._files[&#39;spike_map&#39;]):
        orig_map = np.load(self._files[&#39;spike_map&#39;])
        if len(orig_map) != len(spike_map):
            raise ValueError(&#39;Spike detection has changed, please re-cluster with overwrite=True&#39;)

    return waveforms, spike_times, spike_map, fs, offsets</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.blech_clustering.BlechClust.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>self, n_pc=3, overwrite=False)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run(self, n_pc=3, overwrite=False):
    if self.clustered and not overwrite:
        return True

    GMM = ClusterGMM(self.params[&#39;max_iterations&#39;],
                     self.params[&#39;num_restarts&#39;], self.params[&#39;threshold&#39;])

    # Collect data from all recordings
    waveforms, spike_times, spike_map, fs, offsets = self.get_spike_data()

    # Save array to map spikes and predictions back to original recordings
    np.save(self._files[&#39;spike_map&#39;], spike_map)

    data, data_columns = compute_waveform_metrics(waveforms, n_pc)
    amplitudes = get_waveform_amplitudes(waveforms)

    # Run GMM for each number of clusters from 2 to max_clusters
    tested_clusters = np.arange(2, self.params[&#39;max_clusters&#39;]+1)
    clust_results = pd.DataFrame(columns=[&#39;clusters&#39;,&#39;converged&#39;,
                                          &#39;BIC&#39;,&#39;spikes_per_cluster&#39;],
                                 index=tested_clusters)
    for n_clust in tested_clusters:
        data_dir = os.path.join(self._data_dir, &#39;%i_clusters&#39; % n_clust)
        plot_dir = os.path.join(self._plot_dir, &#39;%i_clusters&#39; % n_clust)
        wave_plot_dir = os.path.join(self._plot_dir, &#39;%i_clusters_waveforms_ISIs&#39; % n_clust)
        bic_file = os.path.join(data_dir, &#39;bic.npy&#39;)
        pred_file = os.path.join(data_dir, &#39;predictions.npy&#39;)

        if os.path.isfile(bic_file) and os.path.isfile(pred_file):
            bic = np.load(bic_file)
            predictions = np.load(pred_file)
            spikes_per_clust = [len(np.where(predictions == c)[0])
                                for c in np.unique(predictions)]
            clust_results.loc[n_clust] = [n_clust, True, bic, spikes_per_clust]
            continue

        if not os.path.isdir(wave_plot_dir):
            os.makedirs(wave_plot_dir)

        if not os.path.isdir(data_dir):
            os.makedirs(data_dir)

        if not os.path.isdir(plot_dir):
            os.makedirs(plot_dir)

        model, predictions, bic = GMM.fit(data, n_clust)
        if model is None:
            clust_results.loc[n_clust] = [n_clust, bic, False, [0]]
            # Nothing converged
            continue

        # Go through each cluster and throw out any spikes too far from the
        # mean
        spikes_per_clust = []
        for c in range(n_clust):
            idx = np.where(predictions == c)[0]
            mean_amp = np.mean(amplitudes[idx])
            sd_amp = np.std(amplitudes[idx])
            cutoff_amp = mean_amp - (sd_amp * self.params[&#39;wf_amplitude_sd_cutoff&#39;])
            rejected_idx = np.array([i for i in idx if amplitudes[i] &lt;= cutoff_amp])
            if len(rejected_idx) &gt; 0:
                predictions[rejected_idx] = -1

            idx = np.where(predictions == c)[0]
            spikes_per_clust.append(len(idx))

            if len(idx) == 0:
                continue

            # Plot waveforms and ISIs of cluster
            ISIs, violations_1ms, violations_2ms = get_ISI_and_violations(spike_times[idx], fs, spike_map[idx])
            cluster_waves = waveforms[idx]
            cluster_times = spike_times[idx]
            isi_fn = os.path.join(wave_plot_dir, &#39;Cluster%i_ISI.png&#39; % c)
            wave_fn = os.path.join(wave_plot_dir, &#39;Cluster%i_waveforms.png&#39; % c)
            title_str = (&#39;Cluster%i\nviolations_1ms = %i, &#39;
                         &#39;violations_2ms = %i\n&#39;
                         &#39;Number of waveforms = %i&#39; %
                         (c, violations_1ms, violations_2ms, len(idx)))
            dplt.plot_waveforms(cluster_waves, title=title_str, save_file=wave_fn)
            dplt.plot_ISIs(ISIs, total_spikes=len(idx), save_file=isi_fn)


        clust_results.loc[n_clust] = [n_clust, True, bic, spikes_per_clust]

        # Plot feature pairs
        feature_pairs = it.combinations(list(range(data.shape[1])), 2)
        for f1, f2 in feature_pairs:
            fn = &#39;%sVS%s.png&#39; % (data_columns[f1], data_columns[f2])
            fn = os.path.join(plot_dir, fn)
            dplt.plot_cluster_features(data[:, [f1,f2]], predictions,
                                       x_label = data_columns[f1],
                                       y_label = data_columns[f2],
                                       save_file = fn)

        # For each cluster plot mahanalobis distances to all other clusters
        for c in range(n_clust):
            distances = get_mahalanobis_distances_to_cluster(data,  model,
                                                             predictions, c)
            fn = os.path.join(plot_dir, &#39;Mahalanobis_cluster%i.png&#39; % c)
            title = (&#39;Mahalanobis distance of Cluster %i from all other clusters&#39; % c)
            dplt.plot_mahalanobis_to_cluster(distances, title=title, save_file=fn)

        # Save data
        np.save(bic_file, bic)
        np.save(pred_file, predictions)

    # Save results table
    self.results = clust_results
    wt.write_pandas_to_table(clust_results, self._files[&#39;clustering_results&#39;])
    self.clustered = True
    return True</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="blechpy.analysis.blech_clustering.ClusterGMM"><code class="flex name class">
<span>class <span class="ident">ClusterGMM</span></span>
<span>(</span><span>n_iters, n_restarts, thresh)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ClusterGMM(object):
    def __init__(self, n_iters, n_restarts, thresh):
        self.params = {&#39;iterations&#39;: n_iters,
                       &#39;restarts&#39;: n_restarts,
                       &#39;thresh&#39;: thresh}

    def fit(self, data, n_clusters):
        min_bic = None
        best_model = None
        if n_clusters is not None:
            self.params[&#39;clusters&#39;] = n_clusters

        for i in range(self.params[&#39;restarts&#39;]):
            model = GaussianMixture(n_components = self.params[&#39;clusters&#39;],
                                    covariance_type = &#39;full&#39;,
                                    tol = self.params[&#39;thresh&#39;],
                                    random_state = i,
                                    max_iter = self.params[&#39;iterations&#39;])
            model.fit(data)
            if model.converged_:
                new_bic = model.bic(data)
                if min_bic is None:
                    min_bic = model.bic(data)
                    best_model = model
                elif new_bic &lt; min_bic:
                    best_model = model
                    min_bic = new_bic

        predictions = best_model.predict(data)
        self._model = best_model
        self._predictions = predictions
        self._bic = min_bic
        return best_model, predictions, min_bic</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="blechpy.analysis.blech_clustering.ClusterGMM.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, data, n_clusters)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, data, n_clusters):
    min_bic = None
    best_model = None
    if n_clusters is not None:
        self.params[&#39;clusters&#39;] = n_clusters

    for i in range(self.params[&#39;restarts&#39;]):
        model = GaussianMixture(n_components = self.params[&#39;clusters&#39;],
                                covariance_type = &#39;full&#39;,
                                tol = self.params[&#39;thresh&#39;],
                                random_state = i,
                                max_iter = self.params[&#39;iterations&#39;])
        model.fit(data)
        if model.converged_:
            new_bic = model.bic(data)
            if min_bic is None:
                min_bic = model.bic(data)
                best_model = model
            elif new_bic &lt; min_bic:
                best_model = model
                min_bic = new_bic

    predictions = best_model.predict(data)
    self._model = best_model
    self._predictions = predictions
    self._bic = min_bic
    return best_model, predictions, min_bic</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="blechpy.analysis.blech_clustering.SpikeDetection"><code class="flex name class">
<span>class <span class="ident">SpikeDetection</span></span>
<span>(</span><span>file_dir, electrode, params=None, overwrite=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Interface to manage spike detection and data extraction in preparation
for GMM clustering. Intended to help create and access the neccessary
files. If object will detect is file already exist to avoid re-creation
unless overwrite is specified as True.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SpikeDetection(object):
    &#39;&#39;&#39;Interface to manage spike detection and data extraction in preparation
    for GMM clustering. Intended to help create and access the neccessary
    files. If object will detect is file already exist to avoid re-creation
    unless overwrite is specified as True.
    &#39;&#39;&#39;

    def __init__(self, file_dir, electrode, params=None, overwrite=False):
        # Setup paths to files and directories needed
        self._file_dir = file_dir
        self._electrode = electrode
        self._out_dir = os.path.join(file_dir, &#39;spike_detection&#39;,
                                     &#39;electrode_%i&#39; % electrode)
        self._data_dir = os.path.join(self._out_dir, &#39;data&#39;)
        self._plot_dir = os.path.join(self._out_dir, &#39;plots&#39;)
        self._files = {&#39;params&#39;: os.path.join(file_dir,&#39;analysis_params&#39;, &#39;spike_detection_params.json&#39;),
                       &#39;spike_waveforms&#39;: os.path.join(self._data_dir, &#39;spike_waveforms.npy&#39;),
                       &#39;spike_times&#39; : os.path.join(self._data_dir, &#39;spike_times.npy&#39;),
                       &#39;energy&#39; : os.path.join(self._data_dir, &#39;energy.npy&#39;),
                       &#39;spike_amplitudes&#39; : os.path.join(self._data_dir, &#39;spike_amplitudes.npy&#39;),
                       &#39;pca_waveforms&#39; : os.path.join(self._data_dir, &#39;pca_waveforms.npy&#39;),
                       &#39;slopes&#39; : os.path.join(self._data_dir, &#39;spike_slopes.npy&#39;),
                       &#39;recording_cutoff&#39; : os.path.join(self._data_dir, &#39;cutoff_time.txt&#39;)}

        self._status = dict.fromkeys(self._files.keys(), False)
        self._referenced = True

        # Delete existing data if overwrite is True
        if overwrite and os.path.isdir(self._out_dir):
            shutil.rmtree(self._out_dir)

        # See what data already exists
        self._check_existing_files()

        # Make directories if needed
        if not os.path.isdir(self._out_dir):
            os.makedirs(self._out_dir)

        if not os.path.isdir(self._data_dir):
            os.makedirs(self._data_dir)

        if not os.path.isdir(self._plot_dir):
            os.makedirs(self._plot_dir)

        if not os.path.isdir(os.path.join(file_dir, &#39;analysis_params&#39;)):
            os.makedirs(os.path.join(file_dir, &#39;analysis_params&#39;))

        # grab recording cutoff time if it already exists
        # cutoff should be in seconds
        self.recording_cutoff = None
        if os.path.isfile(self._files[&#39;recording_cutoff&#39;]):
            self._status[&#39;recording_cutoff&#39;] = True
            with open(self._files[&#39;recording_cutoff&#39;], &#39;r&#39;) as f:
                self.recording_cutoff = float(f.read())

        # Read in parameters
        # Parameters passed as an argument will overshadow parameters saved in file
        # Input parameters should be formatted as dataset.clustering_parameters
        if params is None and os.path.isfile(self._files[&#39;params&#39;]):
            self.params = wt.read_dict_from_json(self._files[&#39;params&#39;])
        elif params is None:
            raise FileNotFoundError(&#39;params must be provided if spike_detection_params.json does not exist.&#39;)
        else:
            self.params = {}
            self.params[&#39;voltage_cutoff&#39;] = params[&#39;data_params&#39;][&#39;V_cutoff for disconnected headstage&#39;]
            self.params[&#39;max_breach_rate&#39;] = params[&#39;data_params&#39;][&#39;Max rate of cutoff breach per second&#39;]
            self.params[&#39;max_secs_above_cutoff&#39;] = params[&#39;data_params&#39;][&#39;Max allowed seconds with a breach&#39;]
            self.params[&#39;max_mean_breach_rate_persec&#39;] = params[&#39;data_params&#39;][&#39;Max allowed breaches per second&#39;]
            band_lower = params[&#39;bandpass_params&#39;][&#39;Lower freq cutoff&#39;]
            band_upper = params[&#39;bandpass_params&#39;][&#39;Upper freq cutoff&#39;]
            self.params[&#39;bandpass&#39;] = [band_lower, band_upper]
            snapshot_pre = params[&#39;spike_snapshot&#39;][&#39;Time before spike (ms)&#39;]
            snapshot_post = params[&#39;spike_snapshot&#39;][&#39;Time after spike (ms)&#39;]
            self.params[&#39;spike_snapshot&#39;] = [snapshot_pre, snapshot_post]
            self.params[&#39;sampling_rate&#39;] = params[&#39;sampling_rate&#39;]
            # Write params to json file
            wt.write_dict_to_json(self.params, self._files[&#39;params&#39;])
            self._status[&#39;params&#39;] = True

    def _check_existing_files(self):
        &#39;&#39;&#39;Checks which files already exist and updates _status so as to avoid
        re-creation later
        &#39;&#39;&#39;
        for k, v in self._files.items():
            if os.path.isfile(v):
                self._status[k] = True
            else:
                self._status[k] = False

    def run(self):
        status = self._status
        file_dir = self._file_dir
        electrode = self._electrode
        params = self.params
        fs = params[&#39;sampling_rate&#39;]

        # Check if this even needs to be run
        if all(status.values()):
            return electrode, 1, self.recording_cutoff

        # Grab referenced electrode or raw if ref is not available
        ref_el = h5io.get_referenced_trace(file_dir, electrode)
        if ref_el is None:
            print(&#39;Could not find referenced data for electrode %i. Using raw.&#39; % electrode)
            self._referenced = False
            ref_el = h5io.get_raw_trace(file_dir, electrode)
            if ref_el is None:
                raise KeyError(&#39;Neither referenced nor raw data found for electrode %i in %s&#39; % (electrode, file_dir))

        # Filter electrode trace
        filt_el = clustering.get_filtered_electrode(ref_el, freq=params[&#39;bandpass&#39;],
                                               sampling_rate = fs)
        del ref_el
        # Get recording cutoff
        if not status[&#39;recording_cutoff&#39;]:
            self.recording_cutoff = get_recording_cutoff(filt_el, **params)
            with open(self._files[&#39;recording_cutoff&#39;], &#39;w&#39;) as f:
                f.write(str(self.recording_cutoff))

            status[&#39;recording_cutoff&#39;] = True
            fn = os.path.join(self._plot_dir, &#39;cutoff_time.png&#39;)
            dplt.plot_recording_cutoff(filt_el, fs, self.recording_cutoff,
                                       out_file=fn)

        # Truncate electrode trace, deal with early cutoff (&lt;60s)
        if self.recording_cutoff &lt; 60:
            print(&#39;Immediate Cutoff for electrode %i...exiting&#39; % electrode)
            return electrode, 0, self.recording_cutoff

        filt_el = filt_el[:int(self.recording_cutoff*fs)]

        if status[&#39;spike_waveforms&#39;] and status[&#39;spike_times&#39;]:
            waves = np.load(self._files[&#39;spike_waveforms&#39;])
            times = np.load(self._files[&#39;spike_times&#39;])
        else:
            # Detect spikes and get dejittered times and waveforms
            # detect_spikes returns waveforms upsampled by 10x and times in units
            # of samples
            waves, times = detect_spikes(filt_el, params[&#39;spike_snapshot&#39;], fs)
            if waves is None:
                print(&#39;No waveforms detected on electrode %i&#39; % electrode)
                return electrode, 0, self.recording_cutoff

            # Save waveforms and times
            np.save(self._files[&#39;spike_waveforms&#39;], waves)
            np.save(self._files[&#39;spike_times&#39;], times)
            status[&#39;spike_waveforms&#39;] = True
            status[&#39;spike_times&#39;] = True

        # Get various metrics and scale waveforms
        if not status[&#39;spike_amplitudes&#39;]:
            amplitudes = get_waveform_amplitudes(waves)
            np.save(self._files[&#39;spike_amplitudes&#39;], amplitudes)
            status[&#39;spike_amplitudes&#39;] = True

        if not status[&#39;slopes&#39;]:
            slopes = get_spike_slopes(waves)
            np.save(self._files[&#39;slopes&#39;], slopes)
            status[&#39;slopes&#39;] = True

        if not status[&#39;energy&#39;]:
            energy = get_waveform_energy(waves)
            np.save(self._files[&#39;energy&#39;], energy)
            status[&#39;energy&#39;] = True
        else:
            energy=None

        # get pca of scaled waveforms
        if not status[&#39;pca_waveforms&#39;]:
            scaled_waves = scale_waveforms(waves, energy=energy)
            pca_waves, explained_variance_ratio = implement_pca(scaled_waves)

            # Plot explained variance
            fn = os.path.join(self._plot_dir, &#39;pca_variance.png&#39;)
            dplt.plot_explained_pca_variance(explained_variance_ratio,
                                             out_file = fn)

        return electrode, 1, self.recording_cutoff

    def get_spike_waveforms(self):
        &#39;&#39;&#39;Returns spike waveforms if they have been extracted, None otherwise
        Dejittered waveforms upsampled to 10 x sampling_rate

        Returns
        -------
        numpy.array
        &#39;&#39;&#39;
        if os.path.isfile(self._files[&#39;spike_waveforms&#39;]):
            return np.load(self._files[&#39;spike_waveforms&#39;])
        else:
            return None

    def get_spike_times(self):
        &#39;&#39;&#39;Returns spike times if they have been extracted, None otherwise
        In units of samples.

        Returns
        -------
        numpy.array
        &#39;&#39;&#39;
        if os.path.isfile(self._files[&#39;spike_times&#39;]):
            return np.load(self._files[&#39;spike_times&#39;])
        else:
            return None

    def get_energy(self):
        &#39;&#39;&#39;Returns spike energies if they have been extracted, None otherwise

        Returns
        -------
        numpy.array
        &#39;&#39;&#39;
        if os.path.isfile(self._files[&#39;energy&#39;]):
            return np.load(self._files[&#39;energy&#39;])
        else:
            return None

    def get_spike_amplitudes(self):
        &#39;&#39;&#39;Returns spike amplitudes if they have been extracted, None otherwise

        Returns
        -------
        numpy.array
        &#39;&#39;&#39;
        if os.path.isfile(self._files[&#39;spike_amplitudes&#39;]):
            return np.load(self._files[&#39;spike_amplitudes&#39;])
        else:
            return None

    def get_spike_slopes(self):
        &#39;&#39;&#39;Returns spike slopes if they have been extracted, None otherwise

        Returns
        -------
        numpy.array
        &#39;&#39;&#39;
        if os.path.isfile(self._files[&#39;slopes&#39;]):
            return np.load(self._files[&#39;slopes&#39;])
        else:
            return None

    def get_pca_waveforms(self):
        &#39;&#39;&#39;Returns pca of sclaed spike waveforms if they have been extracted,
        None otherwise
        Dejittered waveforms upsampled to 10 x sampling_rate, scaled to energy
        and transformed via PCA

        Returns
        -------
        numpy.array
        &#39;&#39;&#39;
        if os.path.isfile(self._files[&#39;spike_waveforms&#39;]):
            return np.load(self._files[&#39;spike_waveforms&#39;])
        else:
            return None

    def get_clustering_metrics(self, n_pc=3):
        &#39;&#39;&#39;Returns array of metrics to use for feature based clustering
        Row for each waveform with columns:
            - amplitude, energy, spike slope, PC1, PC2, etc
        &#39;&#39;&#39;
        amplitude = self.get_spike_amplitudes()
        energy = self.get_energy()
        slopes = self.get_spike_slopes()
        pca_waves = self.get_pca_waveforms()
        out = np.vstack((amplitude, energy, slopes)).T
        out = np.hstack((out, pca_waves[:,:n_pc]))
        return out

    def __str__(self):
        out = []
        out.append(&#39;SpikeDetection\n--------------&#39;)
        out.append(&#39;Recording Directory: %s&#39; % self._file_dir)
        out.append(&#39;Electrode: %i&#39; % self._electrode)
        out.append(&#39;Output Directory: %s&#39; % self._out_dir)
        out.append(&#39;###################################\n&#39;)
        out.append(&#39;Status:&#39;)
        out.append(pt.print_dict(self._status))
        out.append(&#39;-------------------\n&#39;)
        out.append(&#39;Parameters:&#39;)
        out.append(pt.print_dict(self.params))
        out.append(&#39;-------------------\n&#39;)
        out.append(&#39;Data files:&#39;)
        out.append(pt.print_dict(self._files))
        return &#39;\n&#39;.join(out)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="blechpy.analysis.blech_clustering.SpikeDetection.get_clustering_metrics"><code class="name flex">
<span>def <span class="ident">get_clustering_metrics</span></span>(<span>self, n_pc=3)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns array of metrics to use for feature based clustering
Row for each waveform with columns:
- amplitude, energy, spike slope, PC1, PC2, etc</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_clustering_metrics(self, n_pc=3):
    &#39;&#39;&#39;Returns array of metrics to use for feature based clustering
    Row for each waveform with columns:
        - amplitude, energy, spike slope, PC1, PC2, etc
    &#39;&#39;&#39;
    amplitude = self.get_spike_amplitudes()
    energy = self.get_energy()
    slopes = self.get_spike_slopes()
    pca_waves = self.get_pca_waveforms()
    out = np.vstack((amplitude, energy, slopes)).T
    out = np.hstack((out, pca_waves[:,:n_pc]))
    return out</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.blech_clustering.SpikeDetection.get_energy"><code class="name flex">
<span>def <span class="ident">get_energy</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns spike energies if they have been extracted, None otherwise</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>numpy.array</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_energy(self):
    &#39;&#39;&#39;Returns spike energies if they have been extracted, None otherwise

    Returns
    -------
    numpy.array
    &#39;&#39;&#39;
    if os.path.isfile(self._files[&#39;energy&#39;]):
        return np.load(self._files[&#39;energy&#39;])
    else:
        return None</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.blech_clustering.SpikeDetection.get_pca_waveforms"><code class="name flex">
<span>def <span class="ident">get_pca_waveforms</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns pca of sclaed spike waveforms if they have been extracted,
None otherwise
Dejittered waveforms upsampled to 10 x sampling_rate, scaled to energy
and transformed via PCA</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>numpy.array</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_pca_waveforms(self):
    &#39;&#39;&#39;Returns pca of sclaed spike waveforms if they have been extracted,
    None otherwise
    Dejittered waveforms upsampled to 10 x sampling_rate, scaled to energy
    and transformed via PCA

    Returns
    -------
    numpy.array
    &#39;&#39;&#39;
    if os.path.isfile(self._files[&#39;spike_waveforms&#39;]):
        return np.load(self._files[&#39;spike_waveforms&#39;])
    else:
        return None</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.blech_clustering.SpikeDetection.get_spike_amplitudes"><code class="name flex">
<span>def <span class="ident">get_spike_amplitudes</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns spike amplitudes if they have been extracted, None otherwise</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>numpy.array</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_spike_amplitudes(self):
    &#39;&#39;&#39;Returns spike amplitudes if they have been extracted, None otherwise

    Returns
    -------
    numpy.array
    &#39;&#39;&#39;
    if os.path.isfile(self._files[&#39;spike_amplitudes&#39;]):
        return np.load(self._files[&#39;spike_amplitudes&#39;])
    else:
        return None</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.blech_clustering.SpikeDetection.get_spike_slopes"><code class="name flex">
<span>def <span class="ident">get_spike_slopes</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns spike slopes if they have been extracted, None otherwise</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>numpy.array</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_spike_slopes(self):
    &#39;&#39;&#39;Returns spike slopes if they have been extracted, None otherwise

    Returns
    -------
    numpy.array
    &#39;&#39;&#39;
    if os.path.isfile(self._files[&#39;slopes&#39;]):
        return np.load(self._files[&#39;slopes&#39;])
    else:
        return None</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.blech_clustering.SpikeDetection.get_spike_times"><code class="name flex">
<span>def <span class="ident">get_spike_times</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns spike times if they have been extracted, None otherwise
In units of samples.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>numpy.array</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_spike_times(self):
    &#39;&#39;&#39;Returns spike times if they have been extracted, None otherwise
    In units of samples.

    Returns
    -------
    numpy.array
    &#39;&#39;&#39;
    if os.path.isfile(self._files[&#39;spike_times&#39;]):
        return np.load(self._files[&#39;spike_times&#39;])
    else:
        return None</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.blech_clustering.SpikeDetection.get_spike_waveforms"><code class="name flex">
<span>def <span class="ident">get_spike_waveforms</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns spike waveforms if they have been extracted, None otherwise
Dejittered waveforms upsampled to 10 x sampling_rate</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>numpy.array</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_spike_waveforms(self):
    &#39;&#39;&#39;Returns spike waveforms if they have been extracted, None otherwise
    Dejittered waveforms upsampled to 10 x sampling_rate

    Returns
    -------
    numpy.array
    &#39;&#39;&#39;
    if os.path.isfile(self._files[&#39;spike_waveforms&#39;]):
        return np.load(self._files[&#39;spike_waveforms&#39;])
    else:
        return None</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.blech_clustering.SpikeDetection.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run(self):
    status = self._status
    file_dir = self._file_dir
    electrode = self._electrode
    params = self.params
    fs = params[&#39;sampling_rate&#39;]

    # Check if this even needs to be run
    if all(status.values()):
        return electrode, 1, self.recording_cutoff

    # Grab referenced electrode or raw if ref is not available
    ref_el = h5io.get_referenced_trace(file_dir, electrode)
    if ref_el is None:
        print(&#39;Could not find referenced data for electrode %i. Using raw.&#39; % electrode)
        self._referenced = False
        ref_el = h5io.get_raw_trace(file_dir, electrode)
        if ref_el is None:
            raise KeyError(&#39;Neither referenced nor raw data found for electrode %i in %s&#39; % (electrode, file_dir))

    # Filter electrode trace
    filt_el = clustering.get_filtered_electrode(ref_el, freq=params[&#39;bandpass&#39;],
                                           sampling_rate = fs)
    del ref_el
    # Get recording cutoff
    if not status[&#39;recording_cutoff&#39;]:
        self.recording_cutoff = get_recording_cutoff(filt_el, **params)
        with open(self._files[&#39;recording_cutoff&#39;], &#39;w&#39;) as f:
            f.write(str(self.recording_cutoff))

        status[&#39;recording_cutoff&#39;] = True
        fn = os.path.join(self._plot_dir, &#39;cutoff_time.png&#39;)
        dplt.plot_recording_cutoff(filt_el, fs, self.recording_cutoff,
                                   out_file=fn)

    # Truncate electrode trace, deal with early cutoff (&lt;60s)
    if self.recording_cutoff &lt; 60:
        print(&#39;Immediate Cutoff for electrode %i...exiting&#39; % electrode)
        return electrode, 0, self.recording_cutoff

    filt_el = filt_el[:int(self.recording_cutoff*fs)]

    if status[&#39;spike_waveforms&#39;] and status[&#39;spike_times&#39;]:
        waves = np.load(self._files[&#39;spike_waveforms&#39;])
        times = np.load(self._files[&#39;spike_times&#39;])
    else:
        # Detect spikes and get dejittered times and waveforms
        # detect_spikes returns waveforms upsampled by 10x and times in units
        # of samples
        waves, times = detect_spikes(filt_el, params[&#39;spike_snapshot&#39;], fs)
        if waves is None:
            print(&#39;No waveforms detected on electrode %i&#39; % electrode)
            return electrode, 0, self.recording_cutoff

        # Save waveforms and times
        np.save(self._files[&#39;spike_waveforms&#39;], waves)
        np.save(self._files[&#39;spike_times&#39;], times)
        status[&#39;spike_waveforms&#39;] = True
        status[&#39;spike_times&#39;] = True

    # Get various metrics and scale waveforms
    if not status[&#39;spike_amplitudes&#39;]:
        amplitudes = get_waveform_amplitudes(waves)
        np.save(self._files[&#39;spike_amplitudes&#39;], amplitudes)
        status[&#39;spike_amplitudes&#39;] = True

    if not status[&#39;slopes&#39;]:
        slopes = get_spike_slopes(waves)
        np.save(self._files[&#39;slopes&#39;], slopes)
        status[&#39;slopes&#39;] = True

    if not status[&#39;energy&#39;]:
        energy = get_waveform_energy(waves)
        np.save(self._files[&#39;energy&#39;], energy)
        status[&#39;energy&#39;] = True
    else:
        energy=None

    # get pca of scaled waveforms
    if not status[&#39;pca_waveforms&#39;]:
        scaled_waves = scale_waveforms(waves, energy=energy)
        pca_waves, explained_variance_ratio = implement_pca(scaled_waves)

        # Plot explained variance
        fn = os.path.join(self._plot_dir, &#39;pca_variance.png&#39;)
        dplt.plot_explained_pca_variance(explained_variance_ratio,
                                         out_file = fn)

    return electrode, 1, self.recording_cutoff</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="blechpy.analysis.blech_clustering.SpikeSorter"><code class="flex name class">
<span>class <span class="ident">SpikeSorter</span></span>
<span>(</span><span>rec_dirs, electrode, clustering_dir=None, shell=False)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SpikeSorter(object):
    def __init__(self, rec_dirs, electrode, clustering_dir=None, shell=False):
        if isinstance(rec_dirs, str):
            rec_dirs = [rec_dirs]

        rec_dirs = [x[:-1] if x.endswith(os.sep) else x for x in rec_dirs]
        self.rec_dirs = rec_dirs
        self.electrode = electrode
        if clustering_dir is None:
            if len(rec_dirs) &gt; 1:
                top = os.path.dirname(rec_dirs[0])
                clustering_dir = os.path.join(top, &#39;BlechClust&#39;, &#39;electrode_%i&#39; % electrode)
            else:
                clustering_dir = os.path.join(rec_dirs[0], &#39;BlechClust&#39;, &#39;electrode_%i&#39; % electrode)

        self.clustering_dir = clustering_dir
        try:
            clust = BlechClust(rec_dirs, electrode, out_dir = clustering_dir, no_write=True)
        except FileNotFoundError:
            clust = None

        if clust is None or not clust.clustered:
            raise ValueError(&#39;Recordings have not been clustered yet.&#39;)

        # Match recording directory ordering to clustering
        self.rec_dirs = clust.rec_dirs
        self.clustering = clust
        self._active = None
        self._last_saved = None
        self._previous = None
        self._shell = shell
        self._split_results = None
        self._split_starter = None
        self._split_index = None

    def set_active_clusters(self, solution_num):
        cluster_nums = list(range(solution_num))
        clusters = self.clustering.get_clusters(solution_num, cluster_nums)
        if len(clusters) == 0:
            raise ValueError(&#39;Solution or clusters not found&#39;)

        self._active = clusters

    def save_clusters(self, target_clusters, single_unit, pyramidal, interneuron):
        &#39;&#39;&#39;Saves active clusters as cells, write them to the h5_files in the
        appropriate recording directories

        Parameters
        ----------
        target_clusters: list of int
            indicies of active clusters to save
        single_unit : list of bool
            elements in list must correspond to elements in active clusters
        pyramidal : list of bool
        interneuron : list of bool
        &#39;&#39;&#39;
        if self._active is None:
            return

        if any([i &gt;= len(self._active) for i in target_clusters]):
            raise ValueError(&#39;Target cluster is out of range.&#39;)

        n_clusters = len(target_clusters)
        if (len(single_unit) != n_clusters or len(pyramidal) != n_clusters or
            len(interneuron) != n_clusters):
            raise ValueError(&#39;Length of input lists must match number of &#39;
                             &#39;active clusters. Expected %i&#39; % n_clusters)

        clusters = [self._active[i] for i in target_clusters]
        rec_key = self.clustering._rec_key
        self._last_saved = dict.fromkeys(rec_key.keys(), None)

        for clust, single, pyr, intr in zip(clusters, single_unit,
                                            pyramidal, interneuron):
            for i, rec in rec_key.items():
                idx = np.where(clust[&#39;spike_map&#39;] == i)[0]
                waves = clust[&#39;spike_waveforms&#39;][idx]
                times = clust[&#39;spike_times&#39;][idx]
                unit_name = h5io.add_new_unit(rec, self.electrode, waves,
                                              times, single, pyr, intr)
                if self._last_saved[i] is None:
                    self._last_saved[i] = [unit_name]
                else:
                    self._last_saved[i].append(unit_name)

                metrics_dir = os.path.join(rec,&#39;sorted_unit_metrics&#39;, unit_name)
                if not os.path.isdir(metrics_dir):
                    os.makedirs(metrics_dir)

                # Write cluster info to file
                print_clust = clust.copy()
                for k,v in clust.items():
                    if isinstance(v, np.ndarray):
                        print_clust.pop(k)

                print_clust.pop(&#39;rec_key&#39;)
                print_clust.pop(&#39;fs&#39;)
                clust_info_file = os.path.join(metrics_dir, &#39;cluster.info&#39;)
                with open(clust_info_file, &#39;a+&#39;) as log:
                    print(&#39;%s sorted on %s&#39;
                          % (unit_name,
                             dt.datetime.today().strftime(&#39;%m/%d/%y %H:%M&#39;)),
                          file=log)
                    print(&#39;Cluster info: \n----------&#39;, file=log)
                    print(pt.print_dict(print_clust), file=log)
                    print(&#39;Saved metrics to %s&#39; % metrics_dir, file=log)
                    print(&#39;--------------\n&#39;, file=log)

        userIO.tell_user(&#39;Target clusters successfully saved to recording &#39;
                         &#39;directories.&#39;, shell=True)
        self._active = [self._active[i] for i in range(len(self._active))
                        if i not in target_clusters]
        self._previous = clusters

    def undo_last_save(self):
        if self._last_saved is None:
            return

        rec_key = self.clustering._rec_key
        last_saved = self._last_saved
        for i, rec in rec_key.items():
            for unit in reversed(np.sort(last_saved[i])):
                h5io.delete_unit(rec, unit)

        self._active.extend(self._previous)
        self._last_saved = None
        self._previous = None

    def split_cluster(self, target_clust, n_iter, n_restart, thresh, n_clust,
                      store_split=False, umap=False):
        &#39;&#39;&#39;splits the target active cluster using a GMM
        &#39;&#39;&#39;
        if target_clust &gt;= len(self._active):
            raise ValueError(&#39;Invalid target. Only %i active clusters&#39; % len(self._active))

        cluster = self._active.pop(target_clust)
        self._split_starter = cluster
        GMM = ClusterGMM(n_iter, n_restart, thresh)
        waves = cluster[&#39;spike_waveforms&#39;]
        data, data_columns = compute_waveform_metrics(waves, umap=umap)
        model, predictions, bic = GMM.fit(data, n_clust)
        new_clusts = []
        for i in np.unique(predictions):
            idx = np.where(predictions == i)[0]
            edit_str = (cluster[&#39;manipulations&#39;] + &#39;Split %s into %i &#39;
                        &#39;clusters. This is sub-cluster %i&#39;
                        % (cluster[&#39;manipulations&#39;], n_clust, i))
            tmp_clust = {&#39;Cluster Name&#39;: cluster[&#39;Cluster Name&#39;] + &#39;-%i&#39; % i,
                         &#39;solution_num&#39;: cluster[&#39;solution_num&#39;],
                         &#39;cluster_num&#39;: cluster[&#39;cluster_num&#39;],
                         &#39;cluster_id&#39;: cluster[&#39;cluster_id&#39;]*10+i,
                         &#39;spike_waveforms&#39;: waves[idx],
                         &#39;spike_times&#39;: cluster[&#39;spike_times&#39;][idx],
                         &#39;spike_map&#39;: cluster[&#39;spike_map&#39;][idx],
                         &#39;rec_key&#39;: cluster[&#39;rec_key&#39;].copy(),
                         &#39;fs&#39;: cluster[&#39;fs&#39;],
                         &#39;offsets&#39;: cluster[&#39;offsets&#39;],
                         &#39;manipulations&#39;: edit_str}
            new_clusts.append(tmp_clust)

        # Plot cluster and ask to choose which to keep
        figs = []
        for i, c in enumerate(new_clusts):
            _, viol_1ms, viol_2ms = get_ISI_and_violations(c[&#39;spike_times&#39;], c[&#39;fs&#39;], c[&#39;spike_map&#39;])
            plot_title = (&#39;Index: %i\n1ms violations: %i, 2ms violations: %i\n&#39;
                          &#39;Total Waveforms: %i&#39;
                          % (i, viol_1ms, viol_2ms, len(c[&#39;spike_times&#39;])))
            tmp_fig, _ = dplt.plot_waveforms(c[&#39;spike_waveforms&#39;], title=plot_title)
            figs.append(tmp_fig)
            tmp_fig.show()

        f2 = dplt.plot_waveforms_pca([c[&#39;spike_waveforms&#39;] for c in new_clusts])
        figs.append(f2)
        f2.show()

        if store_split:
            self._split_results = new_clusts
            self._split_index = target_clust
            return new_clusts
        else:
            self._split_starter = None
            selection_list = [&#39;all&#39;] + [&#39;%i&#39; % i for i in range(len(new_clusts))]
            prompt = &#39;Select split clusters to keep\nCancel to reset.&#39;
            ans = userIO.select_from_list(prompt, selection_list,
                                          multi_select=True, shell=self._shell)
            if ans is None or &#39;all&#39; in ans:
                print(&#39;Reset to before split&#39;)
                self._active.insert(target_clust, cluster)
            else:
                keepers = [new_clusts[int(i)] for i in ans]
                self._active.extend(keepers)

            return True

    def set_split(self, choices):
        if self._split_starter is None:
            raise ValueError(&#39;Not split stored.&#39;)

        if len(choices) == 0:
            self._active.insert(self._split_index, self._split_starter)
        else:
            keepers = [self._split_results[i] for i in choices]
            self._active.extend(keepers)

        self._split_index = None
        self._split_results = None
        self._split_starter = None

    def merge_clusters(self, target_clusters):
        if any([i &gt;= len(self._active) for i in target_clusters]):
            raise ValueError(&#39;Target cluster is out of range.&#39;)

        new_clust = []
        for c in target_clusters:
            if len(new_clust) == 0:
                new_clust = deepcopy(self._active[c])
                continue

            clust = self._active[c]
            sm1 = new_clust[&#39;spike_map&#39;]
            sm2 = clust[&#39;spike_map&#39;]
            st1 = new_clust[&#39;spike_times&#39;]
            st2 = clust[&#39;spike_times&#39;]
            sw1 = new_clust[&#39;spike_waveforms&#39;]
            sw2 = clust[&#39;spike_waveforms&#39;]

            spike_map = np.hstack((sm1, sm2))
            spike_times = np.hstack((st1, st2))
            spike_waveforms = np.vstack((sw1, sw2))

            # Re-order to spike_map
            idx = np.argsort(spike_map)
            spike_map = spike_map[idx]
            spike_times = spike_times[idx]
            spike_waveforms = spike_waveforms[idx]

            # Re-order so spike_times within a reocrding are in order
            times = []
            waves = []
            new_map = []
            for i in np.unique(spike_map):
                idx = np.where(spike_map == i)[0]
                st = spike_times[idx]
                sw = spike_waveforms[idx]
                sm = spike_map[idx]
                idx2 = np.argsort(st)
                st = st[idx2]
                sw = sw[idx2]
                sm = sm[idx2]
                times.append(st)
                waves.append(sw)
                new_map.append(sm)

            times = np.hstack(times)
            waves = np.vstack(waves)
            spike_map = np.hstack(new_map)
            del new_map, spike_times, spike_waveforms

            new_clust[&#39;spike_map&#39;] = spike_map
            new_clust[&#39;spike_times&#39;] = times
            new_clust[&#39;spike_waveforms&#39;] = waves
            new_clust[&#39;manipulations&#39;] += &#39;\nMerged with %s.&#39; % clust[&#39;Cluster Name&#39;]
            new_clust[&#39;Cluster Name&#39;] += &#39;+&#39; + clust[&#39;Cluster Name&#39;].replace(&#39;Cluster_&#39;,&#39;&#39;)

        self._active = [self._active[i] for i in range(len(self._active))
                        if i not in target_clusters]

        self._active.append(new_clust)

    def discard_clusters(self, target_clusters):
        self._active = [self._active[i] for i in range(len(self._active))
                        if i not in target_clusters]

    def plot_clusters_waveforms(self, target_clusters):
        if len(target_clusters) == 0:
            return

        for i in target_clusters:
            c = self._active[i]
            isi, v1, v2 = get_ISI_and_violations(c[&#39;spike_times&#39;], c[&#39;fs&#39;], c[&#39;spike_map&#39;])
            title = (&#39;Index : %i\n1ms violations: %0.1f, 2ms violations: %0.1f&#39;
                     &#39;\ntotal waveforms: %i&#39;
                     % (i, v1, v2, len(c[&#39;spike_waveforms&#39;])))
            fig, ax = dplt.plot_waveforms(c[&#39;spike_waveforms&#39;], title=title)
            fig.show()

    def plot_clusters_pca(self, target_clusters):
        if len(target_clusters) == 0:
            return

        waves = [self._active[i][&#39;spike_waveforms&#39;] for i in target_clusters]
        fig = dplt.plot_waveforms_pca(waves, cluster_ids=target_clusters)
        fig.show()

    def plot_clusters_umap(self, target_clusters):
        if len(target_clusters) == 0:
            return

        waves = [self._active[i][&#39;spike_waveforms&#39;] for i in target_clusters]
        fig = dplt.plot_waveforms_umap(waves, cluster_ids=target_clusters)
        fig.show()

    def plot_clusters_raster(self, target_clusters):
        if len(target_clusters) == 0:
            return

        clusters = [self._active[i] for i in target_clusters]
        spike_times = []
        spike_waves = []
        vlines = {}
        for c in clusters:
            # Adjust spike times by offset so recordings are not overlapping
            sm = c[&#39;spike_map&#39;]
            st = c[&#39;spike_times&#39;].copy().astype(&#39;float64&#39;)
            for i in np.unique(sm):
                idx = np.where(sm==i)[0]
                st[idx] += c[&#39;offsets&#39;][i]
                st[idx] = st[idx]/c[&#39;fs&#39;][i]  # convert to seconds
                if vlines.get(i) is None and c[&#39;offsets&#39;][i] != 0:
                    vlines[i] = c[&#39;offsets&#39;][i]/c[&#39;fs&#39;][i]


            spike_times.append(st)
            spike_waves.append(c[&#39;spike_waveforms&#39;])

        fig, ax = dplt.plot_spike_raster(spike_times, spike_waves, target_clusters)
        ax.set_xlabel(&#39;Time (s)&#39;)
        for x in vlines.values():
            ax.axvline(x, color=&#39;black&#39;, linewidth=2)

        fig.show()

    def plot_clusters_ISI(self, target_clusters):
        if len(target_clusters) == 0:
            return

        figs = []
        for i in target_clusters:
            cluster = self._active[i]
            isi, v1, v2 = get_ISI_and_violations(cluster[&#39;spike_times&#39;],
                                                 cluster[&#39;fs&#39;],
                                                 cluster[&#39;spike_map&#39;])
            fig, ax = dplt.plot_ISIs(isi, total_spikes=len(cluster[&#39;spike_times&#39;]))
            title= ax.get_title()
            title = &#39;Index: %i\n%s&#39; % (i, title)
            ax.set_title(title)
            fig.show()

    def get_mean_waveform(self, target_cluster):
        &#39;&#39;&#39;Returns mean waveform of target_cluster in active clusters. Also
        returns St. Dev. of waveforms
        &#39;&#39;&#39;
        cluster = self._active[target_cluster]
        mean_wave = np.mean(cluster[&#39;spike_waveforms&#39;], axis=0)
        std_wave = np.std(cluster[&#39;spike_waveforms&#39;], axis=0)
        return mean_wave, std_wave

    def get_possible_solutions(self):
        results = self.clustering.results.dropna()
        converged = list(results[results[&#39;converged&#39;]].index)
        return converged</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="blechpy.analysis.blech_clustering.SpikeSorter.discard_clusters"><code class="name flex">
<span>def <span class="ident">discard_clusters</span></span>(<span>self, target_clusters)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def discard_clusters(self, target_clusters):
    self._active = [self._active[i] for i in range(len(self._active))
                    if i not in target_clusters]</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.blech_clustering.SpikeSorter.get_mean_waveform"><code class="name flex">
<span>def <span class="ident">get_mean_waveform</span></span>(<span>self, target_cluster)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns mean waveform of target_cluster in active clusters. Also
returns St. Dev. of waveforms</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_mean_waveform(self, target_cluster):
    &#39;&#39;&#39;Returns mean waveform of target_cluster in active clusters. Also
    returns St. Dev. of waveforms
    &#39;&#39;&#39;
    cluster = self._active[target_cluster]
    mean_wave = np.mean(cluster[&#39;spike_waveforms&#39;], axis=0)
    std_wave = np.std(cluster[&#39;spike_waveforms&#39;], axis=0)
    return mean_wave, std_wave</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.blech_clustering.SpikeSorter.get_possible_solutions"><code class="name flex">
<span>def <span class="ident">get_possible_solutions</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_possible_solutions(self):
    results = self.clustering.results.dropna()
    converged = list(results[results[&#39;converged&#39;]].index)
    return converged</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.blech_clustering.SpikeSorter.merge_clusters"><code class="name flex">
<span>def <span class="ident">merge_clusters</span></span>(<span>self, target_clusters)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def merge_clusters(self, target_clusters):
    if any([i &gt;= len(self._active) for i in target_clusters]):
        raise ValueError(&#39;Target cluster is out of range.&#39;)

    new_clust = []
    for c in target_clusters:
        if len(new_clust) == 0:
            new_clust = deepcopy(self._active[c])
            continue

        clust = self._active[c]
        sm1 = new_clust[&#39;spike_map&#39;]
        sm2 = clust[&#39;spike_map&#39;]
        st1 = new_clust[&#39;spike_times&#39;]
        st2 = clust[&#39;spike_times&#39;]
        sw1 = new_clust[&#39;spike_waveforms&#39;]
        sw2 = clust[&#39;spike_waveforms&#39;]

        spike_map = np.hstack((sm1, sm2))
        spike_times = np.hstack((st1, st2))
        spike_waveforms = np.vstack((sw1, sw2))

        # Re-order to spike_map
        idx = np.argsort(spike_map)
        spike_map = spike_map[idx]
        spike_times = spike_times[idx]
        spike_waveforms = spike_waveforms[idx]

        # Re-order so spike_times within a reocrding are in order
        times = []
        waves = []
        new_map = []
        for i in np.unique(spike_map):
            idx = np.where(spike_map == i)[0]
            st = spike_times[idx]
            sw = spike_waveforms[idx]
            sm = spike_map[idx]
            idx2 = np.argsort(st)
            st = st[idx2]
            sw = sw[idx2]
            sm = sm[idx2]
            times.append(st)
            waves.append(sw)
            new_map.append(sm)

        times = np.hstack(times)
        waves = np.vstack(waves)
        spike_map = np.hstack(new_map)
        del new_map, spike_times, spike_waveforms

        new_clust[&#39;spike_map&#39;] = spike_map
        new_clust[&#39;spike_times&#39;] = times
        new_clust[&#39;spike_waveforms&#39;] = waves
        new_clust[&#39;manipulations&#39;] += &#39;\nMerged with %s.&#39; % clust[&#39;Cluster Name&#39;]
        new_clust[&#39;Cluster Name&#39;] += &#39;+&#39; + clust[&#39;Cluster Name&#39;].replace(&#39;Cluster_&#39;,&#39;&#39;)

    self._active = [self._active[i] for i in range(len(self._active))
                    if i not in target_clusters]

    self._active.append(new_clust)</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.blech_clustering.SpikeSorter.plot_clusters_ISI"><code class="name flex">
<span>def <span class="ident">plot_clusters_ISI</span></span>(<span>self, target_clusters)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_clusters_ISI(self, target_clusters):
    if len(target_clusters) == 0:
        return

    figs = []
    for i in target_clusters:
        cluster = self._active[i]
        isi, v1, v2 = get_ISI_and_violations(cluster[&#39;spike_times&#39;],
                                             cluster[&#39;fs&#39;],
                                             cluster[&#39;spike_map&#39;])
        fig, ax = dplt.plot_ISIs(isi, total_spikes=len(cluster[&#39;spike_times&#39;]))
        title= ax.get_title()
        title = &#39;Index: %i\n%s&#39; % (i, title)
        ax.set_title(title)
        fig.show()</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.blech_clustering.SpikeSorter.plot_clusters_pca"><code class="name flex">
<span>def <span class="ident">plot_clusters_pca</span></span>(<span>self, target_clusters)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_clusters_pca(self, target_clusters):
    if len(target_clusters) == 0:
        return

    waves = [self._active[i][&#39;spike_waveforms&#39;] for i in target_clusters]
    fig = dplt.plot_waveforms_pca(waves, cluster_ids=target_clusters)
    fig.show()</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.blech_clustering.SpikeSorter.plot_clusters_raster"><code class="name flex">
<span>def <span class="ident">plot_clusters_raster</span></span>(<span>self, target_clusters)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_clusters_raster(self, target_clusters):
    if len(target_clusters) == 0:
        return

    clusters = [self._active[i] for i in target_clusters]
    spike_times = []
    spike_waves = []
    vlines = {}
    for c in clusters:
        # Adjust spike times by offset so recordings are not overlapping
        sm = c[&#39;spike_map&#39;]
        st = c[&#39;spike_times&#39;].copy().astype(&#39;float64&#39;)
        for i in np.unique(sm):
            idx = np.where(sm==i)[0]
            st[idx] += c[&#39;offsets&#39;][i]
            st[idx] = st[idx]/c[&#39;fs&#39;][i]  # convert to seconds
            if vlines.get(i) is None and c[&#39;offsets&#39;][i] != 0:
                vlines[i] = c[&#39;offsets&#39;][i]/c[&#39;fs&#39;][i]


        spike_times.append(st)
        spike_waves.append(c[&#39;spike_waveforms&#39;])

    fig, ax = dplt.plot_spike_raster(spike_times, spike_waves, target_clusters)
    ax.set_xlabel(&#39;Time (s)&#39;)
    for x in vlines.values():
        ax.axvline(x, color=&#39;black&#39;, linewidth=2)

    fig.show()</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.blech_clustering.SpikeSorter.plot_clusters_umap"><code class="name flex">
<span>def <span class="ident">plot_clusters_umap</span></span>(<span>self, target_clusters)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_clusters_umap(self, target_clusters):
    if len(target_clusters) == 0:
        return

    waves = [self._active[i][&#39;spike_waveforms&#39;] for i in target_clusters]
    fig = dplt.plot_waveforms_umap(waves, cluster_ids=target_clusters)
    fig.show()</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.blech_clustering.SpikeSorter.plot_clusters_waveforms"><code class="name flex">
<span>def <span class="ident">plot_clusters_waveforms</span></span>(<span>self, target_clusters)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_clusters_waveforms(self, target_clusters):
    if len(target_clusters) == 0:
        return

    for i in target_clusters:
        c = self._active[i]
        isi, v1, v2 = get_ISI_and_violations(c[&#39;spike_times&#39;], c[&#39;fs&#39;], c[&#39;spike_map&#39;])
        title = (&#39;Index : %i\n1ms violations: %0.1f, 2ms violations: %0.1f&#39;
                 &#39;\ntotal waveforms: %i&#39;
                 % (i, v1, v2, len(c[&#39;spike_waveforms&#39;])))
        fig, ax = dplt.plot_waveforms(c[&#39;spike_waveforms&#39;], title=title)
        fig.show()</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.blech_clustering.SpikeSorter.save_clusters"><code class="name flex">
<span>def <span class="ident">save_clusters</span></span>(<span>self, target_clusters, single_unit, pyramidal, interneuron)</span>
</code></dt>
<dd>
<section class="desc"><p>Saves active clusters as cells, write them to the h5_files in the
appropriate recording directories</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>target_clusters</code></strong> :&ensp;<code>list</code> of <code>int</code></dt>
<dd>indicies of active clusters to save</dd>
<dt><strong><code>single_unit</code></strong> :&ensp;<code>list</code> of <code>bool</code></dt>
<dd>elements in list must correspond to elements in active clusters</dd>
<dt><strong><code>pyramidal</code></strong> :&ensp;<code>list</code> of <code>bool</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>interneuron</code></strong> :&ensp;<code>list</code> of <code>bool</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_clusters(self, target_clusters, single_unit, pyramidal, interneuron):
    &#39;&#39;&#39;Saves active clusters as cells, write them to the h5_files in the
    appropriate recording directories

    Parameters
    ----------
    target_clusters: list of int
        indicies of active clusters to save
    single_unit : list of bool
        elements in list must correspond to elements in active clusters
    pyramidal : list of bool
    interneuron : list of bool
    &#39;&#39;&#39;
    if self._active is None:
        return

    if any([i &gt;= len(self._active) for i in target_clusters]):
        raise ValueError(&#39;Target cluster is out of range.&#39;)

    n_clusters = len(target_clusters)
    if (len(single_unit) != n_clusters or len(pyramidal) != n_clusters or
        len(interneuron) != n_clusters):
        raise ValueError(&#39;Length of input lists must match number of &#39;
                         &#39;active clusters. Expected %i&#39; % n_clusters)

    clusters = [self._active[i] for i in target_clusters]
    rec_key = self.clustering._rec_key
    self._last_saved = dict.fromkeys(rec_key.keys(), None)

    for clust, single, pyr, intr in zip(clusters, single_unit,
                                        pyramidal, interneuron):
        for i, rec in rec_key.items():
            idx = np.where(clust[&#39;spike_map&#39;] == i)[0]
            waves = clust[&#39;spike_waveforms&#39;][idx]
            times = clust[&#39;spike_times&#39;][idx]
            unit_name = h5io.add_new_unit(rec, self.electrode, waves,
                                          times, single, pyr, intr)
            if self._last_saved[i] is None:
                self._last_saved[i] = [unit_name]
            else:
                self._last_saved[i].append(unit_name)

            metrics_dir = os.path.join(rec,&#39;sorted_unit_metrics&#39;, unit_name)
            if not os.path.isdir(metrics_dir):
                os.makedirs(metrics_dir)

            # Write cluster info to file
            print_clust = clust.copy()
            for k,v in clust.items():
                if isinstance(v, np.ndarray):
                    print_clust.pop(k)

            print_clust.pop(&#39;rec_key&#39;)
            print_clust.pop(&#39;fs&#39;)
            clust_info_file = os.path.join(metrics_dir, &#39;cluster.info&#39;)
            with open(clust_info_file, &#39;a+&#39;) as log:
                print(&#39;%s sorted on %s&#39;
                      % (unit_name,
                         dt.datetime.today().strftime(&#39;%m/%d/%y %H:%M&#39;)),
                      file=log)
                print(&#39;Cluster info: \n----------&#39;, file=log)
                print(pt.print_dict(print_clust), file=log)
                print(&#39;Saved metrics to %s&#39; % metrics_dir, file=log)
                print(&#39;--------------\n&#39;, file=log)

    userIO.tell_user(&#39;Target clusters successfully saved to recording &#39;
                     &#39;directories.&#39;, shell=True)
    self._active = [self._active[i] for i in range(len(self._active))
                    if i not in target_clusters]
    self._previous = clusters</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.blech_clustering.SpikeSorter.set_active_clusters"><code class="name flex">
<span>def <span class="ident">set_active_clusters</span></span>(<span>self, solution_num)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_active_clusters(self, solution_num):
    cluster_nums = list(range(solution_num))
    clusters = self.clustering.get_clusters(solution_num, cluster_nums)
    if len(clusters) == 0:
        raise ValueError(&#39;Solution or clusters not found&#39;)

    self._active = clusters</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.blech_clustering.SpikeSorter.set_split"><code class="name flex">
<span>def <span class="ident">set_split</span></span>(<span>self, choices)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_split(self, choices):
    if self._split_starter is None:
        raise ValueError(&#39;Not split stored.&#39;)

    if len(choices) == 0:
        self._active.insert(self._split_index, self._split_starter)
    else:
        keepers = [self._split_results[i] for i in choices]
        self._active.extend(keepers)

    self._split_index = None
    self._split_results = None
    self._split_starter = None</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.blech_clustering.SpikeSorter.split_cluster"><code class="name flex">
<span>def <span class="ident">split_cluster</span></span>(<span>self, target_clust, n_iter, n_restart, thresh, n_clust, store_split=False, umap=False)</span>
</code></dt>
<dd>
<section class="desc"><p>splits the target active cluster using a GMM</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def split_cluster(self, target_clust, n_iter, n_restart, thresh, n_clust,
                  store_split=False, umap=False):
    &#39;&#39;&#39;splits the target active cluster using a GMM
    &#39;&#39;&#39;
    if target_clust &gt;= len(self._active):
        raise ValueError(&#39;Invalid target. Only %i active clusters&#39; % len(self._active))

    cluster = self._active.pop(target_clust)
    self._split_starter = cluster
    GMM = ClusterGMM(n_iter, n_restart, thresh)
    waves = cluster[&#39;spike_waveforms&#39;]
    data, data_columns = compute_waveform_metrics(waves, umap=umap)
    model, predictions, bic = GMM.fit(data, n_clust)
    new_clusts = []
    for i in np.unique(predictions):
        idx = np.where(predictions == i)[0]
        edit_str = (cluster[&#39;manipulations&#39;] + &#39;Split %s into %i &#39;
                    &#39;clusters. This is sub-cluster %i&#39;
                    % (cluster[&#39;manipulations&#39;], n_clust, i))
        tmp_clust = {&#39;Cluster Name&#39;: cluster[&#39;Cluster Name&#39;] + &#39;-%i&#39; % i,
                     &#39;solution_num&#39;: cluster[&#39;solution_num&#39;],
                     &#39;cluster_num&#39;: cluster[&#39;cluster_num&#39;],
                     &#39;cluster_id&#39;: cluster[&#39;cluster_id&#39;]*10+i,
                     &#39;spike_waveforms&#39;: waves[idx],
                     &#39;spike_times&#39;: cluster[&#39;spike_times&#39;][idx],
                     &#39;spike_map&#39;: cluster[&#39;spike_map&#39;][idx],
                     &#39;rec_key&#39;: cluster[&#39;rec_key&#39;].copy(),
                     &#39;fs&#39;: cluster[&#39;fs&#39;],
                     &#39;offsets&#39;: cluster[&#39;offsets&#39;],
                     &#39;manipulations&#39;: edit_str}
        new_clusts.append(tmp_clust)

    # Plot cluster and ask to choose which to keep
    figs = []
    for i, c in enumerate(new_clusts):
        _, viol_1ms, viol_2ms = get_ISI_and_violations(c[&#39;spike_times&#39;], c[&#39;fs&#39;], c[&#39;spike_map&#39;])
        plot_title = (&#39;Index: %i\n1ms violations: %i, 2ms violations: %i\n&#39;
                      &#39;Total Waveforms: %i&#39;
                      % (i, viol_1ms, viol_2ms, len(c[&#39;spike_times&#39;])))
        tmp_fig, _ = dplt.plot_waveforms(c[&#39;spike_waveforms&#39;], title=plot_title)
        figs.append(tmp_fig)
        tmp_fig.show()

    f2 = dplt.plot_waveforms_pca([c[&#39;spike_waveforms&#39;] for c in new_clusts])
    figs.append(f2)
    f2.show()

    if store_split:
        self._split_results = new_clusts
        self._split_index = target_clust
        return new_clusts
    else:
        self._split_starter = None
        selection_list = [&#39;all&#39;] + [&#39;%i&#39; % i for i in range(len(new_clusts))]
        prompt = &#39;Select split clusters to keep\nCancel to reset.&#39;
        ans = userIO.select_from_list(prompt, selection_list,
                                      multi_select=True, shell=self._shell)
        if ans is None or &#39;all&#39; in ans:
            print(&#39;Reset to before split&#39;)
            self._active.insert(target_clust, cluster)
        else:
            keepers = [new_clusts[int(i)] for i in ans]
            self._active.extend(keepers)

        return True</code></pre>
</details>
</dd>
<dt id="blechpy.analysis.blech_clustering.SpikeSorter.undo_last_save"><code class="name flex">
<span>def <span class="ident">undo_last_save</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def undo_last_save(self):
    if self._last_saved is None:
        return

    rec_key = self.clustering._rec_key
    last_saved = self._last_saved
    for i, rec in rec_key.items():
        for unit in reversed(np.sort(last_saved[i])):
            h5io.delete_unit(rec, unit)

    self._active.extend(self._previous)
    self._last_saved = None
    self._previous = None</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="blechpy.analysis" href="index.html">blechpy.analysis</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="blechpy.analysis.blech_clustering.compute_waveform_metrics" href="#blechpy.analysis.blech_clustering.compute_waveform_metrics">compute_waveform_metrics</a></code></li>
<li><code><a title="blechpy.analysis.blech_clustering.detect_spikes" href="#blechpy.analysis.blech_clustering.detect_spikes">detect_spikes</a></code></li>
<li><code><a title="blechpy.analysis.blech_clustering.get_ISI_and_violations" href="#blechpy.analysis.blech_clustering.get_ISI_and_violations">get_ISI_and_violations</a></code></li>
<li><code><a title="blechpy.analysis.blech_clustering.get_mahalanobis_distances_to_cluster" href="#blechpy.analysis.blech_clustering.get_mahalanobis_distances_to_cluster">get_mahalanobis_distances_to_cluster</a></code></li>
<li><code><a title="blechpy.analysis.blech_clustering.get_recording_cutoff" href="#blechpy.analysis.blech_clustering.get_recording_cutoff">get_recording_cutoff</a></code></li>
<li><code><a title="blechpy.analysis.blech_clustering.get_spike_slopes" href="#blechpy.analysis.blech_clustering.get_spike_slopes">get_spike_slopes</a></code></li>
<li><code><a title="blechpy.analysis.blech_clustering.get_waveform_amplitudes" href="#blechpy.analysis.blech_clustering.get_waveform_amplitudes">get_waveform_amplitudes</a></code></li>
<li><code><a title="blechpy.analysis.blech_clustering.get_waveform_energy" href="#blechpy.analysis.blech_clustering.get_waveform_energy">get_waveform_energy</a></code></li>
<li><code><a title="blechpy.analysis.blech_clustering.implement_pca" href="#blechpy.analysis.blech_clustering.implement_pca">implement_pca</a></code></li>
<li><code><a title="blechpy.analysis.blech_clustering.implement_umap" href="#blechpy.analysis.blech_clustering.implement_umap">implement_umap</a></code></li>
<li><code><a title="blechpy.analysis.blech_clustering.scale_waveforms" href="#blechpy.analysis.blech_clustering.scale_waveforms">scale_waveforms</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="blechpy.analysis.blech_clustering.BlechClust" href="#blechpy.analysis.blech_clustering.BlechClust">BlechClust</a></code></h4>
<ul class="">
<li><code><a title="blechpy.analysis.blech_clustering.BlechClust.get_clusters" href="#blechpy.analysis.blech_clustering.BlechClust.get_clusters">get_clusters</a></code></li>
<li><code><a title="blechpy.analysis.blech_clustering.BlechClust.get_predictions" href="#blechpy.analysis.blech_clustering.BlechClust.get_predictions">get_predictions</a></code></li>
<li><code><a title="blechpy.analysis.blech_clustering.BlechClust.get_spike_data" href="#blechpy.analysis.blech_clustering.BlechClust.get_spike_data">get_spike_data</a></code></li>
<li><code><a title="blechpy.analysis.blech_clustering.BlechClust.run" href="#blechpy.analysis.blech_clustering.BlechClust.run">run</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="blechpy.analysis.blech_clustering.ClusterGMM" href="#blechpy.analysis.blech_clustering.ClusterGMM">ClusterGMM</a></code></h4>
<ul class="">
<li><code><a title="blechpy.analysis.blech_clustering.ClusterGMM.fit" href="#blechpy.analysis.blech_clustering.ClusterGMM.fit">fit</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="blechpy.analysis.blech_clustering.SpikeDetection" href="#blechpy.analysis.blech_clustering.SpikeDetection">SpikeDetection</a></code></h4>
<ul class="">
<li><code><a title="blechpy.analysis.blech_clustering.SpikeDetection.get_clustering_metrics" href="#blechpy.analysis.blech_clustering.SpikeDetection.get_clustering_metrics">get_clustering_metrics</a></code></li>
<li><code><a title="blechpy.analysis.blech_clustering.SpikeDetection.get_energy" href="#blechpy.analysis.blech_clustering.SpikeDetection.get_energy">get_energy</a></code></li>
<li><code><a title="blechpy.analysis.blech_clustering.SpikeDetection.get_pca_waveforms" href="#blechpy.analysis.blech_clustering.SpikeDetection.get_pca_waveforms">get_pca_waveforms</a></code></li>
<li><code><a title="blechpy.analysis.blech_clustering.SpikeDetection.get_spike_amplitudes" href="#blechpy.analysis.blech_clustering.SpikeDetection.get_spike_amplitudes">get_spike_amplitudes</a></code></li>
<li><code><a title="blechpy.analysis.blech_clustering.SpikeDetection.get_spike_slopes" href="#blechpy.analysis.blech_clustering.SpikeDetection.get_spike_slopes">get_spike_slopes</a></code></li>
<li><code><a title="blechpy.analysis.blech_clustering.SpikeDetection.get_spike_times" href="#blechpy.analysis.blech_clustering.SpikeDetection.get_spike_times">get_spike_times</a></code></li>
<li><code><a title="blechpy.analysis.blech_clustering.SpikeDetection.get_spike_waveforms" href="#blechpy.analysis.blech_clustering.SpikeDetection.get_spike_waveforms">get_spike_waveforms</a></code></li>
<li><code><a title="blechpy.analysis.blech_clustering.SpikeDetection.run" href="#blechpy.analysis.blech_clustering.SpikeDetection.run">run</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="blechpy.analysis.blech_clustering.SpikeSorter" href="#blechpy.analysis.blech_clustering.SpikeSorter">SpikeSorter</a></code></h4>
<ul class="">
<li><code><a title="blechpy.analysis.blech_clustering.SpikeSorter.discard_clusters" href="#blechpy.analysis.blech_clustering.SpikeSorter.discard_clusters">discard_clusters</a></code></li>
<li><code><a title="blechpy.analysis.blech_clustering.SpikeSorter.get_mean_waveform" href="#blechpy.analysis.blech_clustering.SpikeSorter.get_mean_waveform">get_mean_waveform</a></code></li>
<li><code><a title="blechpy.analysis.blech_clustering.SpikeSorter.get_possible_solutions" href="#blechpy.analysis.blech_clustering.SpikeSorter.get_possible_solutions">get_possible_solutions</a></code></li>
<li><code><a title="blechpy.analysis.blech_clustering.SpikeSorter.merge_clusters" href="#blechpy.analysis.blech_clustering.SpikeSorter.merge_clusters">merge_clusters</a></code></li>
<li><code><a title="blechpy.analysis.blech_clustering.SpikeSorter.plot_clusters_ISI" href="#blechpy.analysis.blech_clustering.SpikeSorter.plot_clusters_ISI">plot_clusters_ISI</a></code></li>
<li><code><a title="blechpy.analysis.blech_clustering.SpikeSorter.plot_clusters_pca" href="#blechpy.analysis.blech_clustering.SpikeSorter.plot_clusters_pca">plot_clusters_pca</a></code></li>
<li><code><a title="blechpy.analysis.blech_clustering.SpikeSorter.plot_clusters_raster" href="#blechpy.analysis.blech_clustering.SpikeSorter.plot_clusters_raster">plot_clusters_raster</a></code></li>
<li><code><a title="blechpy.analysis.blech_clustering.SpikeSorter.plot_clusters_umap" href="#blechpy.analysis.blech_clustering.SpikeSorter.plot_clusters_umap">plot_clusters_umap</a></code></li>
<li><code><a title="blechpy.analysis.blech_clustering.SpikeSorter.plot_clusters_waveforms" href="#blechpy.analysis.blech_clustering.SpikeSorter.plot_clusters_waveforms">plot_clusters_waveforms</a></code></li>
<li><code><a title="blechpy.analysis.blech_clustering.SpikeSorter.save_clusters" href="#blechpy.analysis.blech_clustering.SpikeSorter.save_clusters">save_clusters</a></code></li>
<li><code><a title="blechpy.analysis.blech_clustering.SpikeSorter.set_active_clusters" href="#blechpy.analysis.blech_clustering.SpikeSorter.set_active_clusters">set_active_clusters</a></code></li>
<li><code><a title="blechpy.analysis.blech_clustering.SpikeSorter.set_split" href="#blechpy.analysis.blech_clustering.SpikeSorter.set_split">set_split</a></code></li>
<li><code><a title="blechpy.analysis.blech_clustering.SpikeSorter.split_cluster" href="#blechpy.analysis.blech_clustering.SpikeSorter.split_cluster">split_cluster</a></code></li>
<li><code><a title="blechpy.analysis.blech_clustering.SpikeSorter.undo_last_save" href="#blechpy.analysis.blech_clustering.SpikeSorter.undo_last_save">undo_last_save</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.7.1</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>
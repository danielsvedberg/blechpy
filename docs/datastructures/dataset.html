<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.7.1" />
<title>blechpy.datastructures.dataset API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>blechpy.datastructures.dataset</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import pandas as pd
import datetime as dt
import pickle
import os
import shutil
import sys
import multiprocessing
import subprocess
from tqdm import tqdm
from copy import deepcopy
from blechpy.utils import print_tools as pt, write_tools as wt, userIO
from blechpy.utils.decorators import Logger
from blechpy.analysis import palatability_analysis as pal_analysis, spike_sorting as ss, spike_analysis
from blechpy.analysis.blech_clust_process import blech_clust_process
from blechpy.plotting import palatability_plot as pal_plt, data_plot as datplt
from blechpy import dio
from blechpy.datastructures.objects import data_object


class dataset(data_object):
    &#39;&#39;&#39;Stores information related to an intan recording directory, allows
    executing basic processing and analysis scripts, and stores parameters data
    for those analyses

    Parameters
    ----------
    file_dir : str (optional)
        absolute path to a recording directory, if left empty a filechooser
        will popup
    &#39;&#39;&#39;

    def __init__(self, file_dir=None, data_name=None, shell=False):
        &#39;&#39;&#39;Initialize dataset object from file_dir, grabs basename from name of
        directory and initializes basic analysis parameters

        Parameters
        ----------
        file_dir : str (optional), file directory for intan recording data

        Throws
        ------
        ValueError
            if file_dir is not provided and no directory is chosen
            when prompted
        NotADirectoryError : if file_dir does not exist
        &#39;&#39;&#39;
        super().__init__(&#39;dataset&#39;, file_dir, data_name=data_name, shell=shell)
        h5_file = dio.h5io.get_h5_filename(self.root_dir)
        if h5_file is None:
            h5_file = os.path.join(self.root_dir, &#39;%s.h5&#39; % self.data_name)

        self.h5_file = h5_file

        self.dataset_creation_date = dt.datetime.today()

        # Outline standard processing pipeline and status check
        self.processing_steps = [&#39;initialize parameters&#39;,
                                 &#39;extract_data&#39;, &#39;create_trial_list&#39;,
                                 &#39;mark_dead_channels&#39;,
                                 &#39;common_average_reference&#39;, &#39;blech_clust_run&#39;,
                                 &#39;cleanup_clustering&#39;,
                                 &#39;sort_units&#39;, &#39;make_unit_plots&#39;,
                                 &#39;units_similarity&#39;, &#39;make_unit_arrays&#39;,
                                 &#39;make_psth_arrays&#39;, &#39;plot_psths&#39;,
                                 &#39;palatability_calculate&#39;, &#39;palatability_plot&#39;,
                                 &#39;overlay_psth&#39;]
        self.process_status = dict.fromkeys(self.processing_steps, False)

    def _change_root(self, new_root=None):
        old_root = self.root_dir
        new_root = super()._change_root(new_root)
        self.h5_file = self.h5_file.replace(old_root, new_root)
        return new_root

    @Logger(&#39;Initializing Parameters&#39;)
    def initParams(self, data_quality=&#39;clean&#39;, emg_port=None,
                   emg_channels=None, car_keyword=None,
                   car_group_areas=None,
                   shell=False, dig_in_names=None,
                   dig_out_names=None, accept_params=False):
        &#39;&#39;&#39;
        Initalizes basic default analysis parameters and allows customization
        of parameters

        Parameters (all optional)
        -------------------------
        data_quality : {&#39;clean&#39;, &#39;noisy&#39;}
            keyword defining which default set of parameters to use to detect
            headstage disconnection during clustering
            default is &#39;clean&#39;. Best practice is to run blech_clust as &#39;clean&#39;
            and re-run as &#39;noisy&#39; if too many early cutoffs occurr
        emg_port : str
            Port (&#39;A&#39;, &#39;B&#39;, &#39;C&#39;) of EMG, if there was an EMG. None (default)
            will query user. False indicates no EMG port and not to query user
        emg_channels : list of int
            channel or channels of EMGs on port specified
            default is None
        car_keyword : str
            Specifes default common average reference groups
            defaults are found in CAR_defaults.json
            Currently &#39;bilateral32&#39; is only keyword available
            If left as None (default) user will be queries to select common
            average reference groups
        shell : bool
            False (default) for GUI. True for command-line interface
        dig_in_names : list of str
            Names of digital inputs. Must match number of digital inputs used
            in recording.
            None (default) queries user to name each dig_in
        dig_out_names : list of str
            Names of digital outputs. Must match number of digital outputs in
            recording.
            None (default) queries user to name each dig_out
        accept_params : bool
            True automatically accepts default parameters where possible,
            decreasing user queries
            False (default) will query user to confirm or edit parameters for
            clustering, spike array and psth creation and palatability/identity
            calculations
        &#39;&#39;&#39;
        # Get parameters from info.rhd
        file_dir = self.root_dir
        rec_info = dio.rawIO.read_rec_info(file_dir)
        ports = rec_info.pop(&#39;ports&#39;)
        channels = rec_info.pop(&#39;channels&#39;)
        sampling_rate = rec_info[&#39;amplifier_sampling_rate&#39;]
        self.rec_info = rec_info
        self.sampling_rate = sampling_rate

        # Get default parameters from files
        clustering_params = dio.params.load_params(&#39;clustering_params&#39;, file_dir,
                                                   default_keyword=data_quality)
        spike_array_params = dio.params.load_params(&#39;spike_array_params&#39;, file_dir)
        psth_params = dio.params.load_params(&#39;psth_params&#39;, file_dir)
        pal_id_params = dio.params.load_params(&#39;pal_id_params&#39;, file_dir)
        spike_array_params[&#39;sampling_rate&#39;] = sampling_rate
        clustering_params[&#39;file_dir&#39;] = file_dir
        clustering_params[&#39;sampling_rate&#39;] = sampling_rate

        # Setup digital input mapping
        #TODO: Setup digital output mapping...ignoring for now
        if rec_info.get(&#39;dig_in&#39;):
            self._setup_din_mapping(dig_in_names, shell)
            dim = self.dig_in_mapping.copy()
            spike_array_params[&#39;laser_channels&#39;] = dim.channel[dim[&#39;laser&#39;]].to_list()
            spike_array_params[&#39;dig_ins_to_use&#39;] = dim.channel[dim[&#39;spike_array&#39;]].to_list()
        else:
            self.dig_in_mapping = None

        # Setup electrode and emg mapping
        self._setup_channel_mapping(ports, channels, emg_port,
                                    emg_channels, shell=shell)

        # Set CAR groups
        self._set_CAR_groups(group_keyword=car_keyword, group_areas=car_group_areas, shell=shell)

        # Confirm parameters
        self.spike_array_params = spike_array_params
        if not accept_params:
            conf = userIO.confirm_parameter_dict
            clustering_params = conf(clustering_params,
                                     &#39;Clustering Parameters&#39;, shell=shell)
            self.edit_spike_array_params(shell=shell)
            psth_params = conf(psth_params,
                               &#39;PSTH Parameters&#39;, shell=shell)
            pal_id_params = conf(pal_id_params,
                                 &#39;Palatability/Identity Parameters\n&#39;
                                 &#39;Valid unit_type is Single, Multi or All&#39;,
                                 shell=shell)

        # Store parameters
        self.clustering_params = clustering_params
        self.pal_id_params = pal_id_params
        self.psth_params = psth_params
        self._write_all_params_to_json()
        self.process_status[&#39;initialize parameters&#39;] = True
        self.save()

    def _set_CAR_groups(self, group_keyword=None, shell=False, group_areas=None):
        &#39;&#39;&#39;Sets that electrode groups for common average referencing and
        defines which brain region electrodes eneded up in

        Parameters
        ----------
        group_keyword : str or int
            Keyword corresponding to a preset electrode grouping in CAR_params.json
            Or integer indicating number of CAR groups
        shell : bool
            True for command-line interface, False (default) for GUI
        &#39;&#39;&#39;
        if not hasattr(self, &#39;electrode_mapping&#39;):
            raise ValueError(&#39;Set electrode mapping before setting CAR groups&#39;)

        em = self.electrode_mapping.copy()

        car_param_file = os.path.join(self.root_dir, &#39;analysis_params&#39;,
                                      &#39;CAR_params.json&#39;)
        if os.path.isfile(car_param_file):
            tmp = dio.params.load_params(&#39;CAR_params&#39;, self.root_dir)
            if tmp is not None:
                group_electrodes = tmp
            else:
                raise ValueError(&#39;CAR_params file exists in recording dir, but is empty&#39;)

        else:
            if group_keyword is None:
                group_keyword = userIO.get_user_input(
                    &#39;Input keyword for CAR parameters or number of CAR groups&#39;,
                    shell=shell)

                if group_keyword is None:
                    ValueError(&#39;Must provide a keyword or number of groups&#39;)

            if group_keyword.isnumeric():
                num_groups = int(group_keyword)
                group_electrodes = dio.params.select_CAR_groups(num_groups, em,
                                                                shell=shell)
            else:
                group_electrodes = dio.params.load_params(&#39;CAR_params&#39;,
                                                          self.root_dir,
                                                          default_keyword=group_keyword)

        num_groups = len(group_electrodes)
        if group_areas is not None and len(group_areas) == num_groups:
            for i, x in enumerate(zip(group_electrodes, group_areas)):
                em.loc[x[0], &#39;area&#39;] = x[1]
                em.loc[x[0], &#39;CAR_group&#39;] = i

        else:
            group_names = [&#39;Group %i&#39; % i for i in range(num_groups)]
            area_dict = dict.fromkeys(group_names, &#39;&#39;)
            area_dict = userIO.fill_dict(area_dict, &#39;Set Areas for CAR groups&#39;,
                                         shell=shell)
            for k, v in area_dict.items():
                i = int(k.replace(&#39;Group&#39;, &#39;&#39;))
                em.loc[group_electrodes[i], &#39;area&#39;] = v
                em.loc[group_electrodes[i], &#39;CAR_group&#39;] = i

        self.CAR_electrodes = group_electrodes
        self.electrode_mapping = em.copy()

    def _setup_din_mapping(self, dig_in_names=None, shell=False):
        &#39;&#39;&#39;sets up dig_in_mapping dataframe  and queries user to fill in columns

        Parameters
        ----------
        dig_in_names : list of str (optional)
        shell : bool (optional)
            True for command-line interface
            False (default) for GUI
        &#39;&#39;&#39;
        rec_info = self.rec_info
        df = pd.DataFrame()
        df[&#39;channel&#39;] = rec_info.get(&#39;dig_in&#39;)
        n_dig_in = len(df)
        # Names
        if dig_in_names:
            df[&#39;name&#39;] = dig_in_names
        else:
            df[&#39;name&#39;] = &#39;&#39;

        # Parameters to query
        df[&#39;palatability_rank&#39;] = 0
        df[&#39;laser&#39;] = False
        df[&#39;spike_array&#39;] = True
        df[&#39;exclude&#39;] = False
        # Re-format for query
        idx = df.index
        df.index = [&#39;dig_in_%i&#39; % x for x in df.channel]
        # Query for user input
        prompt = (&#39;Digital Input Parameters\nSet palatability ranks from 1 to %i&#39;
                  &#39;\nor blank to exclude from pal_id analysis&#39;) % len(df)
        tmp = userIO.fill_dict(df.to_dict(), prompt=prompt, shell=shell)
        # Reformat for storage
        df2 = pd.DataFrame.from_dict(tmp)
        df2 = df2.sort_values(by=[&#39;channel&#39;])
        df2.index = idx
        df2[&#39;palatability_rank&#39;] = df2[&#39;palatability_rank&#39;].fillna(-1).astype(&#39;int&#39;)
        self.dig_in_mapping = df2.copy()
        if os.path.isfile(self.h5_file):
            dio.h5io.write_digital_map_to_h5(self.h5_file, self.dig_in_mapping, &#39;in&#39;)

    def _setup_channel_mapping(self, ports, channels, emg_port, emg_channels, shell=False):
        &#39;&#39;&#39;Creates electrode_mapping and emg_mapping DataFrames with columns:
        - Electrode
        - Port
        - Channel

        Parameters
        ----------
        ports : list of str, item corresponing to each channel
        channels : list of int, channels on each port
        emg_port : str
        emg_channels : list of int
        &#39;&#39;&#39;
        if emg_port is None:
            q = userIO.ask_user(&#39;Do you have an EMG?&#39;, shell=shell)
            if q==1:
                emg_port = userIO.select_from_list(&#39;Select EMG Port:&#39;,
                                                   ports, &#39;EMG Port&#39;,
                                                   shell=shell)
                emg_channels = userIO.select_from_list(
                    &#39;Select EMG Channels:&#39;,
                    [y for x, y in
                     zip(ports, channels)
                     if x == emg_port],
                    title=&#39;EMG Channels&#39;,
                    multi_select=True, shell=shell)

        el_map, em_map = dio.params.flatten_channels(ports, channels,
                                                     emg_port, emg_channels)
        self.electrode_mapping = el_map
        self.emg_mapping = em_map
        if os.path.isfile(self.h5_file):
            dio.h5io.write_electrode_map_to_h5(self.h5_file, self.electrode_mapping)

    def edit_spike_array_params(self, shell=False):
        &#39;&#39;&#39;Edit spike array parameters and adjust dig_in_mapping accordingly

        Parameters
        ----------
        shell : bool, whether to use CLI or GUI
        &#39;&#39;&#39;
        if not hasattr(self, &#39;dig_in_mapping&#39;):
            self.spike_array_params = None
            return

        sa = deepcopy(self.spike_array_params)
        tmp = userIO.fill_dict(sa, &#39;Spike Array Parameters\n(Times in ms)&#39;,
                               shell=shell)
        if tmp is None:
            return

        dim = self.dig_in_mapping
        dim[&#39;spike_array&#39;] = False
        if tmp[&#39;dig_ins_to_use&#39;] != [&#39;&#39;]:
            tmp[&#39;dig_ins_to_use&#39;] = [int(x) for x in tmp[&#39;dig_ins_to_use&#39;]]
            dim.loc[[x in tmp[&#39;dig_ins_to_use&#39;] for x in dim.channel],
                    &#39;spike_array&#39;] = True

        dim[&#39;laser_channels&#39;] = False
        if tmp[&#39;laser_channels&#39;] != [&#39;&#39;]:
            tmp[&#39;laser_channels&#39;] = [int(x) for x in tmp[&#39;laser_channels&#39;]]
            dim.loc[[x in tmp[&#39;laser_channels&#39;] for x in dim.channel],
                    &#39;laser&#39;] = True

        self.spike_array_params = tmp.copy()
        wt.write_params_to_json(&#39;spike_array_params&#39;,
                                self.root_dir, tmp)

    def edit_clustering_params(self, shell=False):
        &#39;&#39;&#39;Allows user interface for editing clustering parameters

        Parameters
        ----------
        shell : bool (optional)
            True if you want command-line interface, False for GUI (default)
        &#39;&#39;&#39;
        tmp = userIO.fill_dict(self.clustering_params,
                               &#39;Clustering Parameters\n(Times in ms)&#39;,
                               shell=shell)
        if tmp:
            self.clustering_params = tmp
            wt.write_params_to_json(&#39;clustering_params&#39;, self.root_dir, tmp)

    def edit_psth_params(self, shell=False):
        &#39;&#39;&#39;Allows user interface for editing psth parameters

        Parameters
        ----------
        shell : bool (optional)
            True if you want command-line interface, False for GUI (default)
        &#39;&#39;&#39;
        tmp = userIO.fill_dict(self.psth_params,
                               &#39;PSTH Parameters\n(Times in ms)&#39;,
                               shell=shell)
        if tmp:
            self.psth_params = tmp
            wt.write_params_to_json(&#39;psth_params&#39;, self.root_dir, tmp)

    def edit_pal_id_params(self, shell=False):
        &#39;&#39;&#39;Allows user interface for editing palatability/identity parameters

        Parameters
        ----------
        shell : bool (optional)
            True if you want command-line interface, False for GUI (default)
        &#39;&#39;&#39;
        tmp = userIO.fill_dict(self.pal_id_params,
                               &#39;Palatability/Identity Parameters\n(Times in ms)&#39;,
                               shell=shell)
        if tmp:
            self.pal_id_params = tmp
            wt.write_params_to_json(&#39;pal_id_params&#39;, self.root_dir, tmp)

    def __str__(self):
        &#39;&#39;&#39;Put all information about dataset in string format

        Returns
        -------
        str : representation of dataset object
        &#39;&#39;&#39;
        out1 = super().__str__()
        out = [out1]
        out.append(&#39;\nObject creation date: &#39;
                   + self.dataset_creation_date.strftime(&#39;%m/%d/%y&#39;))

        if hasattr(self, &#39;raw_h5_file&#39;):
            out.append(&#39;Deleted Raw h5 file: &#39;+self.raw_h5_file)

        out.append(&#39;h5 File: &#39;+self.h5_file)
        out.append(&#39;&#39;)

        out.append(&#39;--------------------&#39;)
        out.append(&#39;Processing Status&#39;)
        out.append(&#39;--------------------&#39;)
        out.append(pt.print_dict(self.process_status))
        out.append(&#39;&#39;)

        if not hasattr(self, &#39;rec_info&#39;):
            return &#39;\n&#39;.join(out)

        info = self.rec_info

        out.append(&#39;--------------------&#39;)
        out.append(&#39;Recording Info&#39;)
        out.append(&#39;--------------------&#39;)
        out.append(pt.print_dict(self.rec_info))
        out.append(&#39;&#39;)

        out.append(&#39;--------------------&#39;)
        out.append(&#39;Electrodes&#39;)
        out.append(&#39;--------------------&#39;)
        out.append(pt.print_dataframe(self.electrode_mapping))
        out.append(&#39;&#39;)

        if hasattr(self, &#39;CAR_electrodes&#39;):
            out.append(&#39;--------------------&#39;)
            out.append(&#39;CAR Groups&#39;)
            out.append(&#39;--------------------&#39;)
            headers = [&#39;Group %i&#39; % x for x in range(len(self.CAR_electrodes))]
            out.append(pt.print_list_table(self.CAR_electrodes, headers))
            out.append(&#39;&#39;)

        if not self.emg_mapping.empty:
            out.append(&#39;--------------------&#39;)
            out.append(&#39;EMG&#39;)
            out.append(&#39;--------------------&#39;)
            out.append(pt.print_dataframe(self.emg_mapping))
            out.append(&#39;&#39;)

        if info.get(&#39;dig_in&#39;):
            out.append(&#39;--------------------&#39;)
            out.append(&#39;Digital Input&#39;)
            out.append(&#39;--------------------&#39;)
            out.append(pt.print_dataframe(self.dig_in_mapping))
            out.append(&#39;&#39;)

        if info.get(&#39;dig_out&#39;):
            out.append(&#39;--------------------&#39;)
            out.append(&#39;Digital Output&#39;)
            out.append(&#39;--------------------&#39;)
            out.append(pt.print_dataframe(self.dig_out_mapping))
            out.append(&#39;&#39;)

        out.append(&#39;--------------------&#39;)
        out.append(&#39;Clustering Parameters&#39;)
        out.append(&#39;--------------------&#39;)
        out.append(pt.print_dict(self.clustering_params))
        out.append(&#39;&#39;)

        out.append(&#39;--------------------&#39;)
        out.append(&#39;Spike Array Parameters&#39;)
        out.append(&#39;--------------------&#39;)
        out.append(pt.print_dict(self.spike_array_params))
        out.append(&#39;&#39;)

        out.append(&#39;--------------------&#39;)
        out.append(&#39;PSTH Parameters&#39;)
        out.append(&#39;--------------------&#39;)
        out.append(pt.print_dict(self.psth_params))
        out.append(&#39;&#39;)

        out.append(&#39;--------------------&#39;)
        out.append(&#39;Palatability/Identity Parameters&#39;)
        out.append(&#39;--------------------&#39;)
        out.append(pt.print_dict(self.pal_id_params))
        out.append(&#39;&#39;)

        return &#39;\n&#39;.join(out)

    @Logger(&#39;Writing parameters to JSON&#39;)
    def _write_all_params_to_json(self):
        &#39;&#39;&#39;Writes all parameters to json files in analysis_params folder in the
        recording directory
        &#39;&#39;&#39;
        print(&#39;Writing all parameters to json file in analysis_params folder...&#39;)
        clustering_params = self.clustering_params
        spike_array_params = self.spike_array_params
        psth_params = self.psth_params
        pal_id_params = self.pal_id_params
        CAR_params = self.CAR_electrodes

        rec_dir = self.root_dir
        wt.write_params_to_json(&#39;clustering_params&#39;, rec_dir, clustering_params)
        wt.write_params_to_json(&#39;spike_array_params&#39;, rec_dir, spike_array_params)
        wt.write_params_to_json(&#39;psth_params&#39;, rec_dir, psth_params)
        wt.write_params_to_json(&#39;pal_id_params&#39;, rec_dir, pal_id_params)
        wt.write_params_to_json(&#39;CAR_params&#39;, rec_dir, CAR_params)

    @Logger(&#39;Extracting Data&#39;)
    def extract_data(self, filename=None, shell=False):
        &#39;&#39;&#39;Create hdf5 store for data and read in Intan .dat files. Also create
        subfolders for processing outputs

        Parameters
        ----------
        data_quality: {&#39;clean&#39;, &#39;noisy&#39;} (optional)
            Specifies quality of data for default clustering parameters
            associated. Should generally first process with clean (default)
            parameters and then try noisy after running blech_clust and
            checking if too many electrodes as cutoff too early
        &#39;&#39;&#39;
        if self.rec_info[&#39;file_type&#39;] is None:
            raise ValueError(&#39;Unsupported recording type. Cannot extract yet.&#39;)

        if filename is None:
            filename = self.h5_file

        print(&#39;\nExtract Intan Data\n--------------------&#39;)
        # Create h5 file
        tmp = dio.h5io.create_empty_data_h5(filename, shell)
        if tmp is None:
            return

        # Create arrays for raw data in hdf5 store
        dio.h5io.create_hdf_arrays(filename, self.rec_info,
                                   self.electrode_mapping, self.emg_mapping)

        # Read in data to arrays
        dio.h5io.read_files_into_arrays(filename,
                                        self.rec_info,
                                        self.electrode_mapping,
                                        self.emg_mapping)

        # Write electrode and digital input mapping into h5 file
        # TODO: write EMG and digital output mapping into h5 file
        dio.h5io.write_electrode_map_to_h5(self.h5_file, self.electrode_mapping)
        if self.dig_in_mapping is not None:
            dio.h5io.write_digital_map_to_h5(self.h5_file, self.dig_in_mapping, &#39;in&#39;)

        # update status
        self.h5_file = filename
        self.process_status[&#39;extract_data&#39;] = True
        self.save()
        print(&#39;\nData Extraction Complete\n--------------------&#39;)

    @Logger(&#39;Creating Trial List&#39;)
    def create_trial_list(self):
        &#39;&#39;&#39;Create lists of trials based on digital inputs and outputs and store
        to hdf5 store
        Can only be run after data extraction
        &#39;&#39;&#39;
        if not self.process_status[&#39;extract_data&#39;]:
            userIO.tell_user(&#39;Must extract data before creating trial list&#39;,shell=True)
            return

        if self.rec_info.get(&#39;dig_in&#39;):
            in_list = dio.h5io.create_trial_data_table(
                self.h5_file,
                self.dig_in_mapping,
                self.sampling_rate,
                &#39;in&#39;)
            self.dig_in_trials = in_list
        else:
            print(&#39;No digital input data found&#39;)

        if self.rec_info.get(&#39;dig_out&#39;):
            out_list = dio.h5io.create_trial_data_table(
                self.h5_file,
                self.dig_out_mapping,
                self.sampling_rate,
                &#39;out&#39;)
            self.dig_out_trials = out_list
        else:
            print(&#39;No digital output data found&#39;)

        self.process_status[&#39;create_trial_list&#39;] = True
        self.save()

    @Logger(&#39;Marking Dead Channels&#39;)
    def mark_dead_channels(self, dead_channels=None, shell=False):
        &#39;&#39;&#39;Plots small piece of raw traces and a metric to help identify dead
        channels. Once user marks channels as dead a new column is added to
        electrode mapping

        Parameters
        ----------
        dead_channels : list of int, optional
            if this is specified then nothing is plotted, those channels are
            simply marked as dead
        shell : bool, optional
        &#39;&#39;&#39;
        print(&#39;Marking dead channels\n----------&#39;)
        em = self.electrode_mapping.copy()
        if dead_channels is None:
            userIO.tell_user(&#39;Making traces figure for dead channel detection...&#39;,
                             shell=True)
            save_file = os.path.join(self.root_dir, &#39;Electrode_Traces.png&#39;)
            fig, ax = datplt.plot_traces_and_outliers(self.h5_filei, save_file=save_file)
            if not shell:
                # Better to open figure outside of python since its a lot of
                # data on figure and matplotlib is slow
                subprocess.call([&#39;xdg-open&#39;, save_file])
            else:
                userIO.tell_user(&#39;Saved figure of traces to %s for reference&#39;
                                 % save_file, shell=shell)

            choice = userIO.select_from_list(&#39;Select dead channels:&#39;,
                                             em.Electrode.to_list(),
                                             &#39;Dead Channel Selection&#39;,
                                             multi_select=True,
                                             shell=shell)
            dead_channels = list(map(int, choice))

        print(&#39;Marking eletrodes %s as dead.\n&#39;
              &#39;They will be excluded from common average referencing.&#39;
              % dead_channels)
        em[&#39;dead&#39;] = False
        em.loc[dead_channels, &#39;dead&#39;] = True
        self.electrode_mapping = em
        if os.path.isfile(self.h5_file):
            dio.h5io.write_electrode_map_to_h5(self.h5_file, self.electrode_mapping)

        self.process_status[&#39;mark_dead_channels&#39;] = True
        self.save()
        return dead_channels

    @Logger(&#39;Common Average Referencing&#39;)
    def common_average_reference(self):
        &#39;&#39;&#39;Define electrode groups and remove common average from  signals

        Parameters
        ----------
        num_groups : int (optional)
            number of CAR groups, if not provided
            there&#39;s a prompt
        &#39;&#39;&#39;
        if not hasattr(self, &#39;CAR_electrodes&#39;):
            raise ValueError(&#39;CAR_electrodes not set&#39;)

        if not hasattr(self, &#39;electrode_mapping&#39;):
            raise ValueError(&#39;electrode_mapping not set&#39;)

        car_electrodes = self.CAR_electrodes
        num_groups = len(car_electrodes)
        em = self.electrode_mapping.copy()

        if &#39;dead&#39; in em.columns:
            dead_electrodes = em.Electrode[em.dead].to_list()
        else:
            dead_electrodes = []

        # Gather Common Average Reference Groups
        print(&#39;CAR Groups\n&#39;)
        headers = [&#39;Group %i&#39; % x for x in range(num_groups)]
        print(pt.print_list_table(car_electrodes, headers))

        # Reference each group
        for i, x in enumerate(car_electrodes):
            tmp = list(set(x) - set(dead_electrodes))
            dio.h5io.common_avg_reference(self.h5_file, tmp, i)

        # Compress and repack file
        dio.h5io.compress_and_repack(self.h5_file)

        self.process_status[&#39;common_average_reference&#39;] = True
        self.save()

    @Logger(&#39;Running Blech Clust&#39;)
    def blech_clust_run(self, data_quality=None, n_cores=None):
        &#39;&#39;&#39;Write clustering parameters to file and
        Run blech_process on each electrode using GNU parallel

        Parameters
        ----------
        data_quality : {&#39;clean&#39;, &#39;noisy&#39;, None (default)}
            set if you want to change the data quality parameters for cutoff
            and spike detection before running clustering. These parameters are
            automatically set as &#34;clean&#34; during initial parameter setup
        accept_params : bool, False (default)
            set to True in order to skip popup confirmation of parameters when
            running
        &#39;&#39;&#39;
        if data_quality:
            tmp = dio.params.load_params(&#39;clustering_params&#39;, self.root_dir,
                                         default_keyword=data_quality)
            if tmp:
                self.clustering_params = tmp
            else:
                raise ValueError(&#39;%s is not a valid data_quality preset. Must &#39;
                                 &#39;be &#34;clean&#34; or &#34;noisy&#34; or None.&#39;)

        print(&#39;\nRunning Blech Clust\n-------------------&#39;)
        print(&#39;Parameters\n%s&#39; % pt.print_dict(self.clustering_params))

        # Create folders for saving things within recording dir
        data_dir = self.root_dir
        directories = [&#39;spike_waveforms&#39;, &#39;spike_times&#39;,
                       &#39;clustering_results&#39;,
                       &#39;Plots&#39;, &#39;memory_monitor_clustering&#39;]
        for d in directories:
            tmp_dir = os.path.join(data_dir, d)
            if os.path.exists(tmp_dir):
                shutil.rmtree(tmp_dir)

            os.mkdir(tmp_dir)

        # Set file for clusting log
        self.clustering_log = os.path.join(data_dir, &#39;results.log&#39;)
        if os.path.exists(self.clustering_log):
            os.remove(self.clustering_log)

        process_path = os.path.realpath(__file__)
        process_path = os.path.join(os.path.dirname(process_path),
                                    &#39;blech_process.py&#39;)
        em = self.electrode_mapping
        if &#39;dead&#39; in em.columns:
            electrodes = em.Electrode[em[&#39;dead&#39;] == False].tolist()
        else:
            electrodes = em.Electrode.tolist()


        pbar = tqdm(total = len(electrodes))
        results = [(None, None, None)] * (max(electrodes)+1)
        clust_errors = [(x, None) for x in electrodes]
        def update_pbar(ans):
            if isinstance(ans, tuple) and ans[0] is not None:
                results[ans[0]] = ans
            else:
                print(&#39;Unexpected error when clustering an electrode&#39;)

            pbar.update()

        if cores is None or cores &gt; multiprocessing.cpu_count():
            cores = multiprocessing.cpu_count() - 1

        pool = multiprocessing.Pool(cores)
        for x in electrodes:
            pool.apply_async(blech_clust_process,
                             args=(x, data_dir, self.clustering_params),
                             callback=update_pbar)

        pool.close()
        pool.join()
        pbar.close()

        print(&#39;Electrode    Result    Cutoff (s)&#39;)
        cutoffs = {}
        clust_res = {}
        clustered = []
        for x, y, z in results:
            if x is None:
                continue

            clustered.append(x)
            print(&#39;  {:&lt;13}{:&lt;10}{}&#39;.format(x, y, z))
            cutoffs[x] = z
            clust_res[x] = y

        print(&#39;1 - Sucess\n0 - No data or no spikes\n-1 - Error&#39;)

        em = self.electrode_mapping.copy()
        em[&#39;cutoff_time&#39;] = em[&#39;Electrode&#39;].map(cutoffs)
        em[&#39;clustering_result&#39;] = em[&#39;Electrode&#39;].map(clust_res)
        self.electrode_mapping = em.copy()
        self.process_status[&#39;blech_clust_run&#39;] = True
        self.process_status[&#39;cleanup_clustering&#39;] = False
        dio.h5io.write_electrode_map_to_h5(self.h5_file, em)
        self.save()
        print(&#39;Clustering Complete\n------------------&#39;)
        return results

    @Logger(&#39;Cleaning up clustering memory logs. Removing raw data and setting&#39;
            &#39;up hdf5 for unit sorting&#39;)
    def cleanup_clustering(self):
        &#39;&#39;&#39;Consolidates memory monitor files, removes raw and referenced data
        and setups up hdf5 store for sorted units data
        &#39;&#39;&#39;
        h5_file = dio.h5io.cleanup_clustering(self.root_dir)
        self.h5_file = h5_file
        self.process_status[&#39;cleanup_clustering&#39;] = True
        self.save()

    def sort_units(self, shell=False):
        &#39;&#39;&#39;Begins processes to allow labelling of clusters as sorted units

        Parameters
        ----------
        shell : bool
            True if command-line interfaced desired, False for GUI (default)
        &#39;&#39;&#39;
        fs = self.sampling_rate
        ss.sort_units(self.root_dir, fs, shell)
        self.process_status[&#39;sort_units&#39;] = True
        self.save()

    @Logger(&#39;Calculating Units Similarity&#39;)
    def units_similarity(self, similarity_cutoff=50, shell=False):
        if &#39;SSH_CONNECTION&#39; in os.environ:
            shell= True

        metrics_dir = os.path.join(self.root_dir, &#39;sorted_unit_metrics&#39;)
        if not os.path.isdir(metrics_dir):
            raise ValueError(&#39;No sorted unit metrics found. Must sort units before calculating similarity&#39;)

        violation_file = os.path.join(metrics_dir,
                                      &#39;units_similarity_violations.txt&#39;)
        violations, sim = ss.calc_units_similarity(self.h5_file,
                                                   self.sampling_rate,
                                                   similarity_cutoff,
                                                   violation_file)
        if len(violations) == 0:
            userIO.tell_user(&#39;No similarity violations found!&#39;, shell=shell)
            self.process_status[&#39;units_similarity&#39;] = True
            return violations, sim

        out_str = [&#39;Units Similarity Violations Found:&#39;]
        out_str.append(&#39;Unit_1    Unit_2    Similarity&#39;)
        for x,y in violations:
            u1 = dio.h5io.parse_unit_number(x)
            u2 = dio.h5io.parse_unit_number(y)
            out_str.append(&#39;   {:&lt;10}{:&lt;10}{}\n&#39;.format(x, y, sim[u1][u2]))

        out_str.append(&#39;Delete units with dataset.delete_unit(N)&#39;)
        out_str = &#39;\n&#39;.join(out_str)
        userIO.tell_user(out_str, shell=shell)
        self.process_status[&#39;units_similarity&#39;] = True
        self.save()
        return violations, sim

    @Logger(&#39;Deleting Unit&#39;)
    def delete_unit(self, unit_num, shell=False):
        if isinstance(unit_num, str):
            unit_num = dio.h5io.parse_unit_number(unit_num)

        if unit_num is None:
            print(&#39;No unit deleted&#39;)
            return

        q = userIO.ask_user(&#39;Are you sure you want to delete unit%03i?&#39; % unit_num,
                            choices = [&#39;No&#39;,&#39;Yes&#39;], shell=shell)
        if q == 0:
            print(&#39;No unit deleted&#39;)
            return

        else:
            tmp = ss.delete_unit(self.root_dir, unit_num)
            if tmp is False:
                userIO.tell_user(&#39;Unit %i not found in dataset. No unit deleted&#39;
                                 % unit_num, shell=shell)
            else:
                userIO.tell_user(&#39;Unit %i sucessfully deleted.&#39; % unit_num,
                                 shell=shell)

        self.save()

    @Logger(&#39;Making Unit Arrays&#39;)
    def make_unit_arrays(self):
        &#39;&#39;&#39;Make spike arrays for each unit and store in hdf5 store
        &#39;&#39;&#39;
        params = self.spike_array_params

        print(&#39;Generating unit arrays with parameters:\n----------&#39;)
        print(pt.print_dict(params, tabs=1))
        ss.make_spike_arrays(self.h5_file, params)
        self.process_status[&#39;make_unit_arrays&#39;] = True
        self.save()

    @Logger(&#39;Making Unit Plots&#39;)
    def make_unit_plots(self):
        &#39;&#39;&#39;Make waveform plots for each sorted unit
        &#39;&#39;&#39;
        unit_table = self.get_unit_table()
        save_dir = os.path.join(self.root_dir, &#39;unit_waveforms_plots&#39;)
        if os.path.isdir(save_dir):
            shutil.rmtree(save_dir)

        os.mkdir(save_dir)
        for i, row in unit_table.iterrows():
            datplt.make_unit_plots(self.root_dir, row[&#39;unit_name&#39;], save_dir=save_dir)

        self.process_status[&#39;make_unit_plots&#39;] = True
        self.save()

    @Logger(&#39;Making PSTH Arrays&#39;)
    def make_psth_arrays(self):
        &#39;&#39;&#39;Make smoothed firing rate traces for each unit/trial and store in
        hdf5 store
        &#39;&#39;&#39;
        params = self.psth_params
        dig_ins = self.dig_in_mapping
        for idx, row in dig_ins.iterrows():
            spike_analysis.make_psths_for_tastant(self.h5_file,
                                                  params[&#39;window_size&#39;],
                                                  params[&#39;window_step&#39;],
                                                  row[&#39;channel&#39;])

        self.process_status[&#39;make_psth_arrays&#39;] = True
        self.save()

    @Logger(&#39;Calculating Palatability/Identity Metrics&#39;)
    def palatability_calculate(self, shell=False):
        pal_analysis.palatability_identity_calculations(self.root_dir,
                                                        params=self.pal_id_params)
        self.process_status[&#39;palatability_calculate&#39;] = True
        self.save()

    @Logger(&#39;Plotting Palatability/Identity Metrics&#39;)
    def palatability_plot(self, shell=False):
        pal_plt.plot_palatability_identity([self.root_dir], shell=shell)
        self.process_status[&#39;palatability_plot&#39;] = True
        self.save()

    def get_unit_table(self):
        &#39;&#39;&#39;Returns a pandas dataframe with sorted unit information

        Returns
        --------
        pandas.DataFrame with columns:
            unit_name, unit_num, electrode, single_unit,
            regular_spiking, fast_spiking
        &#39;&#39;&#39;
        unit_table = dio.h5io.get_unit_table(self.root_dir)
        return unit_table

    def extract_and_cluster(self, shell=False):
        pass

    def post_sorting(self):
        self.make_unit_plots()
        self.make_unit_arrays()
        self.units_similarity(shell=True)
        self.make_psth_arrays()


def port_in_dataset(rec_dir=None, shell=False):
    &#39;&#39;&#39;Import an existing dataset into this framework
    &#39;&#39;&#39;
    if rec_dir is None:
        rec_dir = userIO.get_filedirs(&#39;Select recording directory&#39;, shell=shell)
        if rec_dir is None:
            return None

    dat = dataset(rec_dir, shell=shell)
    # Check files that will be overwritten: log_file, save_file
    if os.path.isfile(dat.save_file):
        prompt = &#39;%s already exists. Continuing will overwrite this. Continue?&#39; % dat.save_file
        q = userIO.ask_user(prompt, shell=shell)
        if q == 0:
            print(&#39;Aborted&#39;)
            return None

    # if os.path.isfile(dat.h5_file):
    #     prompt = &#39;%s already exists. Continuinlg will overwrite this. Continue?&#39; % dat.h5_file
    #     q = userIO.ask_user(prompt, shell=shell)
    #     if q == 0:
    #         print(&#39;Aborted&#39;)
    #         return None

    if os.path.isfile(dat.log_file):
        prompt = &#39;%s already exists. Continuing will append to this. Continue?&#39; % dat.log_file
        q = userIO.ask_user(prompt, shell=shell)
        if q == 0:
            print(&#39;Aborted&#39;)
            return None

    with open(dat.log_file, &#39;a&#39;) as f:
        print(&#39;\n==========\nPorting dataset into blechpy format\n==========\n&#39;, file=f)
        print(dat, file=f)

    status = dat.process_status
    dat.initParams(shell=shell)


    user_status = status.copy()
    user_status = userIO.fill_dict(user_status,
                                   &#39;Which processes have already been &#39;
                                   &#39;done to the data?&#39;, shell=shell)

    status.update(user_status)
    # if h5 exists data must have been extracted

    if not os.path.isfile(dat.h5_file) or status[&#39;extract_data&#39;] == False:
        dat.save()
        return dat

    # write eletrode map and digital input &amp; output maps to hf5
    dio.h5io.write_electrode_map_to_h5(dat.h5_file, dat.electrode_mapping)
    if dat.rec_info.get(&#39;dig_in&#39;) is not None:
        dio.h5io.write_digital_map_to_h5(dat.h5_file, dat.dig_in_mapping, &#39;in&#39;)

    if dat.rec_info.get(&#39;dig_out&#39;) is not None:
        dio.h5io.write_digital_map_to_h5(dat.h5_file, dat.dig_out_mapping, &#39;out&#39;)


    node_list = dio.h5io.get_node_list(dat.h5_file)

    if (status[&#39;create_trial_list&#39;] == False) and (&#39;digital_in&#39; in node_list):
        dat.create_trial_list()

    dat.save()
    return dat


def validate_data_integrity(rec_dir, verbose=False):
    print(&#39;Raw Data Validation\n&#39; + &#39;-&#39;*19)
    test_names = [&#39;file_type&#39;, &#39;recording_info&#39;, &#39;files&#39;, &#39;dropped_packets&#39;, &#39;data_length&#39;]
    number_names = [&#39;sample_rate&#39;, &#39;dropped_packets&#39;, &#39;missing_files&#39;, &#39;recording_length&#39;]
    tests = dict.fromkeys(test_names, &#39;NOT TESTED&#39;)
    numbers = dict.fromkeys(number_names, -1)
    file_type = dio.rawIO.get_recording_filetype(rec_dir)
    if file_type is None:
        file_type_check = &#39;UNSUPPORTED&#39;
    else:
        tests[&#39;file_type&#39;] = &#39;PASS&#39;

    # Check info.rhd integrity
    info_file = os.path.join(rec_dir, &#39;info.rhd&#39;)
    try:
        rec_info = dio.rawIO.read_rec_info(rec_dir, shell=True)
        with open(info_file, &#39;rb&#39;) as f:
            info = dio.load_intan_rhd_format.read_header(f)

        tests[&#39;recording_info&#39;] = &#39;PASS&#39;
    except FileNotFoundError:
        test[&#39;recording_info&#39;] = &#39;MISSING&#39;
    except Exception as e:
        info_size = os.path.getsize(os.path.join(rec_dir, &#39;info.rhd&#39;))
        if info_size == 0:
            tests[&#39;recording_info&#39;] = &#39;EMPTY&#39;
        else:
            tests[&#39;recording_info&#39;] = &#39;FAIL&#39;

        print(pt.print_dict(tests, tabs=1))
        return tests, numbers

    counts = {x : info(x) for x in info.keys() if &#39;num&#39; in x}
    numbers.update(counts)
    fs = info[&#39;sample_rate&#39;]
    # Check all files needed are present
    files_expected = [&#39;time.dat&#39;]
    if file_type == &#39;one file per signal type&#39;:
        files_expected.append(&#39;amplifier.dat&#39;)
        if rec_info.get(&#39;dig_in&#39;) is not None:
            files_expected.append(&#39;digitalin.dat&#39;)

        if rec_info.get(&#39;dig_out&#39;) is not None:
            files_expected.append(&#39;digitalout.dat&#39;)

        if info[&#39;num_auxilary_input_channels&#39;] &gt; 0:
            files_expected.append(&#39;auxiliary.dat&#39;)

    elif file_type == &#39;one file per channel&#39;:
        for x in info[&#39;amplifier_channels&#39;]:
            files_expected.append(&#39;amp-&#39; + x[&#39;native_channel_name&#39;] + &#39;.dat&#39;)

        for x in info[&#39;board_dig_in_channels&#39;]:
            files_expected.append(&#39;board-%s.dat&#39; % x[&#39;native_channel_name&#39;])

        for x in info[&#39;board_dig_out_channels&#39;]:
            files_expected.append(&#39;board-%s.dat&#39; % x[&#39;native_channel_name&#39;])

        for x in info[&#39;aux_input_channels&#39;]:
            files_expected.append(&#39;aux-%s.dat&#39; % x[&#39;native_channel_name&#39;])


    missing_files = []
    file_list = os.listdir(rec_dir)
    for x in file_expected:
        if x not in file_list:
            missing_file.append(x)

    if len(missing_files) == 0:
        tests[&#39;files&#39;] = &#39;PASS&#39;
    else:
        tests[&#39;files&#39;] = &#39;MISSING&#39;
        numbers[&#39;missing_files&#39;] = missing_files

    # Check time data for dropped packets
    time = dio.rawIO.read_time_dat(rec_dir, sampling_rate=1)  # get raw timestamps
    numbers[&#39;n_samples&#39;] = len(time)
    numbers[&#39;recording_length&#39;] = float(time[-1])/fs
    expected_time = np.arange(time[0], time[-1]+1, 1)
    missing_timestamps = np.setdiff1d(expected_time, time)
    missing_times = np.array([float(x)/fs for x in missing_timestamps])
    if len(missing_timestamps) == 0:
        tests[&#39;dropped_packets&#39;] = &#39;PASS&#39;
    else:
        tests[&#39;dropped_packets&#39;] = &#39;%i&#39; % len(missing_timestamps)
        numbers[&#39;dropped_packets&#39;] = missing_times

    # Check recording length of each trace
    tests[&#39;data_traces&#39;] = &#39;FAIL&#39;
    if file_type == &#39;one file per signal type&#39;:
        try:
            data = dio.rawIO.read_amplifier_dat(rec_dir)
            if data is None:
                tests[&#39;data_traces&#39;] = &#39;UNREADABLE&#39;
            elif data.shape[0] == numbers[&#39;n_samples&#39;]:
                tests[&#39;data_traces&#39;] = &#39;PASS&#39;
            else:
                tests[&#39;data_traces&#39;] = &#39;CUTOFF&#39;
                numbers[&#39;data_trace_length (s)&#39;] = data.shape[0]/fs

        except:
            tests[&#39;data_traces&#39;] = &#39;UNREADABLE&#39;

    elif file_type == &#39;one file per channel&#39;:
        chan_info = pd.DataFrame(columns=[&#39;port&#39;, &#39;channel&#39;, &#39;n_samples&#39;])
        lengths = []
        min_samples = numbers[&#39;n_samples&#39;]
        max_samples = number[&#39;n_samples&#39;]
        for x in info[&#39;amplifier_channels&#39;]:
            fn = os.path.join(rec_dir, &#39;amp-%s.dat&#39; % x[&#39;native_channel_name&#39;])
            if os.path.basename(fn) in missing_files:
                continue

            data = dio.rawIO.read_one_channel_file(fn)
            lengths.append((x[&#39;native_channel_name&#39;], data.shape[0]))
            if data.shape[0] &lt; min_samples:
                min_samples = data.shape[0]

            if data.shape[0] &gt; max_samples:
                max_samples = data.shape[0]

        if min_samples == max_samples:
            tests[&#39;data_traces&#39;] = &#39;PASS&#39;

        else:
            test[&#39;data_traces&#39;] = &#39;CUTOFF&#39;

        numbers[&#39;max_recording_length (s)&#39;] = max_samples/fs
        numbers[&#39;min_recording_length (s)&#39;] = min_samples/fs</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="blechpy.datastructures.dataset.port_in_dataset"><code class="name flex">
<span>def <span class="ident">port_in_dataset</span></span>(<span>rec_dir=None, shell=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Import an existing dataset into this framework</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def port_in_dataset(rec_dir=None, shell=False):
    &#39;&#39;&#39;Import an existing dataset into this framework
    &#39;&#39;&#39;
    if rec_dir is None:
        rec_dir = userIO.get_filedirs(&#39;Select recording directory&#39;, shell=shell)
        if rec_dir is None:
            return None

    dat = dataset(rec_dir, shell=shell)
    # Check files that will be overwritten: log_file, save_file
    if os.path.isfile(dat.save_file):
        prompt = &#39;%s already exists. Continuing will overwrite this. Continue?&#39; % dat.save_file
        q = userIO.ask_user(prompt, shell=shell)
        if q == 0:
            print(&#39;Aborted&#39;)
            return None

    # if os.path.isfile(dat.h5_file):
    #     prompt = &#39;%s already exists. Continuinlg will overwrite this. Continue?&#39; % dat.h5_file
    #     q = userIO.ask_user(prompt, shell=shell)
    #     if q == 0:
    #         print(&#39;Aborted&#39;)
    #         return None

    if os.path.isfile(dat.log_file):
        prompt = &#39;%s already exists. Continuing will append to this. Continue?&#39; % dat.log_file
        q = userIO.ask_user(prompt, shell=shell)
        if q == 0:
            print(&#39;Aborted&#39;)
            return None

    with open(dat.log_file, &#39;a&#39;) as f:
        print(&#39;\n==========\nPorting dataset into blechpy format\n==========\n&#39;, file=f)
        print(dat, file=f)

    status = dat.process_status
    dat.initParams(shell=shell)


    user_status = status.copy()
    user_status = userIO.fill_dict(user_status,
                                   &#39;Which processes have already been &#39;
                                   &#39;done to the data?&#39;, shell=shell)

    status.update(user_status)
    # if h5 exists data must have been extracted

    if not os.path.isfile(dat.h5_file) or status[&#39;extract_data&#39;] == False:
        dat.save()
        return dat

    # write eletrode map and digital input &amp; output maps to hf5
    dio.h5io.write_electrode_map_to_h5(dat.h5_file, dat.electrode_mapping)
    if dat.rec_info.get(&#39;dig_in&#39;) is not None:
        dio.h5io.write_digital_map_to_h5(dat.h5_file, dat.dig_in_mapping, &#39;in&#39;)

    if dat.rec_info.get(&#39;dig_out&#39;) is not None:
        dio.h5io.write_digital_map_to_h5(dat.h5_file, dat.dig_out_mapping, &#39;out&#39;)


    node_list = dio.h5io.get_node_list(dat.h5_file)

    if (status[&#39;create_trial_list&#39;] == False) and (&#39;digital_in&#39; in node_list):
        dat.create_trial_list()

    dat.save()
    return dat</code></pre>
</details>
</dd>
<dt id="blechpy.datastructures.dataset.validate_data_integrity"><code class="name flex">
<span>def <span class="ident">validate_data_integrity</span></span>(<span>rec_dir, verbose=False)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def validate_data_integrity(rec_dir, verbose=False):
    print(&#39;Raw Data Validation\n&#39; + &#39;-&#39;*19)
    test_names = [&#39;file_type&#39;, &#39;recording_info&#39;, &#39;files&#39;, &#39;dropped_packets&#39;, &#39;data_length&#39;]
    number_names = [&#39;sample_rate&#39;, &#39;dropped_packets&#39;, &#39;missing_files&#39;, &#39;recording_length&#39;]
    tests = dict.fromkeys(test_names, &#39;NOT TESTED&#39;)
    numbers = dict.fromkeys(number_names, -1)
    file_type = dio.rawIO.get_recording_filetype(rec_dir)
    if file_type is None:
        file_type_check = &#39;UNSUPPORTED&#39;
    else:
        tests[&#39;file_type&#39;] = &#39;PASS&#39;

    # Check info.rhd integrity
    info_file = os.path.join(rec_dir, &#39;info.rhd&#39;)
    try:
        rec_info = dio.rawIO.read_rec_info(rec_dir, shell=True)
        with open(info_file, &#39;rb&#39;) as f:
            info = dio.load_intan_rhd_format.read_header(f)

        tests[&#39;recording_info&#39;] = &#39;PASS&#39;
    except FileNotFoundError:
        test[&#39;recording_info&#39;] = &#39;MISSING&#39;
    except Exception as e:
        info_size = os.path.getsize(os.path.join(rec_dir, &#39;info.rhd&#39;))
        if info_size == 0:
            tests[&#39;recording_info&#39;] = &#39;EMPTY&#39;
        else:
            tests[&#39;recording_info&#39;] = &#39;FAIL&#39;

        print(pt.print_dict(tests, tabs=1))
        return tests, numbers

    counts = {x : info(x) for x in info.keys() if &#39;num&#39; in x}
    numbers.update(counts)
    fs = info[&#39;sample_rate&#39;]
    # Check all files needed are present
    files_expected = [&#39;time.dat&#39;]
    if file_type == &#39;one file per signal type&#39;:
        files_expected.append(&#39;amplifier.dat&#39;)
        if rec_info.get(&#39;dig_in&#39;) is not None:
            files_expected.append(&#39;digitalin.dat&#39;)

        if rec_info.get(&#39;dig_out&#39;) is not None:
            files_expected.append(&#39;digitalout.dat&#39;)

        if info[&#39;num_auxilary_input_channels&#39;] &gt; 0:
            files_expected.append(&#39;auxiliary.dat&#39;)

    elif file_type == &#39;one file per channel&#39;:
        for x in info[&#39;amplifier_channels&#39;]:
            files_expected.append(&#39;amp-&#39; + x[&#39;native_channel_name&#39;] + &#39;.dat&#39;)

        for x in info[&#39;board_dig_in_channels&#39;]:
            files_expected.append(&#39;board-%s.dat&#39; % x[&#39;native_channel_name&#39;])

        for x in info[&#39;board_dig_out_channels&#39;]:
            files_expected.append(&#39;board-%s.dat&#39; % x[&#39;native_channel_name&#39;])

        for x in info[&#39;aux_input_channels&#39;]:
            files_expected.append(&#39;aux-%s.dat&#39; % x[&#39;native_channel_name&#39;])


    missing_files = []
    file_list = os.listdir(rec_dir)
    for x in file_expected:
        if x not in file_list:
            missing_file.append(x)

    if len(missing_files) == 0:
        tests[&#39;files&#39;] = &#39;PASS&#39;
    else:
        tests[&#39;files&#39;] = &#39;MISSING&#39;
        numbers[&#39;missing_files&#39;] = missing_files

    # Check time data for dropped packets
    time = dio.rawIO.read_time_dat(rec_dir, sampling_rate=1)  # get raw timestamps
    numbers[&#39;n_samples&#39;] = len(time)
    numbers[&#39;recording_length&#39;] = float(time[-1])/fs
    expected_time = np.arange(time[0], time[-1]+1, 1)
    missing_timestamps = np.setdiff1d(expected_time, time)
    missing_times = np.array([float(x)/fs for x in missing_timestamps])
    if len(missing_timestamps) == 0:
        tests[&#39;dropped_packets&#39;] = &#39;PASS&#39;
    else:
        tests[&#39;dropped_packets&#39;] = &#39;%i&#39; % len(missing_timestamps)
        numbers[&#39;dropped_packets&#39;] = missing_times

    # Check recording length of each trace
    tests[&#39;data_traces&#39;] = &#39;FAIL&#39;
    if file_type == &#39;one file per signal type&#39;:
        try:
            data = dio.rawIO.read_amplifier_dat(rec_dir)
            if data is None:
                tests[&#39;data_traces&#39;] = &#39;UNREADABLE&#39;
            elif data.shape[0] == numbers[&#39;n_samples&#39;]:
                tests[&#39;data_traces&#39;] = &#39;PASS&#39;
            else:
                tests[&#39;data_traces&#39;] = &#39;CUTOFF&#39;
                numbers[&#39;data_trace_length (s)&#39;] = data.shape[0]/fs

        except:
            tests[&#39;data_traces&#39;] = &#39;UNREADABLE&#39;

    elif file_type == &#39;one file per channel&#39;:
        chan_info = pd.DataFrame(columns=[&#39;port&#39;, &#39;channel&#39;, &#39;n_samples&#39;])
        lengths = []
        min_samples = numbers[&#39;n_samples&#39;]
        max_samples = number[&#39;n_samples&#39;]
        for x in info[&#39;amplifier_channels&#39;]:
            fn = os.path.join(rec_dir, &#39;amp-%s.dat&#39; % x[&#39;native_channel_name&#39;])
            if os.path.basename(fn) in missing_files:
                continue

            data = dio.rawIO.read_one_channel_file(fn)
            lengths.append((x[&#39;native_channel_name&#39;], data.shape[0]))
            if data.shape[0] &lt; min_samples:
                min_samples = data.shape[0]

            if data.shape[0] &gt; max_samples:
                max_samples = data.shape[0]

        if min_samples == max_samples:
            tests[&#39;data_traces&#39;] = &#39;PASS&#39;

        else:
            test[&#39;data_traces&#39;] = &#39;CUTOFF&#39;

        numbers[&#39;max_recording_length (s)&#39;] = max_samples/fs
        numbers[&#39;min_recording_length (s)&#39;] = min_samples/fs</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="blechpy.datastructures.dataset.dataset"><code class="flex name class">
<span>class <span class="ident">dataset</span></span>
<span>(</span><span>file_dir=None, data_name=None, shell=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Stores information related to an intan recording directory, allows
executing basic processing and analysis scripts, and stores parameters data
for those analyses</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>file_dir</code></strong> :&ensp;<code>str</code> (optional)</dt>
<dd>absolute path to a recording directory, if left empty a filechooser
will popup</dd>
</dl>
<p>Initialize dataset object from file_dir, grabs basename from name of
directory and initializes basic analysis parameters</p>
<h2 id="parameters_1">Parameters</h2>
<dl>
<dt><strong><code>file_dir</code></strong> :&ensp;<code>str</code> (optional), <code>file</code> <code>directory</code> <code>for</code> <code>intan</code> <code>recording</code> <code>data</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="throws">Throws</h2>
<dl>
<dt><strong><code>ValueError</code></strong></dt>
<dd>if file_dir is not provided and no directory is chosen
when prompted</dd>
<dt><strong><code>NotADirectoryError</code></strong> :&ensp;<code>if</code> <code>file_dir</code> <code>does</code> <code>not</code> <code>exist</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class dataset(data_object):
    &#39;&#39;&#39;Stores information related to an intan recording directory, allows
    executing basic processing and analysis scripts, and stores parameters data
    for those analyses

    Parameters
    ----------
    file_dir : str (optional)
        absolute path to a recording directory, if left empty a filechooser
        will popup
    &#39;&#39;&#39;

    def __init__(self, file_dir=None, data_name=None, shell=False):
        &#39;&#39;&#39;Initialize dataset object from file_dir, grabs basename from name of
        directory and initializes basic analysis parameters

        Parameters
        ----------
        file_dir : str (optional), file directory for intan recording data

        Throws
        ------
        ValueError
            if file_dir is not provided and no directory is chosen
            when prompted
        NotADirectoryError : if file_dir does not exist
        &#39;&#39;&#39;
        super().__init__(&#39;dataset&#39;, file_dir, data_name=data_name, shell=shell)
        h5_file = dio.h5io.get_h5_filename(self.root_dir)
        if h5_file is None:
            h5_file = os.path.join(self.root_dir, &#39;%s.h5&#39; % self.data_name)

        self.h5_file = h5_file

        self.dataset_creation_date = dt.datetime.today()

        # Outline standard processing pipeline and status check
        self.processing_steps = [&#39;initialize parameters&#39;,
                                 &#39;extract_data&#39;, &#39;create_trial_list&#39;,
                                 &#39;mark_dead_channels&#39;,
                                 &#39;common_average_reference&#39;, &#39;blech_clust_run&#39;,
                                 &#39;cleanup_clustering&#39;,
                                 &#39;sort_units&#39;, &#39;make_unit_plots&#39;,
                                 &#39;units_similarity&#39;, &#39;make_unit_arrays&#39;,
                                 &#39;make_psth_arrays&#39;, &#39;plot_psths&#39;,
                                 &#39;palatability_calculate&#39;, &#39;palatability_plot&#39;,
                                 &#39;overlay_psth&#39;]
        self.process_status = dict.fromkeys(self.processing_steps, False)

    def _change_root(self, new_root=None):
        old_root = self.root_dir
        new_root = super()._change_root(new_root)
        self.h5_file = self.h5_file.replace(old_root, new_root)
        return new_root

    @Logger(&#39;Initializing Parameters&#39;)
    def initParams(self, data_quality=&#39;clean&#39;, emg_port=None,
                   emg_channels=None, car_keyword=None,
                   car_group_areas=None,
                   shell=False, dig_in_names=None,
                   dig_out_names=None, accept_params=False):
        &#39;&#39;&#39;
        Initalizes basic default analysis parameters and allows customization
        of parameters

        Parameters (all optional)
        -------------------------
        data_quality : {&#39;clean&#39;, &#39;noisy&#39;}
            keyword defining which default set of parameters to use to detect
            headstage disconnection during clustering
            default is &#39;clean&#39;. Best practice is to run blech_clust as &#39;clean&#39;
            and re-run as &#39;noisy&#39; if too many early cutoffs occurr
        emg_port : str
            Port (&#39;A&#39;, &#39;B&#39;, &#39;C&#39;) of EMG, if there was an EMG. None (default)
            will query user. False indicates no EMG port and not to query user
        emg_channels : list of int
            channel or channels of EMGs on port specified
            default is None
        car_keyword : str
            Specifes default common average reference groups
            defaults are found in CAR_defaults.json
            Currently &#39;bilateral32&#39; is only keyword available
            If left as None (default) user will be queries to select common
            average reference groups
        shell : bool
            False (default) for GUI. True for command-line interface
        dig_in_names : list of str
            Names of digital inputs. Must match number of digital inputs used
            in recording.
            None (default) queries user to name each dig_in
        dig_out_names : list of str
            Names of digital outputs. Must match number of digital outputs in
            recording.
            None (default) queries user to name each dig_out
        accept_params : bool
            True automatically accepts default parameters where possible,
            decreasing user queries
            False (default) will query user to confirm or edit parameters for
            clustering, spike array and psth creation and palatability/identity
            calculations
        &#39;&#39;&#39;
        # Get parameters from info.rhd
        file_dir = self.root_dir
        rec_info = dio.rawIO.read_rec_info(file_dir)
        ports = rec_info.pop(&#39;ports&#39;)
        channels = rec_info.pop(&#39;channels&#39;)
        sampling_rate = rec_info[&#39;amplifier_sampling_rate&#39;]
        self.rec_info = rec_info
        self.sampling_rate = sampling_rate

        # Get default parameters from files
        clustering_params = dio.params.load_params(&#39;clustering_params&#39;, file_dir,
                                                   default_keyword=data_quality)
        spike_array_params = dio.params.load_params(&#39;spike_array_params&#39;, file_dir)
        psth_params = dio.params.load_params(&#39;psth_params&#39;, file_dir)
        pal_id_params = dio.params.load_params(&#39;pal_id_params&#39;, file_dir)
        spike_array_params[&#39;sampling_rate&#39;] = sampling_rate
        clustering_params[&#39;file_dir&#39;] = file_dir
        clustering_params[&#39;sampling_rate&#39;] = sampling_rate

        # Setup digital input mapping
        #TODO: Setup digital output mapping...ignoring for now
        if rec_info.get(&#39;dig_in&#39;):
            self._setup_din_mapping(dig_in_names, shell)
            dim = self.dig_in_mapping.copy()
            spike_array_params[&#39;laser_channels&#39;] = dim.channel[dim[&#39;laser&#39;]].to_list()
            spike_array_params[&#39;dig_ins_to_use&#39;] = dim.channel[dim[&#39;spike_array&#39;]].to_list()
        else:
            self.dig_in_mapping = None

        # Setup electrode and emg mapping
        self._setup_channel_mapping(ports, channels, emg_port,
                                    emg_channels, shell=shell)

        # Set CAR groups
        self._set_CAR_groups(group_keyword=car_keyword, group_areas=car_group_areas, shell=shell)

        # Confirm parameters
        self.spike_array_params = spike_array_params
        if not accept_params:
            conf = userIO.confirm_parameter_dict
            clustering_params = conf(clustering_params,
                                     &#39;Clustering Parameters&#39;, shell=shell)
            self.edit_spike_array_params(shell=shell)
            psth_params = conf(psth_params,
                               &#39;PSTH Parameters&#39;, shell=shell)
            pal_id_params = conf(pal_id_params,
                                 &#39;Palatability/Identity Parameters\n&#39;
                                 &#39;Valid unit_type is Single, Multi or All&#39;,
                                 shell=shell)

        # Store parameters
        self.clustering_params = clustering_params
        self.pal_id_params = pal_id_params
        self.psth_params = psth_params
        self._write_all_params_to_json()
        self.process_status[&#39;initialize parameters&#39;] = True
        self.save()

    def _set_CAR_groups(self, group_keyword=None, shell=False, group_areas=None):
        &#39;&#39;&#39;Sets that electrode groups for common average referencing and
        defines which brain region electrodes eneded up in

        Parameters
        ----------
        group_keyword : str or int
            Keyword corresponding to a preset electrode grouping in CAR_params.json
            Or integer indicating number of CAR groups
        shell : bool
            True for command-line interface, False (default) for GUI
        &#39;&#39;&#39;
        if not hasattr(self, &#39;electrode_mapping&#39;):
            raise ValueError(&#39;Set electrode mapping before setting CAR groups&#39;)

        em = self.electrode_mapping.copy()

        car_param_file = os.path.join(self.root_dir, &#39;analysis_params&#39;,
                                      &#39;CAR_params.json&#39;)
        if os.path.isfile(car_param_file):
            tmp = dio.params.load_params(&#39;CAR_params&#39;, self.root_dir)
            if tmp is not None:
                group_electrodes = tmp
            else:
                raise ValueError(&#39;CAR_params file exists in recording dir, but is empty&#39;)

        else:
            if group_keyword is None:
                group_keyword = userIO.get_user_input(
                    &#39;Input keyword for CAR parameters or number of CAR groups&#39;,
                    shell=shell)

                if group_keyword is None:
                    ValueError(&#39;Must provide a keyword or number of groups&#39;)

            if group_keyword.isnumeric():
                num_groups = int(group_keyword)
                group_electrodes = dio.params.select_CAR_groups(num_groups, em,
                                                                shell=shell)
            else:
                group_electrodes = dio.params.load_params(&#39;CAR_params&#39;,
                                                          self.root_dir,
                                                          default_keyword=group_keyword)

        num_groups = len(group_electrodes)
        if group_areas is not None and len(group_areas) == num_groups:
            for i, x in enumerate(zip(group_electrodes, group_areas)):
                em.loc[x[0], &#39;area&#39;] = x[1]
                em.loc[x[0], &#39;CAR_group&#39;] = i

        else:
            group_names = [&#39;Group %i&#39; % i for i in range(num_groups)]
            area_dict = dict.fromkeys(group_names, &#39;&#39;)
            area_dict = userIO.fill_dict(area_dict, &#39;Set Areas for CAR groups&#39;,
                                         shell=shell)
            for k, v in area_dict.items():
                i = int(k.replace(&#39;Group&#39;, &#39;&#39;))
                em.loc[group_electrodes[i], &#39;area&#39;] = v
                em.loc[group_electrodes[i], &#39;CAR_group&#39;] = i

        self.CAR_electrodes = group_electrodes
        self.electrode_mapping = em.copy()

    def _setup_din_mapping(self, dig_in_names=None, shell=False):
        &#39;&#39;&#39;sets up dig_in_mapping dataframe  and queries user to fill in columns

        Parameters
        ----------
        dig_in_names : list of str (optional)
        shell : bool (optional)
            True for command-line interface
            False (default) for GUI
        &#39;&#39;&#39;
        rec_info = self.rec_info
        df = pd.DataFrame()
        df[&#39;channel&#39;] = rec_info.get(&#39;dig_in&#39;)
        n_dig_in = len(df)
        # Names
        if dig_in_names:
            df[&#39;name&#39;] = dig_in_names
        else:
            df[&#39;name&#39;] = &#39;&#39;

        # Parameters to query
        df[&#39;palatability_rank&#39;] = 0
        df[&#39;laser&#39;] = False
        df[&#39;spike_array&#39;] = True
        df[&#39;exclude&#39;] = False
        # Re-format for query
        idx = df.index
        df.index = [&#39;dig_in_%i&#39; % x for x in df.channel]
        # Query for user input
        prompt = (&#39;Digital Input Parameters\nSet palatability ranks from 1 to %i&#39;
                  &#39;\nor blank to exclude from pal_id analysis&#39;) % len(df)
        tmp = userIO.fill_dict(df.to_dict(), prompt=prompt, shell=shell)
        # Reformat for storage
        df2 = pd.DataFrame.from_dict(tmp)
        df2 = df2.sort_values(by=[&#39;channel&#39;])
        df2.index = idx
        df2[&#39;palatability_rank&#39;] = df2[&#39;palatability_rank&#39;].fillna(-1).astype(&#39;int&#39;)
        self.dig_in_mapping = df2.copy()
        if os.path.isfile(self.h5_file):
            dio.h5io.write_digital_map_to_h5(self.h5_file, self.dig_in_mapping, &#39;in&#39;)

    def _setup_channel_mapping(self, ports, channels, emg_port, emg_channels, shell=False):
        &#39;&#39;&#39;Creates electrode_mapping and emg_mapping DataFrames with columns:
        - Electrode
        - Port
        - Channel

        Parameters
        ----------
        ports : list of str, item corresponing to each channel
        channels : list of int, channels on each port
        emg_port : str
        emg_channels : list of int
        &#39;&#39;&#39;
        if emg_port is None:
            q = userIO.ask_user(&#39;Do you have an EMG?&#39;, shell=shell)
            if q==1:
                emg_port = userIO.select_from_list(&#39;Select EMG Port:&#39;,
                                                   ports, &#39;EMG Port&#39;,
                                                   shell=shell)
                emg_channels = userIO.select_from_list(
                    &#39;Select EMG Channels:&#39;,
                    [y for x, y in
                     zip(ports, channels)
                     if x == emg_port],
                    title=&#39;EMG Channels&#39;,
                    multi_select=True, shell=shell)

        el_map, em_map = dio.params.flatten_channels(ports, channels,
                                                     emg_port, emg_channels)
        self.electrode_mapping = el_map
        self.emg_mapping = em_map
        if os.path.isfile(self.h5_file):
            dio.h5io.write_electrode_map_to_h5(self.h5_file, self.electrode_mapping)

    def edit_spike_array_params(self, shell=False):
        &#39;&#39;&#39;Edit spike array parameters and adjust dig_in_mapping accordingly

        Parameters
        ----------
        shell : bool, whether to use CLI or GUI
        &#39;&#39;&#39;
        if not hasattr(self, &#39;dig_in_mapping&#39;):
            self.spike_array_params = None
            return

        sa = deepcopy(self.spike_array_params)
        tmp = userIO.fill_dict(sa, &#39;Spike Array Parameters\n(Times in ms)&#39;,
                               shell=shell)
        if tmp is None:
            return

        dim = self.dig_in_mapping
        dim[&#39;spike_array&#39;] = False
        if tmp[&#39;dig_ins_to_use&#39;] != [&#39;&#39;]:
            tmp[&#39;dig_ins_to_use&#39;] = [int(x) for x in tmp[&#39;dig_ins_to_use&#39;]]
            dim.loc[[x in tmp[&#39;dig_ins_to_use&#39;] for x in dim.channel],
                    &#39;spike_array&#39;] = True

        dim[&#39;laser_channels&#39;] = False
        if tmp[&#39;laser_channels&#39;] != [&#39;&#39;]:
            tmp[&#39;laser_channels&#39;] = [int(x) for x in tmp[&#39;laser_channels&#39;]]
            dim.loc[[x in tmp[&#39;laser_channels&#39;] for x in dim.channel],
                    &#39;laser&#39;] = True

        self.spike_array_params = tmp.copy()
        wt.write_params_to_json(&#39;spike_array_params&#39;,
                                self.root_dir, tmp)

    def edit_clustering_params(self, shell=False):
        &#39;&#39;&#39;Allows user interface for editing clustering parameters

        Parameters
        ----------
        shell : bool (optional)
            True if you want command-line interface, False for GUI (default)
        &#39;&#39;&#39;
        tmp = userIO.fill_dict(self.clustering_params,
                               &#39;Clustering Parameters\n(Times in ms)&#39;,
                               shell=shell)
        if tmp:
            self.clustering_params = tmp
            wt.write_params_to_json(&#39;clustering_params&#39;, self.root_dir, tmp)

    def edit_psth_params(self, shell=False):
        &#39;&#39;&#39;Allows user interface for editing psth parameters

        Parameters
        ----------
        shell : bool (optional)
            True if you want command-line interface, False for GUI (default)
        &#39;&#39;&#39;
        tmp = userIO.fill_dict(self.psth_params,
                               &#39;PSTH Parameters\n(Times in ms)&#39;,
                               shell=shell)
        if tmp:
            self.psth_params = tmp
            wt.write_params_to_json(&#39;psth_params&#39;, self.root_dir, tmp)

    def edit_pal_id_params(self, shell=False):
        &#39;&#39;&#39;Allows user interface for editing palatability/identity parameters

        Parameters
        ----------
        shell : bool (optional)
            True if you want command-line interface, False for GUI (default)
        &#39;&#39;&#39;
        tmp = userIO.fill_dict(self.pal_id_params,
                               &#39;Palatability/Identity Parameters\n(Times in ms)&#39;,
                               shell=shell)
        if tmp:
            self.pal_id_params = tmp
            wt.write_params_to_json(&#39;pal_id_params&#39;, self.root_dir, tmp)

    def __str__(self):
        &#39;&#39;&#39;Put all information about dataset in string format

        Returns
        -------
        str : representation of dataset object
        &#39;&#39;&#39;
        out1 = super().__str__()
        out = [out1]
        out.append(&#39;\nObject creation date: &#39;
                   + self.dataset_creation_date.strftime(&#39;%m/%d/%y&#39;))

        if hasattr(self, &#39;raw_h5_file&#39;):
            out.append(&#39;Deleted Raw h5 file: &#39;+self.raw_h5_file)

        out.append(&#39;h5 File: &#39;+self.h5_file)
        out.append(&#39;&#39;)

        out.append(&#39;--------------------&#39;)
        out.append(&#39;Processing Status&#39;)
        out.append(&#39;--------------------&#39;)
        out.append(pt.print_dict(self.process_status))
        out.append(&#39;&#39;)

        if not hasattr(self, &#39;rec_info&#39;):
            return &#39;\n&#39;.join(out)

        info = self.rec_info

        out.append(&#39;--------------------&#39;)
        out.append(&#39;Recording Info&#39;)
        out.append(&#39;--------------------&#39;)
        out.append(pt.print_dict(self.rec_info))
        out.append(&#39;&#39;)

        out.append(&#39;--------------------&#39;)
        out.append(&#39;Electrodes&#39;)
        out.append(&#39;--------------------&#39;)
        out.append(pt.print_dataframe(self.electrode_mapping))
        out.append(&#39;&#39;)

        if hasattr(self, &#39;CAR_electrodes&#39;):
            out.append(&#39;--------------------&#39;)
            out.append(&#39;CAR Groups&#39;)
            out.append(&#39;--------------------&#39;)
            headers = [&#39;Group %i&#39; % x for x in range(len(self.CAR_electrodes))]
            out.append(pt.print_list_table(self.CAR_electrodes, headers))
            out.append(&#39;&#39;)

        if not self.emg_mapping.empty:
            out.append(&#39;--------------------&#39;)
            out.append(&#39;EMG&#39;)
            out.append(&#39;--------------------&#39;)
            out.append(pt.print_dataframe(self.emg_mapping))
            out.append(&#39;&#39;)

        if info.get(&#39;dig_in&#39;):
            out.append(&#39;--------------------&#39;)
            out.append(&#39;Digital Input&#39;)
            out.append(&#39;--------------------&#39;)
            out.append(pt.print_dataframe(self.dig_in_mapping))
            out.append(&#39;&#39;)

        if info.get(&#39;dig_out&#39;):
            out.append(&#39;--------------------&#39;)
            out.append(&#39;Digital Output&#39;)
            out.append(&#39;--------------------&#39;)
            out.append(pt.print_dataframe(self.dig_out_mapping))
            out.append(&#39;&#39;)

        out.append(&#39;--------------------&#39;)
        out.append(&#39;Clustering Parameters&#39;)
        out.append(&#39;--------------------&#39;)
        out.append(pt.print_dict(self.clustering_params))
        out.append(&#39;&#39;)

        out.append(&#39;--------------------&#39;)
        out.append(&#39;Spike Array Parameters&#39;)
        out.append(&#39;--------------------&#39;)
        out.append(pt.print_dict(self.spike_array_params))
        out.append(&#39;&#39;)

        out.append(&#39;--------------------&#39;)
        out.append(&#39;PSTH Parameters&#39;)
        out.append(&#39;--------------------&#39;)
        out.append(pt.print_dict(self.psth_params))
        out.append(&#39;&#39;)

        out.append(&#39;--------------------&#39;)
        out.append(&#39;Palatability/Identity Parameters&#39;)
        out.append(&#39;--------------------&#39;)
        out.append(pt.print_dict(self.pal_id_params))
        out.append(&#39;&#39;)

        return &#39;\n&#39;.join(out)

    @Logger(&#39;Writing parameters to JSON&#39;)
    def _write_all_params_to_json(self):
        &#39;&#39;&#39;Writes all parameters to json files in analysis_params folder in the
        recording directory
        &#39;&#39;&#39;
        print(&#39;Writing all parameters to json file in analysis_params folder...&#39;)
        clustering_params = self.clustering_params
        spike_array_params = self.spike_array_params
        psth_params = self.psth_params
        pal_id_params = self.pal_id_params
        CAR_params = self.CAR_electrodes

        rec_dir = self.root_dir
        wt.write_params_to_json(&#39;clustering_params&#39;, rec_dir, clustering_params)
        wt.write_params_to_json(&#39;spike_array_params&#39;, rec_dir, spike_array_params)
        wt.write_params_to_json(&#39;psth_params&#39;, rec_dir, psth_params)
        wt.write_params_to_json(&#39;pal_id_params&#39;, rec_dir, pal_id_params)
        wt.write_params_to_json(&#39;CAR_params&#39;, rec_dir, CAR_params)

    @Logger(&#39;Extracting Data&#39;)
    def extract_data(self, filename=None, shell=False):
        &#39;&#39;&#39;Create hdf5 store for data and read in Intan .dat files. Also create
        subfolders for processing outputs

        Parameters
        ----------
        data_quality: {&#39;clean&#39;, &#39;noisy&#39;} (optional)
            Specifies quality of data for default clustering parameters
            associated. Should generally first process with clean (default)
            parameters and then try noisy after running blech_clust and
            checking if too many electrodes as cutoff too early
        &#39;&#39;&#39;
        if self.rec_info[&#39;file_type&#39;] is None:
            raise ValueError(&#39;Unsupported recording type. Cannot extract yet.&#39;)

        if filename is None:
            filename = self.h5_file

        print(&#39;\nExtract Intan Data\n--------------------&#39;)
        # Create h5 file
        tmp = dio.h5io.create_empty_data_h5(filename, shell)
        if tmp is None:
            return

        # Create arrays for raw data in hdf5 store
        dio.h5io.create_hdf_arrays(filename, self.rec_info,
                                   self.electrode_mapping, self.emg_mapping)

        # Read in data to arrays
        dio.h5io.read_files_into_arrays(filename,
                                        self.rec_info,
                                        self.electrode_mapping,
                                        self.emg_mapping)

        # Write electrode and digital input mapping into h5 file
        # TODO: write EMG and digital output mapping into h5 file
        dio.h5io.write_electrode_map_to_h5(self.h5_file, self.electrode_mapping)
        if self.dig_in_mapping is not None:
            dio.h5io.write_digital_map_to_h5(self.h5_file, self.dig_in_mapping, &#39;in&#39;)

        # update status
        self.h5_file = filename
        self.process_status[&#39;extract_data&#39;] = True
        self.save()
        print(&#39;\nData Extraction Complete\n--------------------&#39;)

    @Logger(&#39;Creating Trial List&#39;)
    def create_trial_list(self):
        &#39;&#39;&#39;Create lists of trials based on digital inputs and outputs and store
        to hdf5 store
        Can only be run after data extraction
        &#39;&#39;&#39;
        if not self.process_status[&#39;extract_data&#39;]:
            userIO.tell_user(&#39;Must extract data before creating trial list&#39;,shell=True)
            return

        if self.rec_info.get(&#39;dig_in&#39;):
            in_list = dio.h5io.create_trial_data_table(
                self.h5_file,
                self.dig_in_mapping,
                self.sampling_rate,
                &#39;in&#39;)
            self.dig_in_trials = in_list
        else:
            print(&#39;No digital input data found&#39;)

        if self.rec_info.get(&#39;dig_out&#39;):
            out_list = dio.h5io.create_trial_data_table(
                self.h5_file,
                self.dig_out_mapping,
                self.sampling_rate,
                &#39;out&#39;)
            self.dig_out_trials = out_list
        else:
            print(&#39;No digital output data found&#39;)

        self.process_status[&#39;create_trial_list&#39;] = True
        self.save()

    @Logger(&#39;Marking Dead Channels&#39;)
    def mark_dead_channels(self, dead_channels=None, shell=False):
        &#39;&#39;&#39;Plots small piece of raw traces and a metric to help identify dead
        channels. Once user marks channels as dead a new column is added to
        electrode mapping

        Parameters
        ----------
        dead_channels : list of int, optional
            if this is specified then nothing is plotted, those channels are
            simply marked as dead
        shell : bool, optional
        &#39;&#39;&#39;
        print(&#39;Marking dead channels\n----------&#39;)
        em = self.electrode_mapping.copy()
        if dead_channels is None:
            userIO.tell_user(&#39;Making traces figure for dead channel detection...&#39;,
                             shell=True)
            save_file = os.path.join(self.root_dir, &#39;Electrode_Traces.png&#39;)
            fig, ax = datplt.plot_traces_and_outliers(self.h5_filei, save_file=save_file)
            if not shell:
                # Better to open figure outside of python since its a lot of
                # data on figure and matplotlib is slow
                subprocess.call([&#39;xdg-open&#39;, save_file])
            else:
                userIO.tell_user(&#39;Saved figure of traces to %s for reference&#39;
                                 % save_file, shell=shell)

            choice = userIO.select_from_list(&#39;Select dead channels:&#39;,
                                             em.Electrode.to_list(),
                                             &#39;Dead Channel Selection&#39;,
                                             multi_select=True,
                                             shell=shell)
            dead_channels = list(map(int, choice))

        print(&#39;Marking eletrodes %s as dead.\n&#39;
              &#39;They will be excluded from common average referencing.&#39;
              % dead_channels)
        em[&#39;dead&#39;] = False
        em.loc[dead_channels, &#39;dead&#39;] = True
        self.electrode_mapping = em
        if os.path.isfile(self.h5_file):
            dio.h5io.write_electrode_map_to_h5(self.h5_file, self.electrode_mapping)

        self.process_status[&#39;mark_dead_channels&#39;] = True
        self.save()
        return dead_channels

    @Logger(&#39;Common Average Referencing&#39;)
    def common_average_reference(self):
        &#39;&#39;&#39;Define electrode groups and remove common average from  signals

        Parameters
        ----------
        num_groups : int (optional)
            number of CAR groups, if not provided
            there&#39;s a prompt
        &#39;&#39;&#39;
        if not hasattr(self, &#39;CAR_electrodes&#39;):
            raise ValueError(&#39;CAR_electrodes not set&#39;)

        if not hasattr(self, &#39;electrode_mapping&#39;):
            raise ValueError(&#39;electrode_mapping not set&#39;)

        car_electrodes = self.CAR_electrodes
        num_groups = len(car_electrodes)
        em = self.electrode_mapping.copy()

        if &#39;dead&#39; in em.columns:
            dead_electrodes = em.Electrode[em.dead].to_list()
        else:
            dead_electrodes = []

        # Gather Common Average Reference Groups
        print(&#39;CAR Groups\n&#39;)
        headers = [&#39;Group %i&#39; % x for x in range(num_groups)]
        print(pt.print_list_table(car_electrodes, headers))

        # Reference each group
        for i, x in enumerate(car_electrodes):
            tmp = list(set(x) - set(dead_electrodes))
            dio.h5io.common_avg_reference(self.h5_file, tmp, i)

        # Compress and repack file
        dio.h5io.compress_and_repack(self.h5_file)

        self.process_status[&#39;common_average_reference&#39;] = True
        self.save()

    @Logger(&#39;Running Blech Clust&#39;)
    def blech_clust_run(self, data_quality=None, n_cores=None):
        &#39;&#39;&#39;Write clustering parameters to file and
        Run blech_process on each electrode using GNU parallel

        Parameters
        ----------
        data_quality : {&#39;clean&#39;, &#39;noisy&#39;, None (default)}
            set if you want to change the data quality parameters for cutoff
            and spike detection before running clustering. These parameters are
            automatically set as &#34;clean&#34; during initial parameter setup
        accept_params : bool, False (default)
            set to True in order to skip popup confirmation of parameters when
            running
        &#39;&#39;&#39;
        if data_quality:
            tmp = dio.params.load_params(&#39;clustering_params&#39;, self.root_dir,
                                         default_keyword=data_quality)
            if tmp:
                self.clustering_params = tmp
            else:
                raise ValueError(&#39;%s is not a valid data_quality preset. Must &#39;
                                 &#39;be &#34;clean&#34; or &#34;noisy&#34; or None.&#39;)

        print(&#39;\nRunning Blech Clust\n-------------------&#39;)
        print(&#39;Parameters\n%s&#39; % pt.print_dict(self.clustering_params))

        # Create folders for saving things within recording dir
        data_dir = self.root_dir
        directories = [&#39;spike_waveforms&#39;, &#39;spike_times&#39;,
                       &#39;clustering_results&#39;,
                       &#39;Plots&#39;, &#39;memory_monitor_clustering&#39;]
        for d in directories:
            tmp_dir = os.path.join(data_dir, d)
            if os.path.exists(tmp_dir):
                shutil.rmtree(tmp_dir)

            os.mkdir(tmp_dir)

        # Set file for clusting log
        self.clustering_log = os.path.join(data_dir, &#39;results.log&#39;)
        if os.path.exists(self.clustering_log):
            os.remove(self.clustering_log)

        process_path = os.path.realpath(__file__)
        process_path = os.path.join(os.path.dirname(process_path),
                                    &#39;blech_process.py&#39;)
        em = self.electrode_mapping
        if &#39;dead&#39; in em.columns:
            electrodes = em.Electrode[em[&#39;dead&#39;] == False].tolist()
        else:
            electrodes = em.Electrode.tolist()


        pbar = tqdm(total = len(electrodes))
        results = [(None, None, None)] * (max(electrodes)+1)
        clust_errors = [(x, None) for x in electrodes]
        def update_pbar(ans):
            if isinstance(ans, tuple) and ans[0] is not None:
                results[ans[0]] = ans
            else:
                print(&#39;Unexpected error when clustering an electrode&#39;)

            pbar.update()

        if cores is None or cores &gt; multiprocessing.cpu_count():
            cores = multiprocessing.cpu_count() - 1

        pool = multiprocessing.Pool(cores)
        for x in electrodes:
            pool.apply_async(blech_clust_process,
                             args=(x, data_dir, self.clustering_params),
                             callback=update_pbar)

        pool.close()
        pool.join()
        pbar.close()

        print(&#39;Electrode    Result    Cutoff (s)&#39;)
        cutoffs = {}
        clust_res = {}
        clustered = []
        for x, y, z in results:
            if x is None:
                continue

            clustered.append(x)
            print(&#39;  {:&lt;13}{:&lt;10}{}&#39;.format(x, y, z))
            cutoffs[x] = z
            clust_res[x] = y

        print(&#39;1 - Sucess\n0 - No data or no spikes\n-1 - Error&#39;)

        em = self.electrode_mapping.copy()
        em[&#39;cutoff_time&#39;] = em[&#39;Electrode&#39;].map(cutoffs)
        em[&#39;clustering_result&#39;] = em[&#39;Electrode&#39;].map(clust_res)
        self.electrode_mapping = em.copy()
        self.process_status[&#39;blech_clust_run&#39;] = True
        self.process_status[&#39;cleanup_clustering&#39;] = False
        dio.h5io.write_electrode_map_to_h5(self.h5_file, em)
        self.save()
        print(&#39;Clustering Complete\n------------------&#39;)
        return results

    @Logger(&#39;Cleaning up clustering memory logs. Removing raw data and setting&#39;
            &#39;up hdf5 for unit sorting&#39;)
    def cleanup_clustering(self):
        &#39;&#39;&#39;Consolidates memory monitor files, removes raw and referenced data
        and setups up hdf5 store for sorted units data
        &#39;&#39;&#39;
        h5_file = dio.h5io.cleanup_clustering(self.root_dir)
        self.h5_file = h5_file
        self.process_status[&#39;cleanup_clustering&#39;] = True
        self.save()

    def sort_units(self, shell=False):
        &#39;&#39;&#39;Begins processes to allow labelling of clusters as sorted units

        Parameters
        ----------
        shell : bool
            True if command-line interfaced desired, False for GUI (default)
        &#39;&#39;&#39;
        fs = self.sampling_rate
        ss.sort_units(self.root_dir, fs, shell)
        self.process_status[&#39;sort_units&#39;] = True
        self.save()

    @Logger(&#39;Calculating Units Similarity&#39;)
    def units_similarity(self, similarity_cutoff=50, shell=False):
        if &#39;SSH_CONNECTION&#39; in os.environ:
            shell= True

        metrics_dir = os.path.join(self.root_dir, &#39;sorted_unit_metrics&#39;)
        if not os.path.isdir(metrics_dir):
            raise ValueError(&#39;No sorted unit metrics found. Must sort units before calculating similarity&#39;)

        violation_file = os.path.join(metrics_dir,
                                      &#39;units_similarity_violations.txt&#39;)
        violations, sim = ss.calc_units_similarity(self.h5_file,
                                                   self.sampling_rate,
                                                   similarity_cutoff,
                                                   violation_file)
        if len(violations) == 0:
            userIO.tell_user(&#39;No similarity violations found!&#39;, shell=shell)
            self.process_status[&#39;units_similarity&#39;] = True
            return violations, sim

        out_str = [&#39;Units Similarity Violations Found:&#39;]
        out_str.append(&#39;Unit_1    Unit_2    Similarity&#39;)
        for x,y in violations:
            u1 = dio.h5io.parse_unit_number(x)
            u2 = dio.h5io.parse_unit_number(y)
            out_str.append(&#39;   {:&lt;10}{:&lt;10}{}\n&#39;.format(x, y, sim[u1][u2]))

        out_str.append(&#39;Delete units with dataset.delete_unit(N)&#39;)
        out_str = &#39;\n&#39;.join(out_str)
        userIO.tell_user(out_str, shell=shell)
        self.process_status[&#39;units_similarity&#39;] = True
        self.save()
        return violations, sim

    @Logger(&#39;Deleting Unit&#39;)
    def delete_unit(self, unit_num, shell=False):
        if isinstance(unit_num, str):
            unit_num = dio.h5io.parse_unit_number(unit_num)

        if unit_num is None:
            print(&#39;No unit deleted&#39;)
            return

        q = userIO.ask_user(&#39;Are you sure you want to delete unit%03i?&#39; % unit_num,
                            choices = [&#39;No&#39;,&#39;Yes&#39;], shell=shell)
        if q == 0:
            print(&#39;No unit deleted&#39;)
            return

        else:
            tmp = ss.delete_unit(self.root_dir, unit_num)
            if tmp is False:
                userIO.tell_user(&#39;Unit %i not found in dataset. No unit deleted&#39;
                                 % unit_num, shell=shell)
            else:
                userIO.tell_user(&#39;Unit %i sucessfully deleted.&#39; % unit_num,
                                 shell=shell)

        self.save()

    @Logger(&#39;Making Unit Arrays&#39;)
    def make_unit_arrays(self):
        &#39;&#39;&#39;Make spike arrays for each unit and store in hdf5 store
        &#39;&#39;&#39;
        params = self.spike_array_params

        print(&#39;Generating unit arrays with parameters:\n----------&#39;)
        print(pt.print_dict(params, tabs=1))
        ss.make_spike_arrays(self.h5_file, params)
        self.process_status[&#39;make_unit_arrays&#39;] = True
        self.save()

    @Logger(&#39;Making Unit Plots&#39;)
    def make_unit_plots(self):
        &#39;&#39;&#39;Make waveform plots for each sorted unit
        &#39;&#39;&#39;
        unit_table = self.get_unit_table()
        save_dir = os.path.join(self.root_dir, &#39;unit_waveforms_plots&#39;)
        if os.path.isdir(save_dir):
            shutil.rmtree(save_dir)

        os.mkdir(save_dir)
        for i, row in unit_table.iterrows():
            datplt.make_unit_plots(self.root_dir, row[&#39;unit_name&#39;], save_dir=save_dir)

        self.process_status[&#39;make_unit_plots&#39;] = True
        self.save()

    @Logger(&#39;Making PSTH Arrays&#39;)
    def make_psth_arrays(self):
        &#39;&#39;&#39;Make smoothed firing rate traces for each unit/trial and store in
        hdf5 store
        &#39;&#39;&#39;
        params = self.psth_params
        dig_ins = self.dig_in_mapping
        for idx, row in dig_ins.iterrows():
            spike_analysis.make_psths_for_tastant(self.h5_file,
                                                  params[&#39;window_size&#39;],
                                                  params[&#39;window_step&#39;],
                                                  row[&#39;channel&#39;])

        self.process_status[&#39;make_psth_arrays&#39;] = True
        self.save()

    @Logger(&#39;Calculating Palatability/Identity Metrics&#39;)
    def palatability_calculate(self, shell=False):
        pal_analysis.palatability_identity_calculations(self.root_dir,
                                                        params=self.pal_id_params)
        self.process_status[&#39;palatability_calculate&#39;] = True
        self.save()

    @Logger(&#39;Plotting Palatability/Identity Metrics&#39;)
    def palatability_plot(self, shell=False):
        pal_plt.plot_palatability_identity([self.root_dir], shell=shell)
        self.process_status[&#39;palatability_plot&#39;] = True
        self.save()

    def get_unit_table(self):
        &#39;&#39;&#39;Returns a pandas dataframe with sorted unit information

        Returns
        --------
        pandas.DataFrame with columns:
            unit_name, unit_num, electrode, single_unit,
            regular_spiking, fast_spiking
        &#39;&#39;&#39;
        unit_table = dio.h5io.get_unit_table(self.root_dir)
        return unit_table

    def extract_and_cluster(self, shell=False):
        pass

    def post_sorting(self):
        self.make_unit_plots()
        self.make_unit_arrays()
        self.units_similarity(shell=True)
        self.make_psth_arrays()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="blechpy.datastructures.objects.data_object" href="objects.html#blechpy.datastructures.objects.data_object">data_object</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="blechpy.datastructures.dataset.dataset.blech_clust_run"><code class="name flex">
<span>def <span class="ident">blech_clust_run</span></span>(<span>self, data_quality=None, n_cores=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Write clustering parameters to file and
Run blech_process on each electrode using GNU parallel</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data_quality</code></strong> :&ensp;{<code>'clean'</code>, <code>'noisy'</code>, <code>None</code> (default)}</dt>
<dd>set if you want to change the data quality parameters for cutoff
and spike detection before running clustering. These parameters are
automatically set as "clean" during initial parameter setup</dd>
<dt><strong><code>accept_params</code></strong> :&ensp;<code>bool</code>, <code>False</code> (default)</dt>
<dd>set to True in order to skip popup confirmation of parameters when
running</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@Logger(&#39;Running Blech Clust&#39;)
def blech_clust_run(self, data_quality=None, n_cores=None):
    &#39;&#39;&#39;Write clustering parameters to file and
    Run blech_process on each electrode using GNU parallel

    Parameters
    ----------
    data_quality : {&#39;clean&#39;, &#39;noisy&#39;, None (default)}
        set if you want to change the data quality parameters for cutoff
        and spike detection before running clustering. These parameters are
        automatically set as &#34;clean&#34; during initial parameter setup
    accept_params : bool, False (default)
        set to True in order to skip popup confirmation of parameters when
        running
    &#39;&#39;&#39;
    if data_quality:
        tmp = dio.params.load_params(&#39;clustering_params&#39;, self.root_dir,
                                     default_keyword=data_quality)
        if tmp:
            self.clustering_params = tmp
        else:
            raise ValueError(&#39;%s is not a valid data_quality preset. Must &#39;
                             &#39;be &#34;clean&#34; or &#34;noisy&#34; or None.&#39;)

    print(&#39;\nRunning Blech Clust\n-------------------&#39;)
    print(&#39;Parameters\n%s&#39; % pt.print_dict(self.clustering_params))

    # Create folders for saving things within recording dir
    data_dir = self.root_dir
    directories = [&#39;spike_waveforms&#39;, &#39;spike_times&#39;,
                   &#39;clustering_results&#39;,
                   &#39;Plots&#39;, &#39;memory_monitor_clustering&#39;]
    for d in directories:
        tmp_dir = os.path.join(data_dir, d)
        if os.path.exists(tmp_dir):
            shutil.rmtree(tmp_dir)

        os.mkdir(tmp_dir)

    # Set file for clusting log
    self.clustering_log = os.path.join(data_dir, &#39;results.log&#39;)
    if os.path.exists(self.clustering_log):
        os.remove(self.clustering_log)

    process_path = os.path.realpath(__file__)
    process_path = os.path.join(os.path.dirname(process_path),
                                &#39;blech_process.py&#39;)
    em = self.electrode_mapping
    if &#39;dead&#39; in em.columns:
        electrodes = em.Electrode[em[&#39;dead&#39;] == False].tolist()
    else:
        electrodes = em.Electrode.tolist()


    pbar = tqdm(total = len(electrodes))
    results = [(None, None, None)] * (max(electrodes)+1)
    clust_errors = [(x, None) for x in electrodes]
    def update_pbar(ans):
        if isinstance(ans, tuple) and ans[0] is not None:
            results[ans[0]] = ans
        else:
            print(&#39;Unexpected error when clustering an electrode&#39;)

        pbar.update()

    if cores is None or cores &gt; multiprocessing.cpu_count():
        cores = multiprocessing.cpu_count() - 1

    pool = multiprocessing.Pool(cores)
    for x in electrodes:
        pool.apply_async(blech_clust_process,
                         args=(x, data_dir, self.clustering_params),
                         callback=update_pbar)

    pool.close()
    pool.join()
    pbar.close()

    print(&#39;Electrode    Result    Cutoff (s)&#39;)
    cutoffs = {}
    clust_res = {}
    clustered = []
    for x, y, z in results:
        if x is None:
            continue

        clustered.append(x)
        print(&#39;  {:&lt;13}{:&lt;10}{}&#39;.format(x, y, z))
        cutoffs[x] = z
        clust_res[x] = y

    print(&#39;1 - Sucess\n0 - No data or no spikes\n-1 - Error&#39;)

    em = self.electrode_mapping.copy()
    em[&#39;cutoff_time&#39;] = em[&#39;Electrode&#39;].map(cutoffs)
    em[&#39;clustering_result&#39;] = em[&#39;Electrode&#39;].map(clust_res)
    self.electrode_mapping = em.copy()
    self.process_status[&#39;blech_clust_run&#39;] = True
    self.process_status[&#39;cleanup_clustering&#39;] = False
    dio.h5io.write_electrode_map_to_h5(self.h5_file, em)
    self.save()
    print(&#39;Clustering Complete\n------------------&#39;)
    return results</code></pre>
</details>
</dd>
<dt id="blechpy.datastructures.dataset.dataset.cleanup_clustering"><code class="name flex">
<span>def <span class="ident">cleanup_clustering</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Consolidates memory monitor files, removes raw and referenced data
and setups up hdf5 store for sorted units data</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@Logger(&#39;Cleaning up clustering memory logs. Removing raw data and setting&#39;
        &#39;up hdf5 for unit sorting&#39;)
def cleanup_clustering(self):
    &#39;&#39;&#39;Consolidates memory monitor files, removes raw and referenced data
    and setups up hdf5 store for sorted units data
    &#39;&#39;&#39;
    h5_file = dio.h5io.cleanup_clustering(self.root_dir)
    self.h5_file = h5_file
    self.process_status[&#39;cleanup_clustering&#39;] = True
    self.save()</code></pre>
</details>
</dd>
<dt id="blechpy.datastructures.dataset.dataset.common_average_reference"><code class="name flex">
<span>def <span class="ident">common_average_reference</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Define electrode groups and remove common average from
signals</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>num_groups</code></strong> :&ensp;<code>int</code> (optional)</dt>
<dd>number of CAR groups, if not provided
there's a prompt</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@Logger(&#39;Common Average Referencing&#39;)
def common_average_reference(self):
    &#39;&#39;&#39;Define electrode groups and remove common average from  signals

    Parameters
    ----------
    num_groups : int (optional)
        number of CAR groups, if not provided
        there&#39;s a prompt
    &#39;&#39;&#39;
    if not hasattr(self, &#39;CAR_electrodes&#39;):
        raise ValueError(&#39;CAR_electrodes not set&#39;)

    if not hasattr(self, &#39;electrode_mapping&#39;):
        raise ValueError(&#39;electrode_mapping not set&#39;)

    car_electrodes = self.CAR_electrodes
    num_groups = len(car_electrodes)
    em = self.electrode_mapping.copy()

    if &#39;dead&#39; in em.columns:
        dead_electrodes = em.Electrode[em.dead].to_list()
    else:
        dead_electrodes = []

    # Gather Common Average Reference Groups
    print(&#39;CAR Groups\n&#39;)
    headers = [&#39;Group %i&#39; % x for x in range(num_groups)]
    print(pt.print_list_table(car_electrodes, headers))

    # Reference each group
    for i, x in enumerate(car_electrodes):
        tmp = list(set(x) - set(dead_electrodes))
        dio.h5io.common_avg_reference(self.h5_file, tmp, i)

    # Compress and repack file
    dio.h5io.compress_and_repack(self.h5_file)

    self.process_status[&#39;common_average_reference&#39;] = True
    self.save()</code></pre>
</details>
</dd>
<dt id="blechpy.datastructures.dataset.dataset.create_trial_list"><code class="name flex">
<span>def <span class="ident">create_trial_list</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Create lists of trials based on digital inputs and outputs and store
to hdf5 store
Can only be run after data extraction</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@Logger(&#39;Creating Trial List&#39;)
def create_trial_list(self):
    &#39;&#39;&#39;Create lists of trials based on digital inputs and outputs and store
    to hdf5 store
    Can only be run after data extraction
    &#39;&#39;&#39;
    if not self.process_status[&#39;extract_data&#39;]:
        userIO.tell_user(&#39;Must extract data before creating trial list&#39;,shell=True)
        return

    if self.rec_info.get(&#39;dig_in&#39;):
        in_list = dio.h5io.create_trial_data_table(
            self.h5_file,
            self.dig_in_mapping,
            self.sampling_rate,
            &#39;in&#39;)
        self.dig_in_trials = in_list
    else:
        print(&#39;No digital input data found&#39;)

    if self.rec_info.get(&#39;dig_out&#39;):
        out_list = dio.h5io.create_trial_data_table(
            self.h5_file,
            self.dig_out_mapping,
            self.sampling_rate,
            &#39;out&#39;)
        self.dig_out_trials = out_list
    else:
        print(&#39;No digital output data found&#39;)

    self.process_status[&#39;create_trial_list&#39;] = True
    self.save()</code></pre>
</details>
</dd>
<dt id="blechpy.datastructures.dataset.dataset.delete_unit"><code class="name flex">
<span>def <span class="ident">delete_unit</span></span>(<span>self, unit_num, shell=False)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@Logger(&#39;Deleting Unit&#39;)
def delete_unit(self, unit_num, shell=False):
    if isinstance(unit_num, str):
        unit_num = dio.h5io.parse_unit_number(unit_num)

    if unit_num is None:
        print(&#39;No unit deleted&#39;)
        return

    q = userIO.ask_user(&#39;Are you sure you want to delete unit%03i?&#39; % unit_num,
                        choices = [&#39;No&#39;,&#39;Yes&#39;], shell=shell)
    if q == 0:
        print(&#39;No unit deleted&#39;)
        return

    else:
        tmp = ss.delete_unit(self.root_dir, unit_num)
        if tmp is False:
            userIO.tell_user(&#39;Unit %i not found in dataset. No unit deleted&#39;
                             % unit_num, shell=shell)
        else:
            userIO.tell_user(&#39;Unit %i sucessfully deleted.&#39; % unit_num,
                             shell=shell)

    self.save()</code></pre>
</details>
</dd>
<dt id="blechpy.datastructures.dataset.dataset.edit_clustering_params"><code class="name flex">
<span>def <span class="ident">edit_clustering_params</span></span>(<span>self, shell=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Allows user interface for editing clustering parameters</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>shell</code></strong> :&ensp;<code>bool</code> (optional)</dt>
<dd>True if you want command-line interface, False for GUI (default)</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def edit_clustering_params(self, shell=False):
    &#39;&#39;&#39;Allows user interface for editing clustering parameters

    Parameters
    ----------
    shell : bool (optional)
        True if you want command-line interface, False for GUI (default)
    &#39;&#39;&#39;
    tmp = userIO.fill_dict(self.clustering_params,
                           &#39;Clustering Parameters\n(Times in ms)&#39;,
                           shell=shell)
    if tmp:
        self.clustering_params = tmp
        wt.write_params_to_json(&#39;clustering_params&#39;, self.root_dir, tmp)</code></pre>
</details>
</dd>
<dt id="blechpy.datastructures.dataset.dataset.edit_pal_id_params"><code class="name flex">
<span>def <span class="ident">edit_pal_id_params</span></span>(<span>self, shell=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Allows user interface for editing palatability/identity parameters</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>shell</code></strong> :&ensp;<code>bool</code> (optional)</dt>
<dd>True if you want command-line interface, False for GUI (default)</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def edit_pal_id_params(self, shell=False):
    &#39;&#39;&#39;Allows user interface for editing palatability/identity parameters

    Parameters
    ----------
    shell : bool (optional)
        True if you want command-line interface, False for GUI (default)
    &#39;&#39;&#39;
    tmp = userIO.fill_dict(self.pal_id_params,
                           &#39;Palatability/Identity Parameters\n(Times in ms)&#39;,
                           shell=shell)
    if tmp:
        self.pal_id_params = tmp
        wt.write_params_to_json(&#39;pal_id_params&#39;, self.root_dir, tmp)</code></pre>
</details>
</dd>
<dt id="blechpy.datastructures.dataset.dataset.edit_psth_params"><code class="name flex">
<span>def <span class="ident">edit_psth_params</span></span>(<span>self, shell=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Allows user interface for editing psth parameters</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>shell</code></strong> :&ensp;<code>bool</code> (optional)</dt>
<dd>True if you want command-line interface, False for GUI (default)</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def edit_psth_params(self, shell=False):
    &#39;&#39;&#39;Allows user interface for editing psth parameters

    Parameters
    ----------
    shell : bool (optional)
        True if you want command-line interface, False for GUI (default)
    &#39;&#39;&#39;
    tmp = userIO.fill_dict(self.psth_params,
                           &#39;PSTH Parameters\n(Times in ms)&#39;,
                           shell=shell)
    if tmp:
        self.psth_params = tmp
        wt.write_params_to_json(&#39;psth_params&#39;, self.root_dir, tmp)</code></pre>
</details>
</dd>
<dt id="blechpy.datastructures.dataset.dataset.edit_spike_array_params"><code class="name flex">
<span>def <span class="ident">edit_spike_array_params</span></span>(<span>self, shell=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Edit spike array parameters and adjust dig_in_mapping accordingly</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>shell</code></strong> :&ensp;<code>bool</code>, <code>whether</code> <code>to</code> <code>use</code> <code>CLI</code> or <code>GUI</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def edit_spike_array_params(self, shell=False):
    &#39;&#39;&#39;Edit spike array parameters and adjust dig_in_mapping accordingly

    Parameters
    ----------
    shell : bool, whether to use CLI or GUI
    &#39;&#39;&#39;
    if not hasattr(self, &#39;dig_in_mapping&#39;):
        self.spike_array_params = None
        return

    sa = deepcopy(self.spike_array_params)
    tmp = userIO.fill_dict(sa, &#39;Spike Array Parameters\n(Times in ms)&#39;,
                           shell=shell)
    if tmp is None:
        return

    dim = self.dig_in_mapping
    dim[&#39;spike_array&#39;] = False
    if tmp[&#39;dig_ins_to_use&#39;] != [&#39;&#39;]:
        tmp[&#39;dig_ins_to_use&#39;] = [int(x) for x in tmp[&#39;dig_ins_to_use&#39;]]
        dim.loc[[x in tmp[&#39;dig_ins_to_use&#39;] for x in dim.channel],
                &#39;spike_array&#39;] = True

    dim[&#39;laser_channels&#39;] = False
    if tmp[&#39;laser_channels&#39;] != [&#39;&#39;]:
        tmp[&#39;laser_channels&#39;] = [int(x) for x in tmp[&#39;laser_channels&#39;]]
        dim.loc[[x in tmp[&#39;laser_channels&#39;] for x in dim.channel],
                &#39;laser&#39;] = True

    self.spike_array_params = tmp.copy()
    wt.write_params_to_json(&#39;spike_array_params&#39;,
                            self.root_dir, tmp)</code></pre>
</details>
</dd>
<dt id="blechpy.datastructures.dataset.dataset.extract_and_cluster"><code class="name flex">
<span>def <span class="ident">extract_and_cluster</span></span>(<span>self, shell=False)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extract_and_cluster(self, shell=False):
    pass</code></pre>
</details>
</dd>
<dt id="blechpy.datastructures.dataset.dataset.extract_data"><code class="name flex">
<span>def <span class="ident">extract_data</span></span>(<span>self, filename=None, shell=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Create hdf5 store for data and read in Intan .dat files. Also create
subfolders for processing outputs</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data_quality</code></strong> :&ensp;{<code>'clean'</code>, <code>'noisy'</code>} (optional)</dt>
<dd>Specifies quality of data for default clustering parameters
associated. Should generally first process with clean (default)
parameters and then try noisy after running blech_clust and
checking if too many electrodes as cutoff too early</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@Logger(&#39;Extracting Data&#39;)
def extract_data(self, filename=None, shell=False):
    &#39;&#39;&#39;Create hdf5 store for data and read in Intan .dat files. Also create
    subfolders for processing outputs

    Parameters
    ----------
    data_quality: {&#39;clean&#39;, &#39;noisy&#39;} (optional)
        Specifies quality of data for default clustering parameters
        associated. Should generally first process with clean (default)
        parameters and then try noisy after running blech_clust and
        checking if too many electrodes as cutoff too early
    &#39;&#39;&#39;
    if self.rec_info[&#39;file_type&#39;] is None:
        raise ValueError(&#39;Unsupported recording type. Cannot extract yet.&#39;)

    if filename is None:
        filename = self.h5_file

    print(&#39;\nExtract Intan Data\n--------------------&#39;)
    # Create h5 file
    tmp = dio.h5io.create_empty_data_h5(filename, shell)
    if tmp is None:
        return

    # Create arrays for raw data in hdf5 store
    dio.h5io.create_hdf_arrays(filename, self.rec_info,
                               self.electrode_mapping, self.emg_mapping)

    # Read in data to arrays
    dio.h5io.read_files_into_arrays(filename,
                                    self.rec_info,
                                    self.electrode_mapping,
                                    self.emg_mapping)

    # Write electrode and digital input mapping into h5 file
    # TODO: write EMG and digital output mapping into h5 file
    dio.h5io.write_electrode_map_to_h5(self.h5_file, self.electrode_mapping)
    if self.dig_in_mapping is not None:
        dio.h5io.write_digital_map_to_h5(self.h5_file, self.dig_in_mapping, &#39;in&#39;)

    # update status
    self.h5_file = filename
    self.process_status[&#39;extract_data&#39;] = True
    self.save()
    print(&#39;\nData Extraction Complete\n--------------------&#39;)</code></pre>
</details>
</dd>
<dt id="blechpy.datastructures.dataset.dataset.get_unit_table"><code class="name flex">
<span>def <span class="ident">get_unit_table</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns a pandas dataframe with sorted unit information</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pandas.DataFrame</code> <code>with</code> <code>columns</code>:</dt>
<dd>unit_name, unit_num, electrode, single_unit,
regular_spiking, fast_spiking</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_unit_table(self):
    &#39;&#39;&#39;Returns a pandas dataframe with sorted unit information

    Returns
    --------
    pandas.DataFrame with columns:
        unit_name, unit_num, electrode, single_unit,
        regular_spiking, fast_spiking
    &#39;&#39;&#39;
    unit_table = dio.h5io.get_unit_table(self.root_dir)
    return unit_table</code></pre>
</details>
</dd>
<dt id="blechpy.datastructures.dataset.dataset.initParams"><code class="name flex">
<span>def <span class="ident">initParams</span></span>(<span>self, data_quality='clean', emg_port=None, emg_channels=None, car_keyword=None, car_group_areas=None, shell=False, dig_in_names=None, dig_out_names=None, accept_params=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Initalizes basic default analysis parameters and allows customization
of parameters</p>
<h2 id="parameters-all-optional">Parameters (all optional)</h2>
<p>data_quality : {'clean', 'noisy'}
keyword defining which default set of parameters to use to detect
headstage disconnection during clustering
default is 'clean'. Best practice is to run blech_clust as 'clean'
and re-run as 'noisy' if too many early cutoffs occurr
emg_port : str
Port ('A', 'B', 'C') of EMG, if there was an EMG. None (default)
will query user. False indicates no EMG port and not to query user
emg_channels : list of int
channel or channels of EMGs on port specified
default is None
car_keyword : str
Specifes default common average reference groups
defaults are found in CAR_defaults.json
Currently 'bilateral32' is only keyword available
If left as None (default) user will be queries to select common
average reference groups
shell : bool
False (default) for GUI. True for command-line interface
dig_in_names : list of str
Names of digital inputs. Must match number of digital inputs used
in recording.
None (default) queries user to name each dig_in
dig_out_names : list of str
Names of digital outputs. Must match number of digital outputs in
recording.
None (default) queries user to name each dig_out
accept_params : bool
True automatically accepts default parameters where possible,
decreasing user queries
False (default) will query user to confirm or edit parameters for
clustering, spike array and psth creation and palatability/identity
calculations</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@Logger(&#39;Initializing Parameters&#39;)
def initParams(self, data_quality=&#39;clean&#39;, emg_port=None,
               emg_channels=None, car_keyword=None,
               car_group_areas=None,
               shell=False, dig_in_names=None,
               dig_out_names=None, accept_params=False):
    &#39;&#39;&#39;
    Initalizes basic default analysis parameters and allows customization
    of parameters

    Parameters (all optional)
    -------------------------
    data_quality : {&#39;clean&#39;, &#39;noisy&#39;}
        keyword defining which default set of parameters to use to detect
        headstage disconnection during clustering
        default is &#39;clean&#39;. Best practice is to run blech_clust as &#39;clean&#39;
        and re-run as &#39;noisy&#39; if too many early cutoffs occurr
    emg_port : str
        Port (&#39;A&#39;, &#39;B&#39;, &#39;C&#39;) of EMG, if there was an EMG. None (default)
        will query user. False indicates no EMG port and not to query user
    emg_channels : list of int
        channel or channels of EMGs on port specified
        default is None
    car_keyword : str
        Specifes default common average reference groups
        defaults are found in CAR_defaults.json
        Currently &#39;bilateral32&#39; is only keyword available
        If left as None (default) user will be queries to select common
        average reference groups
    shell : bool
        False (default) for GUI. True for command-line interface
    dig_in_names : list of str
        Names of digital inputs. Must match number of digital inputs used
        in recording.
        None (default) queries user to name each dig_in
    dig_out_names : list of str
        Names of digital outputs. Must match number of digital outputs in
        recording.
        None (default) queries user to name each dig_out
    accept_params : bool
        True automatically accepts default parameters where possible,
        decreasing user queries
        False (default) will query user to confirm or edit parameters for
        clustering, spike array and psth creation and palatability/identity
        calculations
    &#39;&#39;&#39;
    # Get parameters from info.rhd
    file_dir = self.root_dir
    rec_info = dio.rawIO.read_rec_info(file_dir)
    ports = rec_info.pop(&#39;ports&#39;)
    channels = rec_info.pop(&#39;channels&#39;)
    sampling_rate = rec_info[&#39;amplifier_sampling_rate&#39;]
    self.rec_info = rec_info
    self.sampling_rate = sampling_rate

    # Get default parameters from files
    clustering_params = dio.params.load_params(&#39;clustering_params&#39;, file_dir,
                                               default_keyword=data_quality)
    spike_array_params = dio.params.load_params(&#39;spike_array_params&#39;, file_dir)
    psth_params = dio.params.load_params(&#39;psth_params&#39;, file_dir)
    pal_id_params = dio.params.load_params(&#39;pal_id_params&#39;, file_dir)
    spike_array_params[&#39;sampling_rate&#39;] = sampling_rate
    clustering_params[&#39;file_dir&#39;] = file_dir
    clustering_params[&#39;sampling_rate&#39;] = sampling_rate

    # Setup digital input mapping
    #TODO: Setup digital output mapping...ignoring for now
    if rec_info.get(&#39;dig_in&#39;):
        self._setup_din_mapping(dig_in_names, shell)
        dim = self.dig_in_mapping.copy()
        spike_array_params[&#39;laser_channels&#39;] = dim.channel[dim[&#39;laser&#39;]].to_list()
        spike_array_params[&#39;dig_ins_to_use&#39;] = dim.channel[dim[&#39;spike_array&#39;]].to_list()
    else:
        self.dig_in_mapping = None

    # Setup electrode and emg mapping
    self._setup_channel_mapping(ports, channels, emg_port,
                                emg_channels, shell=shell)

    # Set CAR groups
    self._set_CAR_groups(group_keyword=car_keyword, group_areas=car_group_areas, shell=shell)

    # Confirm parameters
    self.spike_array_params = spike_array_params
    if not accept_params:
        conf = userIO.confirm_parameter_dict
        clustering_params = conf(clustering_params,
                                 &#39;Clustering Parameters&#39;, shell=shell)
        self.edit_spike_array_params(shell=shell)
        psth_params = conf(psth_params,
                           &#39;PSTH Parameters&#39;, shell=shell)
        pal_id_params = conf(pal_id_params,
                             &#39;Palatability/Identity Parameters\n&#39;
                             &#39;Valid unit_type is Single, Multi or All&#39;,
                             shell=shell)

    # Store parameters
    self.clustering_params = clustering_params
    self.pal_id_params = pal_id_params
    self.psth_params = psth_params
    self._write_all_params_to_json()
    self.process_status[&#39;initialize parameters&#39;] = True
    self.save()</code></pre>
</details>
</dd>
<dt id="blechpy.datastructures.dataset.dataset.make_psth_arrays"><code class="name flex">
<span>def <span class="ident">make_psth_arrays</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Make smoothed firing rate traces for each unit/trial and store in
hdf5 store</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@Logger(&#39;Making PSTH Arrays&#39;)
def make_psth_arrays(self):
    &#39;&#39;&#39;Make smoothed firing rate traces for each unit/trial and store in
    hdf5 store
    &#39;&#39;&#39;
    params = self.psth_params
    dig_ins = self.dig_in_mapping
    for idx, row in dig_ins.iterrows():
        spike_analysis.make_psths_for_tastant(self.h5_file,
                                              params[&#39;window_size&#39;],
                                              params[&#39;window_step&#39;],
                                              row[&#39;channel&#39;])

    self.process_status[&#39;make_psth_arrays&#39;] = True
    self.save()</code></pre>
</details>
</dd>
<dt id="blechpy.datastructures.dataset.dataset.make_unit_arrays"><code class="name flex">
<span>def <span class="ident">make_unit_arrays</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Make spike arrays for each unit and store in hdf5 store</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@Logger(&#39;Making Unit Arrays&#39;)
def make_unit_arrays(self):
    &#39;&#39;&#39;Make spike arrays for each unit and store in hdf5 store
    &#39;&#39;&#39;
    params = self.spike_array_params

    print(&#39;Generating unit arrays with parameters:\n----------&#39;)
    print(pt.print_dict(params, tabs=1))
    ss.make_spike_arrays(self.h5_file, params)
    self.process_status[&#39;make_unit_arrays&#39;] = True
    self.save()</code></pre>
</details>
</dd>
<dt id="blechpy.datastructures.dataset.dataset.make_unit_plots"><code class="name flex">
<span>def <span class="ident">make_unit_plots</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Make waveform plots for each sorted unit</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@Logger(&#39;Making Unit Plots&#39;)
def make_unit_plots(self):
    &#39;&#39;&#39;Make waveform plots for each sorted unit
    &#39;&#39;&#39;
    unit_table = self.get_unit_table()
    save_dir = os.path.join(self.root_dir, &#39;unit_waveforms_plots&#39;)
    if os.path.isdir(save_dir):
        shutil.rmtree(save_dir)

    os.mkdir(save_dir)
    for i, row in unit_table.iterrows():
        datplt.make_unit_plots(self.root_dir, row[&#39;unit_name&#39;], save_dir=save_dir)

    self.process_status[&#39;make_unit_plots&#39;] = True
    self.save()</code></pre>
</details>
</dd>
<dt id="blechpy.datastructures.dataset.dataset.mark_dead_channels"><code class="name flex">
<span>def <span class="ident">mark_dead_channels</span></span>(<span>self, dead_channels=None, shell=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Plots small piece of raw traces and a metric to help identify dead
channels. Once user marks channels as dead a new column is added to
electrode mapping</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dead_channels</code></strong> :&ensp;<code>list</code> of <code>int</code>, optional</dt>
<dd>if this is specified then nothing is plotted, those channels are
simply marked as dead</dd>
<dt><strong><code>shell</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@Logger(&#39;Marking Dead Channels&#39;)
def mark_dead_channels(self, dead_channels=None, shell=False):
    &#39;&#39;&#39;Plots small piece of raw traces and a metric to help identify dead
    channels. Once user marks channels as dead a new column is added to
    electrode mapping

    Parameters
    ----------
    dead_channels : list of int, optional
        if this is specified then nothing is plotted, those channels are
        simply marked as dead
    shell : bool, optional
    &#39;&#39;&#39;
    print(&#39;Marking dead channels\n----------&#39;)
    em = self.electrode_mapping.copy()
    if dead_channels is None:
        userIO.tell_user(&#39;Making traces figure for dead channel detection...&#39;,
                         shell=True)
        save_file = os.path.join(self.root_dir, &#39;Electrode_Traces.png&#39;)
        fig, ax = datplt.plot_traces_and_outliers(self.h5_filei, save_file=save_file)
        if not shell:
            # Better to open figure outside of python since its a lot of
            # data on figure and matplotlib is slow
            subprocess.call([&#39;xdg-open&#39;, save_file])
        else:
            userIO.tell_user(&#39;Saved figure of traces to %s for reference&#39;
                             % save_file, shell=shell)

        choice = userIO.select_from_list(&#39;Select dead channels:&#39;,
                                         em.Electrode.to_list(),
                                         &#39;Dead Channel Selection&#39;,
                                         multi_select=True,
                                         shell=shell)
        dead_channels = list(map(int, choice))

    print(&#39;Marking eletrodes %s as dead.\n&#39;
          &#39;They will be excluded from common average referencing.&#39;
          % dead_channels)
    em[&#39;dead&#39;] = False
    em.loc[dead_channels, &#39;dead&#39;] = True
    self.electrode_mapping = em
    if os.path.isfile(self.h5_file):
        dio.h5io.write_electrode_map_to_h5(self.h5_file, self.electrode_mapping)

    self.process_status[&#39;mark_dead_channels&#39;] = True
    self.save()
    return dead_channels</code></pre>
</details>
</dd>
<dt id="blechpy.datastructures.dataset.dataset.palatability_calculate"><code class="name flex">
<span>def <span class="ident">palatability_calculate</span></span>(<span>self, shell=False)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@Logger(&#39;Calculating Palatability/Identity Metrics&#39;)
def palatability_calculate(self, shell=False):
    pal_analysis.palatability_identity_calculations(self.root_dir,
                                                    params=self.pal_id_params)
    self.process_status[&#39;palatability_calculate&#39;] = True
    self.save()</code></pre>
</details>
</dd>
<dt id="blechpy.datastructures.dataset.dataset.palatability_plot"><code class="name flex">
<span>def <span class="ident">palatability_plot</span></span>(<span>self, shell=False)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@Logger(&#39;Plotting Palatability/Identity Metrics&#39;)
def palatability_plot(self, shell=False):
    pal_plt.plot_palatability_identity([self.root_dir], shell=shell)
    self.process_status[&#39;palatability_plot&#39;] = True
    self.save()</code></pre>
</details>
</dd>
<dt id="blechpy.datastructures.dataset.dataset.post_sorting"><code class="name flex">
<span>def <span class="ident">post_sorting</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def post_sorting(self):
    self.make_unit_plots()
    self.make_unit_arrays()
    self.units_similarity(shell=True)
    self.make_psth_arrays()</code></pre>
</details>
</dd>
<dt id="blechpy.datastructures.dataset.dataset.sort_units"><code class="name flex">
<span>def <span class="ident">sort_units</span></span>(<span>self, shell=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Begins processes to allow labelling of clusters as sorted units</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>shell</code></strong> :&ensp;<code>bool</code></dt>
<dd>True if command-line interfaced desired, False for GUI (default)</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sort_units(self, shell=False):
    &#39;&#39;&#39;Begins processes to allow labelling of clusters as sorted units

    Parameters
    ----------
    shell : bool
        True if command-line interfaced desired, False for GUI (default)
    &#39;&#39;&#39;
    fs = self.sampling_rate
    ss.sort_units(self.root_dir, fs, shell)
    self.process_status[&#39;sort_units&#39;] = True
    self.save()</code></pre>
</details>
</dd>
<dt id="blechpy.datastructures.dataset.dataset.units_similarity"><code class="name flex">
<span>def <span class="ident">units_similarity</span></span>(<span>self, similarity_cutoff=50, shell=False)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@Logger(&#39;Calculating Units Similarity&#39;)
def units_similarity(self, similarity_cutoff=50, shell=False):
    if &#39;SSH_CONNECTION&#39; in os.environ:
        shell= True

    metrics_dir = os.path.join(self.root_dir, &#39;sorted_unit_metrics&#39;)
    if not os.path.isdir(metrics_dir):
        raise ValueError(&#39;No sorted unit metrics found. Must sort units before calculating similarity&#39;)

    violation_file = os.path.join(metrics_dir,
                                  &#39;units_similarity_violations.txt&#39;)
    violations, sim = ss.calc_units_similarity(self.h5_file,
                                               self.sampling_rate,
                                               similarity_cutoff,
                                               violation_file)
    if len(violations) == 0:
        userIO.tell_user(&#39;No similarity violations found!&#39;, shell=shell)
        self.process_status[&#39;units_similarity&#39;] = True
        return violations, sim

    out_str = [&#39;Units Similarity Violations Found:&#39;]
    out_str.append(&#39;Unit_1    Unit_2    Similarity&#39;)
    for x,y in violations:
        u1 = dio.h5io.parse_unit_number(x)
        u2 = dio.h5io.parse_unit_number(y)
        out_str.append(&#39;   {:&lt;10}{:&lt;10}{}\n&#39;.format(x, y, sim[u1][u2]))

    out_str.append(&#39;Delete units with dataset.delete_unit(N)&#39;)
    out_str = &#39;\n&#39;.join(out_str)
    userIO.tell_user(out_str, shell=shell)
    self.process_status[&#39;units_similarity&#39;] = True
    self.save()
    return violations, sim</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="blechpy.datastructures" href="index.html">blechpy.datastructures</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="blechpy.datastructures.dataset.port_in_dataset" href="#blechpy.datastructures.dataset.port_in_dataset">port_in_dataset</a></code></li>
<li><code><a title="blechpy.datastructures.dataset.validate_data_integrity" href="#blechpy.datastructures.dataset.validate_data_integrity">validate_data_integrity</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="blechpy.datastructures.dataset.dataset" href="#blechpy.datastructures.dataset.dataset">dataset</a></code></h4>
<ul class="">
<li><code><a title="blechpy.datastructures.dataset.dataset.blech_clust_run" href="#blechpy.datastructures.dataset.dataset.blech_clust_run">blech_clust_run</a></code></li>
<li><code><a title="blechpy.datastructures.dataset.dataset.cleanup_clustering" href="#blechpy.datastructures.dataset.dataset.cleanup_clustering">cleanup_clustering</a></code></li>
<li><code><a title="blechpy.datastructures.dataset.dataset.common_average_reference" href="#blechpy.datastructures.dataset.dataset.common_average_reference">common_average_reference</a></code></li>
<li><code><a title="blechpy.datastructures.dataset.dataset.create_trial_list" href="#blechpy.datastructures.dataset.dataset.create_trial_list">create_trial_list</a></code></li>
<li><code><a title="blechpy.datastructures.dataset.dataset.delete_unit" href="#blechpy.datastructures.dataset.dataset.delete_unit">delete_unit</a></code></li>
<li><code><a title="blechpy.datastructures.dataset.dataset.edit_clustering_params" href="#blechpy.datastructures.dataset.dataset.edit_clustering_params">edit_clustering_params</a></code></li>
<li><code><a title="blechpy.datastructures.dataset.dataset.edit_pal_id_params" href="#blechpy.datastructures.dataset.dataset.edit_pal_id_params">edit_pal_id_params</a></code></li>
<li><code><a title="blechpy.datastructures.dataset.dataset.edit_psth_params" href="#blechpy.datastructures.dataset.dataset.edit_psth_params">edit_psth_params</a></code></li>
<li><code><a title="blechpy.datastructures.dataset.dataset.edit_spike_array_params" href="#blechpy.datastructures.dataset.dataset.edit_spike_array_params">edit_spike_array_params</a></code></li>
<li><code><a title="blechpy.datastructures.dataset.dataset.extract_and_cluster" href="#blechpy.datastructures.dataset.dataset.extract_and_cluster">extract_and_cluster</a></code></li>
<li><code><a title="blechpy.datastructures.dataset.dataset.extract_data" href="#blechpy.datastructures.dataset.dataset.extract_data">extract_data</a></code></li>
<li><code><a title="blechpy.datastructures.dataset.dataset.get_unit_table" href="#blechpy.datastructures.dataset.dataset.get_unit_table">get_unit_table</a></code></li>
<li><code><a title="blechpy.datastructures.dataset.dataset.initParams" href="#blechpy.datastructures.dataset.dataset.initParams">initParams</a></code></li>
<li><code><a title="blechpy.datastructures.dataset.dataset.make_psth_arrays" href="#blechpy.datastructures.dataset.dataset.make_psth_arrays">make_psth_arrays</a></code></li>
<li><code><a title="blechpy.datastructures.dataset.dataset.make_unit_arrays" href="#blechpy.datastructures.dataset.dataset.make_unit_arrays">make_unit_arrays</a></code></li>
<li><code><a title="blechpy.datastructures.dataset.dataset.make_unit_plots" href="#blechpy.datastructures.dataset.dataset.make_unit_plots">make_unit_plots</a></code></li>
<li><code><a title="blechpy.datastructures.dataset.dataset.mark_dead_channels" href="#blechpy.datastructures.dataset.dataset.mark_dead_channels">mark_dead_channels</a></code></li>
<li><code><a title="blechpy.datastructures.dataset.dataset.palatability_calculate" href="#blechpy.datastructures.dataset.dataset.palatability_calculate">palatability_calculate</a></code></li>
<li><code><a title="blechpy.datastructures.dataset.dataset.palatability_plot" href="#blechpy.datastructures.dataset.dataset.palatability_plot">palatability_plot</a></code></li>
<li><code><a title="blechpy.datastructures.dataset.dataset.post_sorting" href="#blechpy.datastructures.dataset.dataset.post_sorting">post_sorting</a></code></li>
<li><code><a title="blechpy.datastructures.dataset.dataset.sort_units" href="#blechpy.datastructures.dataset.dataset.sort_units">sort_units</a></code></li>
<li><code><a title="blechpy.datastructures.dataset.dataset.units_similarity" href="#blechpy.datastructures.dataset.dataset.units_similarity">units_similarity</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.7.1</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>